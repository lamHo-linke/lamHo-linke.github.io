<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    普通最小二乘法OLS的逐步求解 |  Dream d&#39;Orange
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/orange-favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
<script src="/js/pace.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-OLS" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  普通最小二乘法OLS的逐步求解
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/06/OLS/" class="article-date">
  <time datetime="2020-03-05T16:32:32.000Z" itemprop="datePublished">2020-03-06</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/Math/">Math</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">2.3k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">12分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="1-一元线性回归"><a href="#1-一元线性回归" class="headerlink" title="1. 一元线性回归"></a>1. 一元线性回归</h2><p>模型：</p>
<p>$$\mathbf{y}\approx\beta _0+\beta _1\mathbf{x}$$ </p>
<p>$$\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix},<br>\quad\mathbf{x}=\begin{pmatrix}x_1\\x_2\\\vdots\\x_n\end{pmatrix}$$</p>
<p>对于每个样本：$$y_i=\hat{\beta}_0+\hat{\beta}_1x_i+\varepsilon_i$$ $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$$<br>残差平方和 $$\text{RSS}=\sum^n_{i=1}(y_i-\hat{y}_i)^2$$</p>
<p>记函数$L(\beta_0,\beta_1)$为 $$L(\beta_0,\beta_1)=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2$$<br>那么 $$(\hat{\beta}_0,\hat{\beta}_1)=\arg\min_{(\beta_0,\beta_1)}L(\beta_0,\beta_1)$$</p>
<h3 id="1-1-OLS第一步：求偏导"><a href="#1-1-OLS第一步：求偏导" class="headerlink" title="1.1 OLS第一步：求偏导"></a>1.1 OLS第一步：求偏导</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    \displaystyle\frac{\partial L}{\partial \beta _0} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)} \\<br>    \displaystyle\frac{\partial L}{\partial \beta _1} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}<br>\end{array}<br>\right.<br>$$</p>
<h3 id="1-2-OLS第二步：让偏导等于零求解"><a href="#1-2-OLS第二步：让偏导等于零求解" class="headerlink" title="1.2 OLS第二步：让偏导等于零求解"></a>1.2 OLS第二步：让偏导等于零求解</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)}=0 \\<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}=0<br>\end{array}<br>\right.<br>$$</p>
<p>约掉-2，把求和里的每一项拆出来<br>$$<br>\left \{<br>\begin{array}{c}<br>    \displaystyle\sum_{i=1}^n{y_i}-n\beta _0-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _1&amp;=0 &amp;&amp;&amp;① \\<br>    \displaystyle\sum_{i=1}^n{x_iy_i}-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _0-\left( \displaystyle\sum_{i=1}^n{x_i^2} \right) \beta _1&amp;=0 &amp;&amp;&amp;②<br>\end{array}<br>\right.<br>$$</p>
<p>消元法，把①式的$\beta_0$用$\beta_1$来表示，然后代入②式求解出$\beta_1$<br>$$<br>\beta _0=\frac{1}{n}\sum_{i=1}^n{y_i}-\left( \frac{1}{n}\sum_{i=1}^n{x_i} \right) \beta _1<br>$$ $$<br>\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) \left( \sum_{i=1}^n{y_i} \right) +\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) ^2\beta _1-\left( \sum_{i=1}^n{x_i^2} \right) \beta _1=0<br>$$<br>求出$\beta_1$的值，这是$\beta_1$的估计值，记为$\hat{\beta_1}$<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) \left( \sum\limits_{i=1}^n{y_i} \right)}{\sum\limits_{i=1}^n{x_i^2}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) ^2}<br>$$ 这个表达式一大坨不好看，还可以简化一下，之后用在数据集上算也简便很多。$\frac{1}{n}\sum\limits_{i=1}^n{x_i}$是$x$的均值，记为$\bar{x}$；同样有$\bar{y}=\sum\limits_{i=1}^n{y_i}$<br>$$<br>\sum_{i=1}^n{x_i}=n\bar{x}，<br>\sum_{i=1}^n{y_i}=n\bar{y}<br>$$<br>先变分子，<br>$$<br>\begin{aligned}\text{分子}<br>&amp;=\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( n\bar{x} \right) \left( n\bar{y} \right) \\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-2n\bar{x}\bar{y}+n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( n\bar{y} \right) -\bar{y}\left( n\bar{x} \right) +n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( \sum_{i=1}^n{y_i} \right)-\bar{y}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}\bar{y}}\\\<br>&amp;=\sum_{i=1}^n{\left( x_iy_i-\bar{x}y_i-\bar{y}x_i+\bar{x}\bar{y} \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}\end{aligned}<br>$$<br>再变分母，<br>$$<br>\begin{aligned}\text{分母}&amp;=\sum_{i=1}^n{x_i^2}-\frac{1}{n}\left( n\bar{x} \right) ^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2n\bar{x}^2+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\left( n\bar{x} \right) \bar{x}+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\bar{x}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}^2}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i^2-2\bar{x}x_i+\bar{x}^2 \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) ^2}\end{aligned}<br>$$<br>最后得到<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>$$<br>刚才的$\beta_0$用$\beta_1$表示，现在求出了$\beta_1$，$\beta_0$也就呼之欲出<br>$$<br>\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}<br>$$<br>最终，得到对于一元线性回归模型OLS的解为<br>$$<br>\left \{<br>\begin{array}{l}<br>    \hat{\beta}_0= \bar{y}-\hat{\beta}_1\bar{x} \\<br>    \hat{\beta}_1= \displaystyle\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>\end{array}<br>\right.<br>$$</p>
<h2 id="2-多元线性回归"><a href="#2-多元线性回归" class="headerlink" title="2. 多元线性回归"></a>2. 多元线性回归</h2><p>模型：$$\mathbf{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon }$$<br>$$\begin{array}{l}\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}\\\<br>\boldsymbol{X}=\begin{pmatrix}1&amp;x_{11}&amp;x_{12}&amp;\dots&amp;x_{1p}\\1&amp;x_{21}&amp;x_{22}&amp;\dots&amp;x_{2p}\\\vdots&amp; &amp;&amp;\ddots&amp;\vdots\\1&amp;x_{n1}&amp;x_{n2}&amp;\dots&amp;x_{np}\end{pmatrix},\quad<br>\boldsymbol{\beta}=\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}\\\<br>\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\end{pmatrix}\end{array}$$<br>对于每个样本：$$\begin{aligned}y_i<br>&amp;=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}+\varepsilon_i\\\<br>&amp;=\begin{pmatrix}1&amp;x_{i1}&amp;x_{i2}&amp;\cdots&amp;x_{ip}\end{pmatrix}\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}+\varepsilon_i\end{aligned}$$<br>残差平方和$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp; =\sum_{i=1}^n{\left( y_i-\hat{y}_i \right) ^2}=\sum_{i=1}^n{\varepsilon _i^2}\\\<br>&amp; =\varepsilon _1^2+\varepsilon _2^2+\cdots +\varepsilon _n^2\\\<br>&amp;=\left( \begin{matrix}<br>    \varepsilon _1&amp;        \varepsilon _2&amp;        \cdots&amp;        \varepsilon _n\<br>\end{matrix} \right) \left( \begin{array}{c}<br>    \varepsilon _1\\\<br>    \varepsilon _2\\\<br>    \vdots\\\<br>    \varepsilon _n<br>\end{array} \right) \\\<br>&amp; =\boldsymbol{\varepsilon }^T\boldsymbol{\varepsilon }\\\<br>&amp; =\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)<br>\end{aligned}<br>$$</p>
<h3 id="2-1-矩阵相乘"><a href="#2-1-矩阵相乘" class="headerlink" title="2.1 矩阵相乘"></a>2.1 矩阵相乘</h3><p>前方一波矩阵运算，先补一点矩阵运算基础小知识：</p>
<ul>
<li>矩阵相乘不满足交换律<br>$$AB\ne BA$$</li>
<li>矩阵相乘满足结合律<br>$$(AB)C=A(BC)$$</li>
<li>矩阵相乘的转置提出来，要把前后两个矩阵位置换一下<br>$$(AB)^T=B^TA^T$$</li>
</ul>
<p>继续算$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp;=\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\left( \mathbf{y}^T-\left( \boldsymbol{X\beta } \right) ^T \right) \left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\left( \boldsymbol{X\beta } \right) ^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\left( \boldsymbol{X\beta } \right) ^T\boldsymbol{X\beta }\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>\end{aligned}$$</p>
<p>第3项$\mathbf{y}^T\boldsymbol{X\beta }$乘出来的结果是一个标量，也就是一个数（一个数怎么转置都是同一个东西）<br>$$\mathbf{y}^T\boldsymbol{X\beta}=<br>\begin{pmatrix}<br>    &amp;&amp; \mathbf{y}^T &amp;&amp;<br>\end{pmatrix}_{1\times n}<br>\begin{pmatrix}<br>    \\\<br>    \\\<br>    \boldsymbol{X\beta}\\\<br>    \\\\<br>\end{pmatrix}_{n\times 1}<br>=(\cdot)_{1\times 1}<br>$$</p>
<p>所以第3项可以写成$\mathbf{y}^T\boldsymbol{X\beta }=\left( \mathbf{y}^T\boldsymbol{X\beta } \right) ^T=\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}$，跟前面第二项一样，合并起来$L(\boldsymbol{\beta})$最后写成这样<br>$$<br>L\left( \boldsymbol{\beta } \right) =\mathbf{y}^T\mathbf{y}-2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<h3 id="2-2-矩阵求导"><a href="#2-2-矩阵求导" class="headerlink" title="2.2 矩阵求导"></a>2.2 矩阵求导</h3><p>接下来要求偏导，又来补一波矩阵小知识，关于对矩阵求导</p>
<ul>
<li>$\boldsymbol{a,b}$都是K阶向量<br>$$<br>\frac{\partial \boldsymbol{a}^T\boldsymbol{b}}{\partial \boldsymbol{b}}=\frac{\partial \boldsymbol{b}^T\boldsymbol{a}}{\partial \boldsymbol{b}}=\boldsymbol{a}<br>$$</li>
<li>$\boldsymbol{A}$是对称矩阵<br>$$<br>\frac{\partial \boldsymbol{b}^T\boldsymbol{Ab}}{\partial \boldsymbol{b}}=2\boldsymbol{Ab}=2\boldsymbol{b}^T\boldsymbol{A}<br>$$</li>
</ul>
<p>OK，刀磨好了继续砍柴，来求$L(\boldsymbol{\beta})$的偏导，第一项没有$\boldsymbol{\beta }$，不管它；第二项的$\boldsymbol{\beta }$是px1向量，$\boldsymbol{X}^T\mathbf{y}$也是px1向量<br>$$<br>\frac{\partial 2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}}{\partial \boldsymbol{\beta }}=\frac{\partial 2\boldsymbol{\beta }^T\left( \boldsymbol{X}^T\mathbf{y} \right)}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>第3项的$\boldsymbol{X}^T\boldsymbol{X}$是p×p对称矩阵<br>$$<br>\frac{\partial \boldsymbol{\beta }^T\left( \boldsymbol{X}^T\boldsymbol{X} \right) \boldsymbol{\beta }}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>所以对$\boldsymbol{X}^T\boldsymbol{X}$求出来偏导是<br>$$<br>\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>令偏导等于零求解<br>$$<br>2\boldsymbol{X}^T\boldsymbol{X\beta }=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>约掉2，然后两边都乘$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}$（需要$\boldsymbol{X}^T\boldsymbol{X}$可逆）<br>$$<br>\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\boldsymbol{X\beta }=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\left( \boldsymbol{X}^T\boldsymbol{X} \right) =\boldsymbol{I}$，$\boldsymbol{I}$是一个pxp的单位矩阵，那么$\boldsymbol{I\beta }=\boldsymbol{\beta }$，所以求出来<br>$$<br>\hat{\beta}_{OLS}=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>Done！</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="Q1：什么函数能找到它的最小值？"><a href="#Q1：什么函数能找到它的最小值？" class="headerlink" title="Q1：什么函数能找到它的最小值？"></a>Q1：什么函数能找到它的最小值？</h3><p>令偏导为零能找到最小值？<br>对于残差平方和这个函数$L(\boldsymbol{\beta})$是的，这是一个凸函数。<br>一元函数$f\left( x \right) =\left( x-4 \right) ^2+3$，我们求一阶导数$f’\left( x \right) =2\left( x-4 \right)$，再求二阶导数$f’’\left( x \right) =2$，<font color='red'>二阶导大于零</font>，所以这个函数是凸函数，可以在导数等于零的时候找到最小值。<br>多元函数$L(\boldsymbol{\beta})$，一阶导刚才我们求出来，是$\displaystyle\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }$，继续求二阶导$\displaystyle\frac{\partial ^2L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }^2}=2\boldsymbol{X}^T\boldsymbol{X}$，求出来这个二阶导是个矩阵。跟一元函数类似的，<font color='red'>二阶导如果是一个正定矩阵（类比正实数）</font>，那么这个函数是凸函数，可以找到最小值。而$2\boldsymbol{X}^T\boldsymbol{X}$是一个正定矩阵，因为$\boldsymbol{X}$是满秩的（基于各变量X相互独立的基本假设）。</p>
<h3 id="Q2：-boldsymbol-X-T-boldsymbol-X-可逆？"><a href="#Q2：-boldsymbol-X-T-boldsymbol-X-可逆？" class="headerlink" title="Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？"></a>Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？</h3><p>$\boldsymbol{X}^T\boldsymbol{X}$可逆？<br>$\boldsymbol{X}^T\boldsymbol{X}$是正定矩阵  $\Longrightarrow\boldsymbol{X}^T\boldsymbol{X}$的行列式大于零<br>行列式$\ne 0\Longrightarrow$ 可逆</p>
<p>这些问题在「<a href="/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/" title="矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵">矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵</a>」这篇文章有详解，这里一两句话说不完。</p>
<p>$$ ===== Fin =====$$</p>
<p>参考资料：</p>
<ol>
<li><a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" target="_blank" rel="noopener">OLS in Matrix Form</a></li>
</ol>

      
      <!-- reward -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    
    
      <!-- copyright -->
      
        <div class="declare">
          <ul class="post-copyright">
            <li>
              <i class="ri-copyright-line"></i>
              <strong>版权声明： </strong s>
              本博客所有文章除特别声明外，均采用 <a href="https://www.apache.org/licenses/LICENSE-2.0.html" rel="external nofollow"
                target="_blank">Apache License 2.0</a> 许可协议。转载请注明出处！
            </li>
          </ul>
        </div>
        
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2020/03/06/OLS/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Machine-Learning/" rel="tag">Machine Learning</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Math/" rel="tag">Math</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵
          
        </div>
      </a>
    
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        app_id: 'IWtOhB4nNoD1F4BDWzQYMALe-gzGzoHsz',
        app_key: 'k0iOFayd99sSBGuohToahHe3',
        path: window.location.pathname,
        notify: 'false',
        verify: 'false',
        avatar: 'mp',
        placeholder: '有任何意见可以留言告诉我哦(〃^▽^〃) 也可以点侧边栏的 Me° 通过email联系我~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        an 橘
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/orange_logo.png" alt="Dream d&#39;Orange"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">· 起 始 ·</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/ML">机器学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Math">数 · 统</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">Tags</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Voyage">地平线上这个世界</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020">Me°</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>走过路过赏杯奶茶呗~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['砥砺名行，不疾不徐','',''],
    startDelay: 0,
    typeSpeed: 300,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer:'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">




<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

    
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>