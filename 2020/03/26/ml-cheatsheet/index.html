<!DOCTYPE html>
<html lang="en">

<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    Machine Learning CheatSheet |  Dream d&#39;Orange
  </title>
  <meta name="generator" content="hexo-theme-yilia-plus">
  
  <link rel="shortcut icon" href="/orange-favicon.ico" />
  
  
<link rel="stylesheet" href="/css/main.css">

  
<script src="/js/pace.min.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


  

  

</head>

</html>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-ml-cheatsheet" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  Machine Learning CheatSheet
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/26/ml-cheatsheet/" class="article-date">
  <time datetime="2020-03-26T09:08:11.000Z" itemprop="datePublished">2020-03-26</time>
</a>
      
  <div class="article-category">
    <a class="article-category-link" href="/categories/ML/">ML</a>
  </div>

      
      
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> 字数统计:</span>
            <span class="post-count">3.9k字</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> 阅读时长≈</span>
            <span class="post-count">15分钟</span>
        </span>
    </span>
</div>

      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    
    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><ol>
<li><p>KNN中的K如何选取的？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> #1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；</p>
<p> #2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</p>
<p> #3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</p>
<p> 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p>
 </details>








</li>
</ol>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ol>
<li><p>什么是最小二乘法？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 一般我们所说的最小二乘法，指的是普通最小二乘法（Ordinary Least Squares），只适用于线性回归问题。通过求解目标函数对参数的偏导函数，使得偏导函数为零，求出使得残差平方和最小化的最优解 $$\hat{\beta}^{\text{OLS}}=(X^TX)^{-1}X^Ty$$ 只有符合线性回归假设的问题才能用OLS求解。</p>
 </details>



</li>
</ol>
<ol start="2">
<li>以下关于最小二乘法正确的是<br> A. 最小二乘估计是线性有偏估计中方差最小的<br> B. 最小二乘估计是线性无偏估计中方差最小的<br> C. 最小二乘估计是线性有偏估计中方差最大的<br> D. 最小二乘估计是线性无偏估计中方差最大的 <details>
 <summary><u>答案：</u></summary>
 B.
 </details>   




</li>
</ol>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><ol>
<li><p>逻辑斯特回归为什么要对特征进行离散化？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
 <ol>
 <li>离散特征的增加和减少都很容易，易于模型的快速迭代；
 <li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
 <li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
 <li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
 <li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
 <li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
 <li>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
 </ol>

<p> 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
 </details>



</li>
</ol>
<ol start="2">
<li><p>简单介绍下logistics回归？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> $n$ 个样本：$\left( \mathbf{x}_1,y_1 \right) ,\cdots ,\left( \mathbf{x}_n,y_n \right)$<br> x 有p个特征：$\mathbf{x}_i=\left( x_{i}^{\left( 1 \right)},\cdots ,x_{i}^{\left( p \right)} \right) ,\ i=1,\cdots ,n$<br> y 只有两种取值0和1：$y_i=\left\{ 0,1 \right\}$ </p>
<p> Sigmoid函数：$$\sigma(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^z}$$</p>
<p> 逻辑回归，就是在线性回归的基础上，用sigmoid函数做非线性变换，使得预测值在0~1之间，作为分类 $y=1$ 的概率值。<br> $$\begin{aligned}\hat{y}_i&amp;=\mathbb{P}\left( y_i=1\ |\ \mathbf{x}_i \right) =\frac{1}{1+e^{-\mathbf{x}_i\boldsymbol{\beta }}}\\1-\hat{y}_i&amp;=\mathbb{P}\left( y_i=0\ |\ \mathbf{x}_i \right) =\frac{e^{-\mathbf{x}_i\boldsymbol{\beta }}}{1+e^{-\mathbf{x}_i\boldsymbol{\beta }}}\end{aligned}$$</p>
<p> Loss function $$L\left( \hat{y}_i,y_i \right) =-\left( y_i\log \hat{y}_i+\left( 1-y_i \right) \log \left( 1-\hat{y}_i \right) \right)$$</p>
<p> Cost function $$\begin{aligned}J\left( \boldsymbol{\beta } \right) &amp;=\frac{1}{n}\sum_{i=1}^n{L\left( \hat{y}_i,y_i \right)}\\\<br> &amp;=-\frac{1}{n}\sum_{i=1}^n{\left( y_i\log \hat{y}_i+\left( 1-y_i \right) \log \left( 1-\hat{y}_i \right) \right)}\\\<br> &amp;=-\frac{1}{n}\sum_{i=1}^n{\left( y_i\left( \mathbf{x}_i\boldsymbol{\beta } \right) -\log \left( 1+e^{\mathbf{x}_i\boldsymbol{\beta }} \right) \right)}\end{aligned}$$</p>
<p> 梯度下降法 $$\boldsymbol{\beta} ^{\left[ j+1 \right]}=\boldsymbol{\beta} ^{\left[ j \right]}-\alpha \nabla _{\boldsymbol{\beta }}J\left( \boldsymbol{\beta } \right)$$ $$\nabla _{\boldsymbol{\beta }}J\left( \boldsymbol{\beta } \right) =\frac{1}{n}\boldsymbol{X}^T\left( \sigma \left( \boldsymbol{X\beta } \right) -\mathbf{y} \right)$$</p>
 </details>



</li>
</ol>
<ol start="3">
<li><p>关于逻辑回归和 SVM 不正确的是<br> A. 逻辑回归目标函数是最小化后验概率<br> B. 逻辑回归可以用于预测事件发生概率的大小<br> C. SVM目标是结构风险最小化<br> D. SVM可以有效避免模型过拟合</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。<br> 逻辑回归目标是似然函数最大化，并不是后验概率最大化，加上正则项的逻辑回归才可以看作是后验概率最大化。<br> SVM的软间隔相当于是在经验风险最小化的基础上添加了正则化项，所以SVM应该是结构风险最小化。</p>
<ul>
<li>经验风险最小化（empirical risk minimization, ERM）$$\min_{f\in F}\frac{1}{n}\sum^n_{i=1}L(y_i,f(x_i))$$</li>
<li>结构风险最小化（structural minimization, SRM）$$\min_{f\in F}\left[\frac{1}{n}\sum^n_{i=1}L(y_i,f(x_i))+\lambda J(f)\right]$$</details>






</li>
</ul>
</li>
</ol>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ol>
<li>下列不是SVM核函数的是：<br> A. 多项式核函数<br> B. logistic核函数<br> C. 径向基核函数<br> D. Sigmoid核函数 <details>
 <summary><u>答案：</u></summary>
 选B。
 </details>



</li>
</ol>
<ol start="2">
<li><p>介绍一下核函数。</p>
 <details>
 <summary><u>答案：</u></summary>
 核技巧是构造一个核函数，可以表示输入低维空间的向量，经过某个变换后在高维空间里的向量内积。满足了Mercer条件的函数，都可以作为核函数，表示成向量内积。以下是几个常用的核函数：

<ul>
<li><p><b>线性核</b>：$K(x_i,x_j)=x_i^Tx_j$<br>$\hspace{1em}$这是SVM的原始形式那个内积，没做高维映射。</p>
</li>
<li><p><b>多项式核</b>：$K(x_i,x_j)=(\gamma x_i^Tx_j+b)^d$<br>$\hspace{1em}$类似于多项式回归，d是多项式的阶数。如果d较大计算量增大，所以阶数d不能取太高。</p>
</li>
<li><p><b>RBF核</b>，又叫径向基核或者高斯核：$K(x_i,x_j)=\exp(-\gamma\lVert x_i-x_j\rVert^2)$<br>$\hspace{1em}$应用最多的核函数，好处是只有一个参数，将样本映射到无穷维，对大样本或者小样本性能都较好。$\gamma$越大，高斯分布越窄；$\gamma$越小，高斯分布越宽。$\gamma$相当于调整模型的复杂度，$\gamma$越小模型复杂度越低；$\gamma$越高，模型复杂度越大。</p>
</li>
<li><p><b>sigmoid核</b>：$K(x_i,x_j)=\tanh(\gamma x_i^Tx_j+b)$<br>$\hspace{1em}$相当于神经网络。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>关于支持向量机SVM，下列说法错误的是<br> A. L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力<br> B. Hinge 损失函数，作用是最小化经验分类错误<br> C. 分类间隔为 $1/\lVert w\rVert$，$\lVert w\rVert$ 代表向量的模<br> D. 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。间隔是 $2/\lVert w\rVert$<br> A. 软间隔，有更强的泛化能力。<br> D. 软间隔加的正则项惩罚系数C（下图的 $\lambda$），即对误差的宽容度。C越高，说明越不能容忍出现误差，容易过拟合。C越小，容易欠拟合。</p>
 <img src="https://pic.downk.cc/item/5e8e87b8504f4bcb04e0e87d.png">
 </details>






</li>
</ol>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><ol>
<li><p>经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，这叫做拼写检查。根据谷歌一员工写的文章 (<a href="http://norvig.com/spell-correct.html" target="_blank" rel="noopener">http://norvig.com/spell-correct.html</a>) 显示，Google的拼写检查基于贝叶斯方法。请说说你的理解，具体Google是怎么利用贝叶斯方法，实现“拼写检查”的功能。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做 $c$（correct），拼写错误的情况记做 $w$（wrong），那么“拼写检查”要做的事情就是：在发生 $w$ 的情况下，试图推断出 $c$。换言之：已知 $w$，然后在若干个备选方案中，找出可能性最大的那个 $c$，也就是求 $P(c|w)$ 的最大值。<br> 根据贝叶斯定理，有：$$P(c|w)=\frac{P(w|c)P(c)}{P(w)}$$ 由于对于所有备选的 $c$ 来说，对应的都是同一个 $w$，所以它们的 $P(w)$ 是相同的，因此我们只要最大化 $P(w|c)P(c)$即可。其中：$P(c)$ 表示某个正确的词的出现“概率”，它可以用“频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，$P(c)$ 就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。<br> $P(w|c)$ 表示在试图拼写 $c$ 的情况下，出现拼写错误 $w$ 的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，$P(w|c)$ 就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见<a href="http://blog.csdn.net/v_july_v/article/details/8701148#t4" target="_blank" rel="noopener">http://blog.csdn.net/v_july_v/article/details/8701148#t4</a><br> 所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。</p>
 </details>



</li>
</ol>
<ol start="2">
<li>朴素贝叶斯“朴素”在哪里？ <details>
 <summary><u>答案：</u></summary>
 Naive Bayes的假设：所有特征都是相互独立的。而这个假设在现实中是不存在的，是很naive的假设。
 </details>



</li>
</ol>
<ol start="3">
<li><p>简单说说贝叶斯定理。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 贝叶斯公式 $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$</p>
<ul>
<li>先验概率 $P(A)$：预估事件A发生的概率</li>
<li>后验概率 $P(A|B)$：收集到在A条件下相关事件B发生的概率，借助这个实际发生的概率来“纠正”A的概率</li>
<li>标准似然度 $\displaystyle\frac{P(B|A)}{P(B)}$：<ul>
<li>标准似然度大于1，意味着“先验概率”被增强，事件A发生的可能性变大；</li>
<li>标准似然度等于1，意味着事件B无助于判断事件A的可能性；</li>
<li>标准似然度小于1，意味着“先验概率”被削弱，事件A的可能性变小。</details>



</li>
</ul>
</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p>Naive Bayes是一种特殊的Bayes分类器，特征变量是X，类别标签是C，它的一个假定是:<br> A. 各类别的先验概率P(C)是相等的<br> B. 以0为均值，sqr(2)/2为标准差的正态分布<br> C. 特征变量X的各个维度是类别条件独立随机变量<br> D. P(X|C)是高斯分布</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是：<br> A. 这个被重复的特征在模型中的决定作用会被加强<br> B. 模型效果相比无重复特征的情况下精确度会降低<br> C. 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。<br> D. 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题<br> E. NB可以用来做最小二乘回归<br> F. 以上说法都不正确</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选BD。NB的核心在于它假设向量的所有特征之间是独立的。重复特征破坏了这个假设，使得模型变差。</p>
 </details>



</li>
</ol>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习 (*)"></a>集成学习 (*)</h2><ol>
<li><p>随机森林、Boosting、AdaBoost、GBDT和XGBoost的区别。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <a href="https://xijunlee.github.io/2017/06/03/集成学习总结/" target="_blank" rel="noopener">https://xijunlee.github.io/2017/06/03/集成学习总结/</a></p>
 </details>



</li>
</ol>
<ol start="2">
<li>为什么XGBoost要用泰勒展开，优势在哪里？ <details>
 <summary><u>答案：</u></summary>
 目标函数难求解，运用泰勒公式展开原目标函数从而近似求解，只需要求一阶和二阶导，大大简化计算，更快速更准确。
 </details>



</li>
</ol>
<ol start="3">
<li>XGBoost如何寻找最优特征？是又放回还是无放回的呢？ <details>
 <summary><u>答案：</u></summary>
 XGBoost在生成树的时候，类似决策树的生成，不同的是以目标函数（也就是损失函数）作为特征选取标准。XGBoost利用梯度优化模型算法，样本是不放回的，但xgboost支持子采样，也就是每轮计算可以不使用全部样本。
 </details>



</li>
</ol>
<ol start="4">
<li><p>说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2…，请写出最终的决策公式。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <a href="https://blog.csdn.net/v_july_v/article/details/40718799" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/40718799</a></p>
 </details>



</li>
</ol>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><ol>
<li><p>影响聚类算法效果的主要原因中，不正确的是：<br> A. 特征选取<br> B. 模式相似性测度<br> C. 分类准则<br> D. 已知类别的样本质量</p>
 <details>
 <summary><u>答案：</u></summary>
 选D。聚类是对无类别的数据进行聚类，不使用已经标记好的数据。
 </details>
</li>
<li><p> 2.1. 为了初步观察不同性别说话人的语音特征的区别，甲尝试用聚类的方法对所获得的语音特征进行分析。在对语音数据做了简单的处理后，甲得到了部分对话中一共M句话的特征x1, x2, …, xM，其中每个xi均为N维实数向量。然后甲通过K-Means算法将这M句话聚类为两类。试写出针对该问题的K-Means聚类代码或伪代码</p>
 <details>
 <summary><u>答案：</u></summary>
 #1. Input: $X=(\mathbf{x}_1,\cdots,\mathbf{x}_N)$, $K$<br>
 #2. Initialize $K$ centroids: $c_1^{(0)},\cdots,c_K^{(0)}$<br>
 #3. for $i=1,\cdots,N$：<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $d_{ij}=\text{distance}(\mathbf{x}_i, c_j)$<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $k=\left\{j: \min\{d_{ij}\}\right\}$<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $\mathbf{x}_i\in$ cluster $k$<br>
 #4. for $j=1,\cdots,K$：<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $c_j^{new}=\text{mean}(\mathbf{x}_i, \text{for } \mathbf{x}_i\in \text{cluster }j)$<br>
 #5. repeat #3 #4, till $|{c^{old}-c^{new}}|<\varepsilon$, END。
 </details>

<p> 2.2. 聚类完成后，甲通过训练好的聚类模型对另外挑选的1000句话（包括800句男声说话与200句女声说话）的说话人性别进行了预测，预测结果如下：800句男声说话中预测正确了600句，200句女声说话中预测正确了100句。请从不同角度评价该分类结果的表现，并提出改进的建议。</p>
 <details>
 <summary><u>答案：</u></summary>
 1). 准确率：$\displaystyle\frac{600+100}{1000}=0.7$。

<p> 2). 两类样本的比例是8/2，分类不均衡。以男声作为正类 (1)，女声作为负类 (0)，混淆矩阵如下：</p>
 <img src="https://pic.downk.cc/item/5e82e9e4504f4bcb04fb800a.png"  width=250>
 召回率：$R=\displaystyle\frac{600}{600+200}=\displaystyle\frac{3}{4}$

<p> 精确率：$P=\displaystyle\frac{600}{600+100}=\displaystyle\frac{6}{7}$</p>
<p> 召回率R和精确率P都是越大越好。但往往当R较大，P会较小，反之亦然。而F1-score可以平衡两者$\displaystyle\frac{1}{F_1}=\displaystyle\frac{1}{2}(\displaystyle\frac{1}{P}+\displaystyle\frac{1}{R})$<br> $$F_1=\displaystyle\frac{2PR}{P+R}=0.8$$</p>
<p> 3). 真正例率：$TPR=\displaystyle\frac{600}{600+200}=\displaystyle\frac{3}{4}$，男声中，预测正确的概率是75%。<br> 假正例率：$FPR=\displaystyle\frac{100}{100+100}=\displaystyle\frac{1}{2}$，女声中，预测错误的概率是50%。<br> 对于女声样本，这个分类器的效果很差，把女声判断为男声和女声的概率是相等的。</p>
<p> 改进：由于样本分类不均衡使得模型效果不佳，可以在本身算法上做集成学习，若干个弱分类器集成可以增强分类器效果。</p>
 </details>

<p> 2.3. 甲认为上述聚类结果并不能作为很好的识别说话人性别的参考标准，所以决定采用神经网络构造说话人性别分类模型。他设计得网络结构如下：第一层Input Layer为输入层，输入x为N1维实向量；第二、第三层为隐藏层Hidden Layer，其中第二层有N2个节点，第三层有2个节点，每个节点均为一个实数；最后一层为输出层Output Layer，输出2维向量y=(Prob0, Prob1)，Prob0和Prob1分别代表对应输入x的语句说话人为女 (0) 和男 (1) 的概率。激活函数使用sigmoid函数，代价函数使用交叉熵函数。<br> 现在甲有M个训练样本(xi, Yi), i=1,2,…,M。其中xi为N1维向量，Yi为对应句子的实际说话人性别，若Yi=(0,1)表示说话人为男，若Yi=(1,0)表示说话人为女。试根据甲设计得网络结构，构造函数返回对这M个样本的单次前向传播后的平均代价函数值。</p>
 <details>
 <summary><u>答案：</u></summary>
 <img src="https://pic.downk.cc/item/5e82fc7d504f4bcb0409e049.png"  width=350>
 Input：$\mathbf{x}=(x_1,\cdots,x_{N1})$，Output：$y$是男声的概率Prob1，Prob0=1-Prob1
 $$\begin{aligned}
 &z^{[1]}=W^{[1]}\mathbf{x}+b^{[1]}, &&a^{[1]}=\sigma(z^{[1]})\\\\
 &z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}, &&a^{[2]}=\sigma(z^{[2]})\\\\
 &z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}, &&a^{[3]}=\sigma(z^{[3]})
 \end{aligned}
 $$
 单个样本$\mathbf{x}_i$的代价函数
 $$L(a^{[3](i)},y_i)=-(y_i\log a^{[3](i)}+(1-y_i)\log(1-a^{[3](i)}))$$
 M个样本的平均代价函数
 $$J=\frac{1}{M}\sum^M_{i=1}L(a^{[3](i)},y_i)$$
 </details>



</li>
</ol>
<ol start="3">
<li><p>K-means的复杂度？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> K-means算法：</p>
<blockquote>
<p>选择K个点作为初始质心<br> repeat：<br> $\hspace{2em}$将每个点指派到最近的质心，形成K个簇<br> $\hspace{2em}$重新计算每个簇的质心<br> until 簇不发生变化或达到最大迭代次数</p>
</blockquote>
<p> 时间复杂度：$O(tKmn)$，其中 t 为迭代次数，K 为簇的数目，m 为样本数，n 为维数<br> 空间复杂度：$O\left((m+K)n\right)$，其中 K 为簇的数目，m 为样本数，n 为维数</p>
<p> 关于时间复杂度和空间复杂度：<a href="https://zhuanlan.zhihu.com/p/50479555" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50479555</a></p>
 </details>




</li>
</ol>
<h2 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h2><ol>
<li><p>通常可以通过关联规则挖掘来发现啤酒和尿布的关系， 那么如果对于一条规则A →B, 如果同时购买A和B的顾客比例是4/7, 而购买A的顾客当中也购买了B的顾客比例是1/2, 而购买B的顾客当中也购买了A的顾客比例是1/3,则以下对于规则A →B的支持度(support)和置信度(confidence)分别是多少？<br> A. 4/7，1/3<br> B. 3/7，1/2<br> C. 4/7，1/2<br> D. 4/7，2/3</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。A-&gt;B<br> support = P(AB)<br> confidence = P(B|A)<br> lift = P(B|A)/P(B)</p>
 </details>



</li>
</ol>
<h2 id="监督，回归，分类"><a href="#监督，回归，分类" class="headerlink" title="监督，回归，分类"></a>监督，回归，分类</h2><ol>
<li>下列算法中，均为监督学习算法的为：<br> A. CRF，SVM，Logistic Regression<br> B. K-Means，KNN，决策树<br> C. 决策树，KNN，朴素贝叶斯<br> D. K-Means，SVM，朴素贝叶斯 <details>
 <summary><u>答案：</u></summary>
 选AC。聚类是对无类别的数据进行聚类，不使用已经标记好的数据。
 </details>



</li>
</ol>
<ol start="2">
<li>在分类问题中，我们经常会遇到正负样本数据量不等的情况，比如正样本为10w条数据，负样本只有1w条数据，以下最不合适的处理方法是：<br> A. 将负样本重复10次，生成10w样本量，打乱顺序参与分类<br> B. 直接进行分类，可以最大限度利用数据<br> C. 从10w正样本中随机抽取1w参与分类<br> D. 将负样本每个权重设置为10，正样本权重为1，参与训练过程 <details>
 <summary><u>答案：</u></summary>
 选B。A. 重采样，通过多次复制小样本，改变数据分布消除不平衡，可能导致过拟合；C. 欠采样，通过随机抽样减少多样本规模，提高少数类的分类性能，可能丢失多数类的重要信息；D. 权值调整。
 </details>



</li>
</ol>
<ol start="3">
<li><p>在K-Means或KNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 两个n维样本$\mathbf{x}=(x_1,\cdots,x_n), \mathbf{y}=(y_1,\cdots,y_n)$</p>
<p> 欧氏距离 $$d=\sqrt{\sum^n_{i=1}(x_i-y_i)^2}$$<br> 曼哈顿距离 $$d=\sum^n_{i=1}|x_i-y_i|$$<br> 在2维平面，欧氏距离就是两点间的直线距离，曼哈顿距离就是在x轴和y轴的距离之和。</p>
<p> 这两种距离都是明可夫斯基距离的特殊情况 $$d=\left(\sum^n_{i=1}|x_i-y_i|^p\right)^{1/p}$$<br> p=1时是曼哈顿距离，p=2时是欧氏距离。</p>
 </details>



</li>
</ol>
<ol start="4">
<li><p>线性分类器与非线性分类器的区别以及优劣。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。<br> 常见的线性分类器有：LR，贝叶斯分类，单层感知机，原始SVM<br> 常见的非线性分类器：决策树，RF，GBDT，多层感知机，SVM+非线性核</p>
<p> 线性分类器速度快、编程方便，但是可能拟合效果不会很好<br> 非线性分类器编程复杂，但是效果拟合能力强</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>以下哪些方法不可以直接来对文本分类？<br> A. Kmeans<br> B. 决策树<br> C. 支持向量机<br> D. KNN</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。A是聚类，其他都是分类算法。</p>
 </details>



</li>
</ol>
<ol start="6">
<li><p>在统计模式分类问题中，当先验概率未知时，可以使用 (*)<br> A. 最小最大损失准则<br> B. 最小误判概率准则<br> C. 最小损失准则<br> D. N-P判决</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选AD。</p>
<p> 先验概率未知，也就是说不能用生成模型，只能用判别模型。</p>
<p> A. <strong>最小最大损失准则</strong><br> 考虑p(wi)变化的条件下，是风险最小。</p>
<p> B. <strong>最小误判概率准则</strong><br> 判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识。</p>
<p> C. <strong>最小损失准则</strong><br> 在B的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为B需要先验概率，所以C也需要先验概率。</p>
<p> D. <strong>N-P判决</strong><br> 限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式。<br> 在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 </p>
<ol>
<li>p(y)已知，直接使用贝叶斯公式求后验概率即可； </li>
<li>p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。</details>



</li>
</ol>
</li>
</ol>
<ol start="7">
<li>以下不属于非监督学习的为<br> A. 关联规则<br> B. Kmeans<br> C. Word2vec<br> D. KNN <details>
 <summary><u>答案：</u></summary>
 选D。
 </details>





</li>
</ol>
<h2 id="过拟合，正则化"><a href="#过拟合，正则化" class="headerlink" title="过拟合，正则化"></a>过拟合，正则化</h2><ol>
<li><p>Overfitting怎么解决？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><b>Regularization</b><br>  l1正则化：（通过假设参数的先验分布为拉普拉斯分布，由最大后验概率估计导出。）<br>  $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}|\beta_j|\leq C$$<br>  l2正则化：（通过假设参数的先验分布为高斯分布，由最大后验概率估计导出。）<br>  $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}\beta_j^2\leq C$$ 假设只有两个参数$\beta_0, \beta_1$，可以在二维平面表示成这样  <img src="https://pic.downk.cc/item/5e845dd8504f4bcb040d5354.png" width=350>
  正则化对参数的限制：l1正则化是让个别参数近似于零，从而消除这些变量达到降维的效果，同时包含了特征选择和正则化；l2正则化则是控制每个参数的权重来调整参数，使得所有参数趋向零但不为零。</li>
<li><b>Dropout</b><br>  随机drop掉某些神经元，防止过拟合。<br>  Training：让神经元以$p$的概率被激活（也就是$1-p$的概率被设置为0），这样每个w都可以随机参与训练，模型泛化能力更强。  <img src="https://pic.downk.cc/item/5e8453d0504f4bcb0406611e.png" width=250>
  Test：在测试集不会dropout，要用所有的神经元，每个神经元需要乘上p作为权重。</li>
<li><b>Batch Normalization</b><br>  这个方法给每层的输出都做一次归一化（纵向规范化），使得下一层的输入接近高斯分布。因为在深度学习里，无论一开始的input有没有做归一化处理，随着层数的加深，隐藏层接收到的input value（上一层的output）也会产生大幅度的波动，BN就是scaling这些值。<br>  $$\begin{aligned}\mu_B&amp;=\frac{1}{m}\sum^m_{i=1}x_i\\\<br>  \sigma_B^2&amp;=\frac{1}{m}\sum^m_{i=1}(x_i-\mu_B)^2\\\<br>  \hat{x}_i&amp;=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\varepsilon}}\\\<br>  y_i&amp;=\gamma\hat{x}_i+\beta\end{aligned}$$ $\varepsilon$是为了让分母不为零，避免出现计算错误。$\gamma, \beta$两个参数对最后的结果进行缩放和平移，需要被学习。</li>
<li><b>交叉验证</b></li>
<li><b>Early Stopping</b><br>  结合cross-validation使用，当在测试集上连续的几次迭代中都出现损失上升，则训练停止。</li>
<li><b>数据集扩增</b></li>
<li><b>特征选择/特征降维</b></details>



</li>
</ul>
</li>
</ol>
<ol start="2">
<li><p>L1和L2的区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> L1正则化：（通过假设参数的先验分布为拉普拉斯分布，由最大后验概率估计导出。）<br> $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}|\beta_j|\leq C$$<br> L2正则化：（通过假设参数的先验分布为高斯分布，由最大后验概率估计导出。）<br> $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}\beta_j^2\leq C$$ 正则化对参数的限制：L1正则化是让个别参数近似于零，从而消除这些变量达到降维的效果，同时包含了特征选择和正则化；L2正则化则是控制每个参数的权重来调整参数，使得所有参数趋向零但不为零。    </p>
<p> 几何含义：<br> 假设只有两个参数$\beta_0, \beta_1$，可以在二维平面表示成这样</p>
 <img src="https://pic.downk.cc/item/5e845dd8504f4bcb040d5354.png" width=350>
 红色的等高线代表损失函数，等高线中心的红点代表损失函数取到最小值。

<p> 左图（L1正则化）：灰底方块表示惩罚项（绝对值之和）$$|\beta_0|+|\beta_1|\leq C$$<br> 右图（L2正则化）：灰底圆形表示惩罚项（平方和）$$\beta_0^2+\beta_1^2\leq C$$<br> 黑点（圆点）表示惩罚项取到最小值（$|\beta_0|+|\beta_1|=0$ 或 $\beta_0^2+\beta_1^2=0$），也就是零。方块（或圆形）与等高线相切的点，也就是蓝点，同时使得损失函数与惩罚项之和最小。 </p>
 </details>



</li>
</ol>
<ol start="3">
<li><p>L1和L2正则先验分别服从什么分布？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> L1是Laplace先验分布，L2是高斯先验分布。<br> <a href="https://blog.csdn.net/m0_38045485/article/details/82147817" target="_blank" rel="noopener">https://blog.csdn.net/m0_38045485/article/details/82147817</a>  </p>
 </details>






</li>
</ol>
<h2 id="梯度下降法，EM算法，牛顿法，拟牛顿法"><a href="#梯度下降法，EM算法，牛顿法，拟牛顿法" class="headerlink" title="梯度下降法，EM算法，牛顿法，拟牛顿法"></a>梯度下降法，EM算法，牛顿法，拟牛顿法</h2><ol>
<li><p>SGD中 S (stochastic) 代表什么?</p>
 <details>
 <summary><u>答案：</u></summary>

<p> BGD (Batch Gradient Descent) 每次迭代都用全部样本来计算。而SGD (Stochastic Gradient Descent) 每次迭代只随机选取一个样本来计算。<br> BGD的优点：全局最优解；易于并行实现；缺点：当样本数目很多时，训练过程会很慢。<br> SGD的优点：训练速度快；缺点：准确度下降，并不是全局最优；不易于并行实现。</p>
 </details>



</li>
</ol>
<ol start="2">
<li><p>请简要说说EM算法。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 有时候因为样本的产生和隐含变量有关（隐含变量不能被观察），求模型的参数时一般采用极大似然估计，由于有隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数，EM算法一般分为2步：</p>
<ul>
<li><p>E步：选取一组参数，求出在该参数下隐含变量的条件概率值；</p>
</li>
<li><p>M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。<br>重复上面2步直至收敛。</p>
<p>(E-step) For i: $$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$$ (M-step) $$\theta:=\arg\max_\theta\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>说说梯度下降法。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法的步骤，是让参数值往梯度下降最快的方向迭代，从而快速达到目标函数的最小值。<br> 比如我们要最小化 cost function $J(\theta)$，梯度下降法的步骤是先选取参数的初始值，还要设定一个超参数步长$\alpha$，每次迭代用当前的参数减去梯度乘以$\alpha$值，$\alpha$的大小决定往梯度下降最快的方向跨多大一步，直到收敛。<br> #1. 选定$\alpha$，初始化参数值$\theta^{(0)}$<br> #2. 迭代参数值 $\theta^{(t+1)}:=\theta^{(t)}-\alpha\nabla_\theta J$<br> 直到 $|\theta^{(t+1)}-\theta^{(t)}|&lt;\varepsilon$</p>
<p> 缺点：</p>
<ul>
<li>对于步长的选择需要慎重，$\alpha$过大，参数值动荡很大，会错过最小值；如果$\alpha$过小，计算的时间会变长。</li>
<li>当样本量很大时，梯度的计算非常耗时。</details>



</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p>梯度下降法找到的一定是下降最快的方向么？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（二维参数）上下降最快的方向。<br> 牛顿方向（Hessian矩阵）才一般被认为是下降最快的方向，可以达到Superlinear的收敛速度。梯度下降类的算法的收敛速度一般是Linear甚至Sublinear的（在某些带复杂约束的问题）。<br> 梯度下降法比牛顿法更常用，是因为数据较大时要计算Hessian矩阵非常耗时，而梯度下降法的收敛速度已经足够了。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>牛顿法和梯度下降法有什么不同？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法迭代公式： $\theta^{(t+1)}:=\theta^{(t)}-\alpha\nabla_\theta J$<br> 牛顿法迭代公式： $\theta^{(t+1)}:=\theta^{(t)}-\alpha H^{-1}\nabla_\theta J$</p>
<p> 梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的Hessian矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长，因为要计算Hessian矩阵的逆。梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。</p>
 </details>



</li>
</ol>
<ol start="6">
<li><p>什么是拟牛顿法？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。<br> <a href="https://zhuanlan.zhihu.com/p/37524275" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37524275</a></p>
 </details>



</li>
</ol>
<ol start="7">
<li><p>函数 $f(\theta_0,\theta_1)$ 输出一个数。$f$ 未知，且平滑。假设我们用梯度下降法求解 $\min f(\theta_0,\theta_1)$。下面哪些描述是对的：<br> A. 如果开始的几次迭代使得函数值不减反增，可能是因为步长 $\alpha$ 设置得太大<br> B. 如果 $\theta_0,\theta_1$ 初始值就在函数的全局最小，那迭代不会改变它们的值<br> C. 把 $\alpha$ 设得很小没什么坏处，只会加快收敛速度<br> D. 无论 $\theta_0,\theta_1$ 初始值是多少，只要 $\alpha$ 足够小，梯度下降法就能收敛到同一个结果</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选AB。<br> C. $\alpha$ 设得很小会使得收敛速度过慢。<br> D. 初始值选取不同，可能会收敛到不同的局部最小值。</p>
 </details>



</li>
</ol>
<ol start="8">
<li><p>假设对于线性回归问题 $h_\theta(x)=\theta_0+\theta_1x$，训练某个训练集能找到使得 $J(\theta_0,\theta_1)=0$ 的 $\theta_0,\theta_1$。下面描述正确的是：<br> A. 可以用一条直线完美拟合训练集<br> B. A成立的条件，必须有 $y^{(i)}=0, \forall i$<br> C. 梯度下降法可能会到达一个局部最小值，到不了全局最小值<br> D. C成立的条件，必须有 $\theta_0=0,\theta_1=0$ 使得 $h_\theta(x)=0$</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。<br> A. 对于完美无噪音的训练集可能存在。<br> C. $J$ 是凸函数，只有一个最小值，求到的就是全局最优解。</p>
 </details>



</li>
</ol>
<ol start="9">
<li><p>说说共轭梯度法？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。<br> <a href="https://blog.csdn.net/qq547276542/article/details/78186050" target="_blank" rel="noopener">https://blog.csdn.net/qq547276542/article/details/78186050</a></p>
 </details>








</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li><p>谈谈判别式模型和生成式模型？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><p>判别式模型：由数据直接学习决策函数 $Y = f(X)$，或者由条件分布概率 $P(Y|X)$ 作为预测模型，即判别模型。比如KNN、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、LR、boosting、CRF。</p>
</li>
<li><p>生成式模型：由数据学习联合概率密度分布函数 $P(X,Y)$，然后求出条件概率分布 $P(Y|X)$ 作为预测的模型，即生成模型。比如NB、HMM、高斯混合模型、LDA主题模型、限制玻尔兹曼机。</p>
<p>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="2">
<li><p>机器学习中，为何要对数据做归一化？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> #1. 归一化能提高梯度下降法求解<br> #2. 归一化有可能提高精度，特别对于要计算距离的KNN、K-Means这种算法</p>
<ul>
<li><p>（归一化）最值归一化：把所有数据映射到 0-1 之间（适用于有明显边界的情况）$$x_{scale}=\frac{x-\min}{\max-\min}$$ <code>sklearn.preprocessing.MinMaxScaler</code></p>
</li>
<li><p>（标准化）均值方差归一化：把所有数据归一到均值为0、方差为1的分布中（没有明显边界；有可能存在极端数据值）$$x_{scale}=\frac{x-\mu}{\sigma}$$ <code>sklearn.preprocessing.StandardScaler</code></p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>哪些机器学习算法不需要做归一化处理？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林。</p>
 </details>



</li>
</ol>
<ol start="4">
<li><p>对于树形结构为什么不需要归一化？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。<br> 另外注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>协方差和相关性有什么区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><p>协方差 $$Cov(X,Y)=\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$$ 用来衡量两个变量是正相关，还是负相关：</p>
<ul>
<li>正相关：X变大，Y也变大，协方差为正；</li>
<li>负相关：X变大，Y变小，协方差为负。</li>
</ul>
<p>$Cov(X,Y)=0 \Longrightarrow$ X，Y不相关<br>X，Y相互独立 $\Longrightarrow Cov(X,Y)=0$</p>
</li>
<li><p>相关系数 $$\rho=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$ 可以看做是协方差的标准化，取值范围是 $[-1,1]$。根据定义和协方差一样，正值表示正相关，负值表示负相关。$|\rho|$ 越大（接近1）相关性越强，越小（接近0）则越不相关。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="6">
<li><p>熵、联合熵、条件熵、相对熵、互信息的定义。</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li>熵：衡量不确定性（熵越大，不确定性越大，包含信息量越多） $$H(X)=-\sum_xp(x)\log p(x)$$</li>
<li>联合熵：(X,Y)一起出现的不确定性 $$H(X,Y)=-\sum_{x,y}p(x,y)\log p(x,y)$$</li>
<li>条件熵：X确定时，Y的不确定性 $$\begin{aligned}H(Y|X)<br>&amp;=\sum_xp(x)H(Y|X=x)\\\<br>&amp;=-\sum_xp(x)\sum_yp(x|y)\log p(y|x)\\\<br>&amp;=-\sum_x\sum_yp(x,y)\log p(y|x)\\\<br>&amp;=-\sum_{x,y}p(x,y)\log p(y|x)\\\<br>&amp;=-\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)}<br>\end{aligned}$$ $$H(Y|X)=H(X,Y)-H(X)$$</li>
<li>互信息 $$I(X,Y)=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$$ $$I(X,Y)=H(X)+H(Y)-H(X,Y)$$</li>
<li>交叉熵：衡量两个概率分布的相似度 $$H(p,q)=-\sum_xp(x)\log q(x)$$</li>
<li>相对熵（KL散度）：衡量两个概率分布的距离（不相似度） $$D(p||q)=\sum_xp(x)\log\frac{p(x)}{q(x)}$$ $$H(p,q)=H(p)+D(p||q)$$</details>




</li>
</ul>
</li>
</ol>
<ol start="7">
<li><p>什么是分布式数据库？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 分布式数据库系统是在集中式数据库系统成熟技术的基础上发展起来的，但不是简单地把集中式数据库分散地实现，它具有自己的性质和特征。集中式数据库系统的许多概念和技术，如数据独立性、数据共享和减少冗余度、并发控制、完整性、安全性和恢复等在分布式数据库系统中都有了不同的、更加丰富的内容。</p>
 </details>



</li>
</ol>
<ol start="8">
<li><p>#include和#include“filename.h”有什么区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 用 #include 格式来引用标准库的头文件（编译器将从标准库目录开始搜索）。<br> 用 #include “filename.h” 格式来引用非标准库的头文件（编译器将从用户的工作目录开始搜索）。</p>
 </details>



</li>
</ol>
<ol start="9">
<li>某超市研究销售纪录数据后发现，买啤酒的人很大概率也会购买尿布，这种属于数据挖掘的哪类问题？<br> A. 关联规则发现<br> B. 聚类<br> C. 分类<br> D. 自然语言处理 <details>
 <summary><u>答案：</u></summary>
 选A。
 </details>



</li>
</ol>
<ol start="10">
<li>将原始数据进行集成、变换、维度规约、数值规约是在以下哪个步骤的任务？<br>A. 频繁模式挖掘<br>B. 分类和预测<br>C. 数据预处理<br>D. 数据流挖掘<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="11">
<li>下面哪种不属于数据预处理的方法？<br>A. 变量代换<br>B. 离散化<br>C. 聚集<br>D. 估计遗漏值<details>
<summary><u>答案：</u></summary>
选D。
</details>



</li>
</ol>
<ol start="12">
<li>什么是KDD？<br>A. 数据挖掘与知识发现<br>B. 领域知识发现<br>C. 文档知识发现<br>D. 动态知识发现<details>
<summary><u>答案：</u></summary>
选A。
</details>



</li>
</ol>
<ol start="13">
<li>当不知道数据所带标签时，可以使用哪种技术促使带同类标签的数据与带其他标签的数据相分离？<br>A. 分类<br>B. 聚类<br>C. 关联分析<br>D. 隐马尔可夫链<details>
<summary><u>答案：</u></summary>
选A。
</details>



</li>
</ol>
<ol start="14">
<li>建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？<br>A. 根据内容检索<br>B. 建模描述<br>C. 预测建模<br>D. 寻找模式和规则<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="15">
<li>以下哪种方法不属于特征选择的标准方法：<br>A. 嵌入<br>B. 过滤<br>C. 包装<br>D. 抽样<details>
<summary><u>答案：</u></summary>
选D。
</details>



</li>
</ol>
<ol start="16">
<li><p>说下红黑树的五个性质。</p>
<details>
<summary><u>答案：</u></summary>

<p>#1. 每个结点要么是红的要么是黑的。<br>#2. 根结点是黑的。<br>#3. 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。<br>#4. 如果一个结点是红的，那么它的两个儿子都是黑的。<br>#5. 对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。</p>
<p><a href="https://blog.csdn.net/v_july_v/article/details/6105630" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/6105630</a></p>
</details>



</li>
</ol>
<ol start="17">
<li><p>简述下什么是生成对抗网络。（*）</p>
<details>
<summary><u>答案：</u></summary>

<p>GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的；另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。<br>更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。</p>
</details>



</li>
</ol>
<ol start="18">
<li><p>快速排序算法。</p>
<details>
<summary><u>答案：</u></summary>

<p>算法原理参考：<a href="https://wiki.jikexueyuan.com/project/easy-learn-algorithm/fast-sort.html" target="_blank" rel="noopener">https://wiki.jikexueyuan.com/project/easy-learn-algorithm/fast-sort.html</a> （选第一个元素做pivot）<br>代码参考：<a href="https://www.geeksforgeeks.org/python-program-for-quicksort/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/python-program-for-quicksort/</a> （选最后一个元素做pivot）</p>
<p>快排是把一个序列分为较小和较大的2个子序列，然后递归地排序两个子序列。步骤是：</p>
<ol>
<li>挑选基准值：从数列中挑出一个元素，称为“基准”（pivot）；
<li>分割（partition）：重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（与基准值相等的数可以到任何一边）。在这个分割结束之后，对基准值的排序就已经完成；
<li>递归排序子序列：递归地将小于基准值元素的子序列和大于基准值元素的子序列排序。
</ol>

<p>选择基准的方式有几种：</p>
<ul>
<li>第一个元素
<li>最后一个元素
<li>随机选择一个元素
<li>中位数
</ul>    

<p>假如选第一个元素作为pivot：</p>
<img src="https://pic.downk.cc/item/5e8d3e67504f4bcb04d5906a.png" width=350>
<img src="https://pic.downk.cc/item/5e8d3e8f504f4bcb04d5a708.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3e9f504f4bcb04d5b03c.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3eab504f4bcb04d5b750.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3eba504f4bcb04d5c098.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3ec8504f4bcb04d5c976.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3ed4504f4bcb04d5d0bf.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3edf504f4bcb04d5d751.png" width=300>

<p>伪代码：</p>
<pre><code>// low --&gt; Starting index, high --&gt; Ending index
quickSort(arr[], low, high)
{
    if (low &lt; high)
    {
        // pi is partitioning index, arr[pi] is now at right place
        pi = partition(arr, low, high);

        quickSort(arr, low, pi - 1);    // Before pi
        quickSort(arr, pi + 1, high);   // After pi
    }
}</code></pre><p>* 其实Python的<code>sorted</code>函数就可以排序，快得一批。</p>
</details>



</li>
</ol>
<ol start="19">
<li><p>说说常见的损失函数。</p>
<details>
<summary><u>答案：</u></summary>

<ol>
<li><b>0-1 损失函数</b>$$ L\left(Y,f(X)\right)=\left\{
\begin{aligned}
&1, && Y\ne f(X)\\\\
&0, && Y=f(X)
\end{aligned}\right.$$ 0-1 损失是用在分类问题上，也就是分类对了损失为零，分类错了损失为1，但它是非凸函数，难以求解，不太适用。

<li><b>绝对值损失函数</b>$$L\left(y,f(X)\right)=\lvert y-f(X)\rvert$$ 这个也是非凸函数，用得比较少。<br>
绝对平均误差（Mean Absolute Error, MAE）$$\text{MAE}=\frac{1}{n}\sum^n_{i=1}\lvert y_i-f(X_i)\rvert$$ 对异常值的敏感度较低，最小化MAE得到是y的中位数。

<li><b>平方损失函数</b>$$L\left(y,f(X)\right)=\left(y-f(X)\right) ^2$$ 这个是凸函数，在回归模型比较常用。<br>
均方误差（Mean Squared Error, MSE）$$\text{MSE}=\frac{1}{n}\sum^n_{i=1}\left(y_i-f(X_i)\right)^2$$ 最小化MSE得到的是y的均值。

<li><b>log对数损失函数（交叉熵）</b>$$L\left(y,P(y|X)\right)=-\log P(y|X)$$ log损失适用于表示概率分布，适用于分类问题，相对Hinge损失它会对噪声更敏感。逻辑回归的损失函数就是交叉熵 $$\text{cross entropy}=\frac{1}{n}\sum^n_{i=1}\left\{-y_i\log{f(X_i)}-(1-y_i)\log(1-f(X_i))\right\}$$

<li><b>Hinge损失函数</b>$$L\left(y,f(X))\right)=\max(0,1-yf(X))$$ Hinge损失适用于分类问题，如果分类正确损失为0，分类错误损失为 $1-yf(X)$。SVM的损失函数就是用的这个。    
</ol>
</details>



</li>
</ol>
<ol start="20">
<li><p>交叉熵函数与最大似然函数的联系和区别？</p>
<details>
<summary><u>答案：</u></summary>

<p>区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。</p>
<p>联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说<strong>最小化交叉熵函数的本质就是对数似然函数的最大化。</strong><br>给一个服从Bernoulli分布变量 $X\sim\mathcal{B}(p)$ $$\begin{aligned}&amp;P(X=1)=p\\&amp;P(X=0)=1-p\end{aligned}$$ $$P(X)=p^X(1-p)^{1-X}$$ 假设我们有数据样本 $D$，最大化下面的对数似然函数 $$\begin{aligned}\log P(D)&amp;=\log \prod^N_i P(D_i)=\sum_i\log p(D_i)\\&amp;=\sum_i(D_i\log p+(1-D_i)\log(1-p))\end{aligned}$$ 所以我们是要 $\max\left\{ D_i\log p+(1-D_i)\log(1-p)\right\}$，等同于 $\min\left\{-\left(D_i\log p+(1-D_i)\log(1-p)\right)\right\}$，那就是最小化交叉熵损失函数咯。</p>
</details>



</li>
</ol>
<ol start="21">
<li><p>计算三个稠密矩阵 A、B、C 的乘积 ABC，假定三个矩阵的尺寸分别为 m*n, n*p,p*q，且 m&lt;n&lt;p&lt;q，以下计算效率最高的是<br>A. (AB)C<br>B. A(BC)<br>C. (AC)B<br>D. (BC)A</p>
<details>
<summary><u>答案：</u></summary>

<p>选A。首先C和D是错误的，乘不了的。<br><strong>选项A的计算量</strong><br>(AB)：A的行（每行n个数）乘以B的列（每列n个数）–&gt; n次乘法，相加 –&gt; n-1次加法，总共 2n-1 次运算。然后A的m行乘B的p列，(AB)的运算次数总共是 mp(2n-1)。<br>(AB)得到一个 m*p 的矩阵，再乘C，运算次数是 mq(2p-1)。选项A的计算量是 <strong>mp(2n-1)+mq(2p-1)</strong>。<br><strong>选项B的计算量</strong><br>(BC)的计算量是 nq(2p-1)，得到 n*q 矩阵，与A相乘，计算量是 mq(2n-1)。选项B的计算量是 <strong>nq(2p-1)+mq(2n-1)</strong>。</p>
<p>对比一下，选项B的计算量比较大【mp(2n-1)&lt;mq(2n-1)，mq(2p-1)&lt;nq(2p-1)】，所以选A。</p>
</details>



</li>
</ol>
<ol start="22">
<li><p>已知一组数据的协方差矩阵P，下面关于主分量说法错误的是<br>A. 主分量分析的最佳准则是对一组数据进行按一组正交基分解，在只取相同数量分量的条件下，以均方误差计算截尾误差最小<br>B. 在经主分量分解后，协方差矩阵成为对角矩阵<br>C. 主分量分析就是K-L变换<br>D. 主分量是通过求协方差矩阵的特征值得到</p>
<details>
<summary><u>答案：</u></summary>

<p>选C。K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。KLT是一种对于连续或离散的随机过程都可进行的变换，PCA则是KLT处理离散情况的算法；定义上KLT比PCA广泛，而实际上PCA比KLT实用。</p>
</details>



</li>
</ol>
<ol start="23">
<li><p>模式识别中，马式距离较之于欧式距离的优点是<br>A. 平移不变性<br>B. 旋转不变性<br>C. 尺度不变性<br>D. 考虑了模式的分布</p>
<details>
<summary><u>答案：</u></summary>

<p>选CD。<br>欧式距离特性有：平移不变性、旋转不变性。$$d_E(\mathbf{x},\mathbf{y})=\sqrt{(\mathbf{x}-\mathbf{y})^T(\mathbf{x}-\mathbf{y})}$$<br>马式距离特性有：平移不变性、旋转不变性、尺度缩放不变性、不受量纲影响、考虑了模式的分布。$$d_M(\mathbf{x},\mathbf{y})=\sqrt{(\mathbf{x}-\mathbf{y})^T\Sigma^{-1}(\mathbf{x}-\mathbf{y})}$$ $\Sigma$是$\mathbf{x}$和$\mathbf{y}$协方差矩阵。</p>
</details>



</li>
</ol>
<ol start="24">
<li><p>下面程序的功能是输出数组的全排列,选择正确的选项,完成其功能。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">perm</span><span class="params">(<span class="keyword">int</span> <span class="built_in">list</span>[], <span class="keyword">int</span> k, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (▊▊▊)</span><br><span class="line">&#123;</span><br><span class="line">    copy(<span class="built_in">list</span>,<span class="built_in">list</span>+m,ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>,<span class="string">" "</span>));</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=k; i&lt;=m; i++)</span><br><span class="line">&#123;</span><br><span class="line">    swap(&amp;<span class="built_in">list</span>[k],&amp;<span class="built_in">list</span>[i]);</span><br><span class="line">    (▊▊▊);</span><br><span class="line">    swap(&amp;<span class="built_in">list</span>[k],&amp;<span class="built_in">list</span>[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>A. k!=m 和 perm(list,k+1,m)<br>B. k==m 和 perm(list,k+1,m)<br>C. k!=m 和 perm(list,k,m)<br>D. k==m 和 perm(list,k,m)</p>
<details>
<summary><u>答案：</u></summary>
选B。k输入时为0，递归直到k=m结束。
第一个swap是依次将第k个元素和k~m个元素交换，交换完后进入递归，再全排列其子数组。递归调用，之所以从k+1开始，是因为如果传入的是k的话，递归传进去的参数不变，递归将永无止境的递归下去，因为k永远不会等于m。
第二个swap是将该层交换完后的数组再还原，目的是为了使递归返回后不改变上一层的数组元素顺序，方便下一次交换。
</details>



</li>
</ol>
<ol start="25">
<li>若有33个长度不等的初始归并段，做7路平衡归并排序，为组织最佳归并树，应增加长度为0的初始归并段的个数是<strong>____</strong>。<br>A. 0<br>B. 2<br>C. 4<br>D. 6<details>
<summary><u>答案：</u></summary>
(33+2)/7=5;
(5+2)/7=1;
2+2=4.选C
</details>



</li>
</ol>
<ol start="26">
<li>将一个整数序列整理为升序，两趟处理后序列变为10,12,21,9,7,3,4,25，则采用的排序算法可能是<strong>____</strong>。<br>A. 插入排序<br>B. 选择排序<br>C. 快速排序<br>D. 堆排序<details>
<summary><u>答案：</u></summary>
选A。
B错，排两次应该前两个数最小。
C错，快排每次选一个“基准”，左边全部小于基准，右边全部大于基准。
D错，第一趟就应该使得第一个非叶子节点len/2-1=3 (9) 比 2*3+1=7 (25) 大。
第一个非叶子节点 i=len/2-1，与它的子节点 2i+1, 2i+2 对比。
</details>



</li>
</ol>
<ol start="27">
<li>在机器学习任务中经常假设矩阵为n×n的对称矩阵A， 则以下说法正确的是<br>A. 对称矩阵为满秩矩阵<br>B. 对称矩阵的列向量之间正交<br>C. 对应于A的不同特征值的特征向量之间正交<br>D. 对应于A的相同特征值得特征向量之间正交<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="28">
<li>有一类二叉树用三叉链表来存储的时候除了带有指向左右孩子节点的两个指针，还有指向父节点的指针，那么这样一棵二叉树有2个节点，那么有多少指针指向NULL（注：根节点的父指针指向NULL，对于不存在的节点表示为NULL）？<br>A. 1<br>B. 2<br>C. 3<br>D. 4<details>
<summary><u>答案：</u></summary>
选D。
<img src='https://pic.downk.cc/item/5eb55c59c2a9a83be5b85ede.png' width=400>
</details>



</li>
</ol>
<ol start="29">
<li>下列最短路径算法的叙述中正确的是（）<br>A. Dijkstra算法通常用于求每一对顶点间的最短路径；<br>B. Dijkstra算法不允许图中带有负权值的边，而Floyd算法则可以适用；<br>C. Floyd算法通常用于求某一顶点到其他各顶点的最短路径；<br>D. Floyd算法允许有包含负权值的边组成的回路，而Dijkstra算法不允许；<details>
<summary><u>答案：</u></summary>
选B。
Dijkstra算法是计算图中的一个点到其它点的最小路径.
Floyd算法计算图中任意一对点的最短路径.
</details>



</li>
</ol>
<ol start="30">
<li><p>判断一个数组或序列是正序,倒序还是乱序,需要我们将这个数组完整的遍历一遍通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应的位置并插入的排序算法是（ ）<br>A. 选择排序<br>B. 希尔排序<br>C. 插入排序<br>D. 归并排序</p>
<details>
<summary><u>答案：</u></summary>

<p>选C。<br>选择排序：每次从数组中选出一个最小数（最大数）放到数组最前面，存放在序列的起始位置，直到全部待排序的数据元素排完。<br>希尔排序：设置增量分割数组，逐步进行直接插入排序，增量逐趟减少，并最后使得整个数组基本有序，再对整体进行直接插入排序。<br>插入排序：构建有序序列，未排序数据依次从已排序数据按从后往前比较，插入到合适的位置。<br>归并排序：把序列分成两个长度为n/2的子序列，对这两个子序列分别归并排序（循环将两个数组的第一个值比较，并弹出第一个值，直到数组长度都不存在），将两个排序好的子序列合并成一个最终的排序序列。</p>
</details>



</li>
</ol>
<ol start="31">
<li><p>字符串有5个字符q,w,e,r,t，出现的频率分别为1,2,3,4,5，如果采用Huffman编码对字符串编码，则每个字符编码的平均长度是（）?<br>A. 2.2<br>B. 2.4<br>C. 2.6<br>D. 2.8<br>E. 3.0</p>
<details>
<summary><u>答案：</u></summary>

<p>选B。</p>
<img src='https://pic.downk.cc/item/5eb6b5a3c2a9a83be5273cb8.png' width=250>

<table>
<thead>
<tr>
<th align="center">字符</th>
<th align="center">编码</th>
<th align="center">长度</th>
</tr>
</thead>
<tbody><tr>
<td align="center">q</td>
<td align="center">010</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">w</td>
<td align="center">011</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">e</td>
<td align="center">00</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">r</td>
<td align="center">10</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">t</td>
<td align="center">11</td>
<td align="center">2</td>
</tr>
</tbody></table>
<p>(3+3+2+2+2)/5=2.4</p>
</details>




</li>
</ol>

      
      <!-- reward -->
      
      <div id="reward-btn">
        打赏
      </div>
      
    </div>
    
    
      <!-- copyright -->
      
        <div class="declare">
          <ul class="post-copyright">
            <li>
              <i class="ri-copyright-line"></i>
              <strong>版权声明： </strong s>
              本博客所有文章除特别声明外，均采用 <a href="https://www.apache.org/licenses/LICENSE-2.0.html" rel="external nofollow"
                target="_blank">Apache License 2.0</a> 许可协议。转载请注明出处！
            </li>
          </ul>
        </div>
        
    <footer class="article-footer">
      
          
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=http://yoursite.com/2020/03/26/ml-cheatsheet/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CheatSheet/" rel="tag">CheatSheet</a></li></ul>


    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/03/30/Python-CheatSheet/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            Python CheatSheet
          
        </div>
      </a>
    
    
      <a href="/2020/03/24/Lagrange-Multipliers/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">Lagrange_Multipliers</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        app_id: 'IWtOhB4nNoD1F4BDWzQYMALe-gzGzoHsz',
        app_key: 'k0iOFayd99sSBGuohToahHe3',
        path: window.location.pathname,
        notify: 'false',
        verify: 'false',
        avatar: 'mp',
        placeholder: '有任何意见可以留言告诉我哦(〃^▽^〃) 也可以点侧边栏的 Me° 通过email联系我~',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2020
        an 橘
      </li>
      <li>
        
        Powered by
        
        
        <a href="https://hexo.io" target="_blank">Hexo</a> Theme <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <span>
  <i>PV:<span id="busuanzi_value_page_pv"></span></i>
  <i>UV:<span id="busuanzi_value_site_uv"></span></i>
</span>
        
      </li>
      
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    <aside class="sidebar">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/orange_logo.png" alt="Dream d&#39;Orange"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">· 起 始 ·</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/ML">机器学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Math">数 · 统</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">Tags</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories/Voyage">地平线上这个世界</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2020">Me°</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>走过路过赏杯奶茶呗~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.png">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<script src="/js/share.js"></script>



<script src="/fancybox/jquery.fancybox.min.js"></script>




<script>
  try {
    var typed = new Typed("#subtitle", {
    strings: ['砥砺名行，不疾不徐','',''],
    startDelay: 0,
    typeSpeed: 300,
    loop: true,
    backSpeed: 100,
    showCursor: true
    });
  } catch (err) {
  }
  
</script>




<script src="/js/tocbot.min.js"></script>

<script>
  // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
  tocbot.init({
    tocSelector: '.tocbot',
    contentSelector: '.article-entry',
    headingSelector: 'h1, h2, h3, h4, h5, h6',
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer:'main',
    positionFixedSelector: '.tocbot',
    positionFixedClass: 'is-position-fixed',
    fixedSidebarOffset: 'auto',
    onClick: (e) => {
      $('.toc-link').removeClass('is-active-link');
      $(`a[href=${e.target.hash}]`).addClass('is-active-link');
      $(e.target.hash).scrollIntoView();
      return false;
    }
  });
</script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">




<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>

    
  </div><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>

</html>