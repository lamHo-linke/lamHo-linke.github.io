<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>矩阵的一些事</title>
    <url>/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/</url>
    <content><![CDATA[<p>还没写好，稍等~</p>
]]></content>
  </entry>
  <entry>
    <title>白痴版普通最小二乘法OLS的求解过程</title>
    <url>/2020/03/06/%E7%99%BD%E7%97%B4%E7%89%88%E6%99%AE%E9%80%9A%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95OLS%E7%9A%84%E6%B1%82%E8%A7%A3%E8%BF%87%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="1-一元线性回归"><a href="#1-一元线性回归" class="headerlink" title="1. 一元线性回归"></a>1. 一元线性回归</h2><p>模型：</p>
<p>$$\mathbf{y}\approx\beta _0+\beta _1\mathbf{x}$$ </p>
<p>$$\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix},<br>\quad\mathbf{x}=\begin{pmatrix}x_1\\x_2\\\vdots\\x_n\end{pmatrix}$$</p>
<p>对于每个样本：$$y_i=\hat{\beta}_0+\hat{\beta}_1x_i+\varepsilon_i$$ $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$$<br>残差平方和 $$\text{RSS}=\sum^n_{i=1}(y_i-\hat{y}_i)^2$$</p>
<p>记函数$L(\beta_0,\beta_1)$为 $$L(\beta_0,\beta_1)=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2$$<br>那么 $$(\hat{\beta}_0,\hat{\beta}_1)=\arg\min_{(\beta_0,\beta_1)}L(\beta_0,\beta_1)$$</p>
<h3 id="1-1-OLS第一步：求偏导"><a href="#1-1-OLS第一步：求偏导" class="headerlink" title="1.1 OLS第一步：求偏导"></a>1.1 OLS第一步：求偏导</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    \displaystyle\frac{\partial L}{\partial \beta _0} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)} \\<br>    \displaystyle\frac{\partial L}{\partial \beta _1} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}<br>\end{array}<br>\right.<br>$$</p>
<h3 id="1-2-OLS第二步：让偏导等于零求解"><a href="#1-2-OLS第二步：让偏导等于零求解" class="headerlink" title="1.2 OLS第二步：让偏导等于零求解"></a>1.2 OLS第二步：让偏导等于零求解</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)}=0 \\<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}=0<br>\end{array}<br>\right.<br>$$</p>
<p>约掉-2，把求和里的每一项拆出来<br>$$<br>\left \{<br>\begin{array}{c}<br>    \displaystyle\sum_{i=1}^n{y_i}-n\beta _0-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _1&amp;=0 &amp;&amp;&amp;① \\<br>    \displaystyle\sum_{i=1}^n{x_iy_i}-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _0-\left( \displaystyle\sum_{i=1}^n{x_i^2} \right) \beta _1&amp;=0 &amp;&amp;&amp;②<br>\end{array}<br>\right.<br>$$</p>
<p>消元法，把①式的$\beta_0$用$\beta_1$来表示，然后代入②式求解出$\beta_1$<br>$$<br>\beta _0=\frac{1}{n}\sum_{i=1}^n{y_i}-\left( \frac{1}{n}\sum_{i=1}^n{x_i} \right) \beta _1<br>$$ $$<br>\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) \left( \sum_{i=1}^n{y_i} \right) +\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) ^2\beta _1-\left( \sum_{i=1}^n{x_i^2} \right) \beta _1=0<br>$$<br>求出$\beta_1$的值，这是$\beta_1$的估计值，记为$\hat{\beta_1}$<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) \left( \sum\limits_{i=1}^n{y_i} \right)}{\sum\limits_{i=1}^n{x_i^2}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) ^2}<br>$$ 这个表达式一大坨不好看，还可以简化一下，之后用在数据集上算也简便很多。$\frac{1}{n}\sum\limits_{i=1}^n{x_i}$是$x$的均值，记为$\bar{x}$；同样有$\bar{y}=\sum\limits_{i=1}^n{y_i}$<br>$$<br>\sum_{i=1}^n{x_i}=n\bar{x}，<br>\sum_{i=1}^n{y_i}=n\bar{y}<br>$$<br>先变分子，<br>$$<br>\begin{aligned}\text{分子}<br>&amp;=\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( n\bar{x} \right) \left( n\bar{y} \right) \\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-2n\bar{x}\bar{y}+n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( n\bar{y} \right) -\bar{y}\left( n\bar{x} \right) +n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( \sum_{i=1}^n{y_i} \right)-\bar{y}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}\bar{y}}\\\<br>&amp;=\sum_{i=1}^n{\left( x_iy_i-\bar{x}y_i-\bar{y}x_i+\bar{x}\bar{y} \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}\end{aligned}<br>$$<br>再变分母，<br>$$<br>\begin{aligned}\text{分母}&amp;=\sum_{i=1}^n{x_i^2}-\frac{1}{n}\left( n\bar{x} \right) ^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2n\bar{x}^2+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\left( n\bar{x} \right) \bar{x}+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\bar{x}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}^2}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i^2-2\bar{x}x_i+\bar{x}^2 \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) ^2}\end{aligned}<br>$$<br>最后得到<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>$$<br>刚才的$\beta_0$用$\beta_1$表示，现在求出了$\beta_1$，$\beta_0$也就呼之欲出<br>$$<br>\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}<br>$$<br>最终，得到对于一元线性回归模型OLS的解为<br>$$<br>\left \{<br>\begin{array}{l}<br>    \hat{\beta}_0= \bar{y}-\hat{\beta}_1\bar{x} \\<br>    \hat{\beta}_1= \displaystyle\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>\end{array}<br>\right.<br>$$</p>
<h2 id="2-多元线性回归"><a href="#2-多元线性回归" class="headerlink" title="2. 多元线性回归"></a>2. 多元线性回归</h2><p>模型：$$\mathbf{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon }$$<br>$$\begin{array}{l}\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}\\\<br>\boldsymbol{X}=\begin{pmatrix}1&amp;x_{11}&amp;x_{12}&amp;\dots&amp;x_{1p}\\1&amp;x_{21}&amp;x_{22}&amp;\dots&amp;x_{2p}\\\vdots&amp; &amp;&amp;\ddots&amp;\vdots\\1&amp;x_{n1}&amp;x_{n2}&amp;\dots&amp;x_{np}\end{pmatrix},\quad<br>\boldsymbol{\beta}=\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}\\\<br>\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\end{pmatrix}\end{array}$$<br>对于每个样本：$$\begin{aligned}y_i<br>&amp;=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}+\varepsilon_i\\\<br>&amp;=\begin{pmatrix}1&amp;x_{i1}&amp;x_{i2}&amp;\cdots&amp;x_{ip}\end{pmatrix}\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}+\varepsilon_i\end{aligned}$$<br>残差平方和$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp; =\sum_{i=1}^n{\left( y_i-\hat{y}_i \right) ^2}=\sum_{i=1}^n{\varepsilon _i^2}\\\<br>&amp; =\varepsilon _1^2+\varepsilon _2^2+\cdots +\varepsilon _n^2\\\<br>&amp;=\left( \begin{matrix}<br>    \varepsilon _1&amp;        \varepsilon _2&amp;        \cdots&amp;        \varepsilon _n\<br>\end{matrix} \right) \left( \begin{array}{c}<br>    \varepsilon _1\\\<br>    \varepsilon _2\\\<br>    \vdots\\\<br>    \varepsilon _n<br>\end{array} \right) \\\<br>&amp; =\boldsymbol{\varepsilon }^T\boldsymbol{\varepsilon }\\\<br>&amp; =\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)<br>\end{aligned}<br>$$</p>
<h3 id="2-1-矩阵相乘"><a href="#2-1-矩阵相乘" class="headerlink" title="2.1 矩阵相乘"></a>2.1 矩阵相乘</h3><p>前方一波矩阵运算，先补一点矩阵运算基础小知识：</p>
<ul>
<li>矩阵相乘不满足交换律<br>$$AB\ne BA$$</li>
<li>矩阵相乘满足结合律<br>$$(AB)C=A(BC)$$</li>
<li>矩阵相乘的转置提出来，要把前后两个矩阵位置换一下<br>$$(AB)^T=B^TA^T$$</li>
</ul>
<p>继续算$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp;=\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\left( \mathbf{y}^T-\left( \boldsymbol{X\beta } \right) ^T \right) \left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\left( \boldsymbol{X\beta } \right) ^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\left( \boldsymbol{X\beta } \right) ^T\boldsymbol{X\beta }\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>\end{aligned}$$</p>
<p>第3项$\mathbf{y}^T\boldsymbol{X\beta }$乘出来的结果是一个标量，也就是一个数（一个数怎么转置都是同一个东西）<br>$$\mathbf{y}^T\boldsymbol{X\beta}=<br>\begin{pmatrix}<br>    &amp;&amp; \mathbf{y}^T &amp;&amp;<br>\end{pmatrix}_{1\times n}<br>\begin{pmatrix}<br>    \\\<br>    \\\<br>    \boldsymbol{X\beta}\\\<br>    \\\\<br>\end{pmatrix}_{n\times 1}<br>=(\cdot)_{1\times 1}<br>$$</p>
<p>所以第3项可以写成$\mathbf{y}^T\boldsymbol{X\beta }=\left( \mathbf{y}^T\boldsymbol{X\beta } \right) ^T=\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}$，跟前面第二项一样，合并起来$L(\boldsymbol{\beta})$最后写成这样<br>$$<br>L\left( \boldsymbol{\beta } \right) =\mathbf{y}^T\mathbf{y}-2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<h3 id="2-2-矩阵求导"><a href="#2-2-矩阵求导" class="headerlink" title="2.2 矩阵求导"></a>2.2 矩阵求导</h3><p>接下来要求偏导，又来补一波矩阵小知识，关于对矩阵求导</p>
<ul>
<li>$\boldsymbol{a,b}$都是K阶向量<br>$$<br>\frac{\partial \boldsymbol{a}^T\boldsymbol{b}}{\partial \boldsymbol{b}}=\frac{\partial \boldsymbol{b}^T\boldsymbol{a}}{\partial \boldsymbol{b}}=\boldsymbol{a}<br>$$</li>
<li>$\boldsymbol{A}$是对称矩阵<br>$$<br>\frac{\partial \boldsymbol{b}^T\boldsymbol{Ab}}{\partial \boldsymbol{b}}=2\boldsymbol{Ab}=2\boldsymbol{b}^T\boldsymbol{A}<br>$$</li>
</ul>
<p>OK，刀磨好了继续砍柴，来求$L(\boldsymbol{\beta})$的偏导，第一项没有$\boldsymbol{\beta }$，不管它；第二项的$\boldsymbol{\beta }$是px1向量，$\boldsymbol{X}^T\mathbf{y}$也是px1向量<br>$$<br>\frac{\partial 2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}}{\partial \boldsymbol{\beta }}=\frac{\partial 2\boldsymbol{\beta }^T\left( \boldsymbol{X}^T\mathbf{y} \right)}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>第3项的$\boldsymbol{X}^T\boldsymbol{X}$是p×p对称矩阵<br>$$<br>\frac{\partial \boldsymbol{\beta }^T\left( \boldsymbol{X}^T\boldsymbol{X} \right) \boldsymbol{\beta }}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>所以对$\boldsymbol{X}^T\boldsymbol{X}$求出来偏导是<br>$$<br>\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>令偏导等于零求解<br>$$<br>2\boldsymbol{X}^T\boldsymbol{X\beta }=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>约掉2，然后两边都乘$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}$（需要$\boldsymbol{X}^T\boldsymbol{X}$可逆）<br>$$<br>\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\boldsymbol{X\beta }=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\left( \boldsymbol{X}^T\boldsymbol{X} \right) =\boldsymbol{I}$，$\boldsymbol{I}$是一个pxp的单位矩阵，那么$\boldsymbol{I\beta }=\boldsymbol{\beta }$，所以求出来<br>$$<br>\hat{\beta}_{OLS}=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>Done！</p>
<h2 id="3-再补充点说明"><a href="#3-再补充点说明" class="headerlink" title="3. 再补充点说明"></a>3. 再补充点说明</h2><h3 id="3-1-Q1：什么函数能找到它的最小值？"><a href="#3-1-Q1：什么函数能找到它的最小值？" class="headerlink" title="3.1 Q1：什么函数能找到它的最小值？"></a>3.1 Q1：什么函数能找到它的最小值？</h3><p>令偏导为零能找到最小值？<br>对于残差平方和这个函数$L(\boldsymbol{\beta})$是的，这是一个凸函数。<br>一元函数$f\left( x \right) =\left( x-4 \right) ^2+3$，我们求一阶导数$f’\left( x \right) =2\left( x-4 \right)$，再求二阶导数$f’’\left( x \right) =2$，<font color='red'>二阶导大于零</font>，所以这个函数是凸函数，可以在导数等于零的时候找到最小值。<br>多元函数$L(\boldsymbol{\beta})$，一阶导刚才我们求出来，是$\displaystyle\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }$，继续求二阶导$\displaystyle\frac{\partial ^2L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }^2}=2\boldsymbol{X}^T\boldsymbol{X}$，求出来这个二阶导是个矩阵。跟一元函数类似的，<font color='red'>二阶导如果是一个正定矩阵（类比正实数）</font>，那么这个函数是凸函数，可以找到最小值。而$2\boldsymbol{X}^T\boldsymbol{X}$是一个正定矩阵，因为$\boldsymbol{X}$是满秩的（基于各变量X相互独立的基本假设）。关于满秩和正定矩阵什么的在「<a href="/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/" title="矩阵的一些事">矩阵的一些事</a>」这篇文章有详解，这里一两句话说不清楚。</p>
<h3 id="3-2-Q2：-boldsymbol-X-T-boldsymbol-X-可逆？"><a href="#3-2-Q2：-boldsymbol-X-T-boldsymbol-X-可逆？" class="headerlink" title="3.2 Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？"></a>3.2 Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？</h3><p>$\boldsymbol{X}^T\boldsymbol{X}$可逆？<br>$\boldsymbol{X}^T\boldsymbol{X}$是正定矩阵  $\Longrightarrow\boldsymbol{X}^T\boldsymbol{X}$的行列式大于零<br>行列式$\ne 0\Longrightarrow$ 可逆</p>
<p>$$ ===== Fin =====$$</p>
<p>参考资料：</p>
<ol>
<li><a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" target="_blank" rel="noopener">OLS in Matrix Form</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>2020春，是世界末日的开端，还是黎明前的黑暗</title>
    <url>/2020/03/05/2020%E6%98%A5%EF%BC%8C%E6%98%AF%E4%B8%96%E7%95%8C%E6%9C%AB%E6%97%A5%E7%9A%84%E5%BC%80%E7%AB%AF%EF%BC%8C%E8%BF%98%E6%98%AF%E9%BB%8E%E6%98%8E%E5%89%8D%E7%9A%84%E9%BB%91%E6%9A%97/</url>
    <content><![CDATA[<h2 id="2020春，被迫进入HARD模式"><a href="#2020春，被迫进入HARD模式" class="headerlink" title="2020春，被迫进入HARD模式"></a>2020春，被迫进入HARD模式</h2><p>开始放春节假了，下午大家都逐渐放慢手脚。收拾好我的小抱枕拿回去洗洗，也没打算拷啥回去做了，想学啥我收藏夹里的东西这辈子都要学不完。好好放个假，两个星期后回来开工！<br>现在过年又不能放鞭炮，我们家又不爱走亲戚，我的关注点都是在贺岁片上。今年的贺岁片都是2D，不搞3D那种花里胡哨的东西，还都挺期待的。<br>20号的时候看到有自媒体总结贺岁片我还兴致勃勃地转发“安排上了”，当时新闻已经报了有传染病病例，但是我们都没有觉得是什么大事。好像那时候是深圳有了病例吧，记不太清了，现在还是各处管制着没有完全解封，但对疫情扩散之前的印象居然有点模糊了。有个在深圳工作的朋友提醒我别乱跑了。我还想这孩子居然忽然这么严肃。<br><img src="https://pic.downk.cc/item/5e60f87398271cb2b8bf58e7.jpg" width=350><br>那晚还是买了贺岁片的电影票，年初一早上一场，晚上一场。<br><img src="https://pic.downk.cc/item/5e60fb8d98271cb2b8c06f53.jpg" width=350><br>很快，我就开始跳脚售票平台不主动给我提供退票渠道了。然后到了23号，贺岁档电影接连宣布退档。<br><img src="https://pic.downk.cc/item/5e60fbab98271cb2b8c07ac1.jpg" width=350><br>再后面支付宝的交易信息是一串的口罩交易记录，付款-退款，付款-退款。现在回头看那段时间真的是昏暗，即使我们不在疫区，身边没有病例，但是大家都受到很大影响。到处买不到口罩，不断收到新确诊病例的消息。明明应该是欢天喜地的春节，却笼罩在一片迷雾中。感觉这种日子看不到尽头。</p>
<h2 id="在家办公初体验"><a href="#在家办公初体验" class="headerlink" title="在家办公初体验"></a>在家办公初体验</h2><p>春节假期快要结束了，大家都很不愿意回去上班。形势很不明朗，病毒传播性很强。各地政府都出文推迟开工时间。我们也如愿推迟了，一开始是推迟两天，再接着是一周，再然后就直接不通知上班时间了，届时再通知。<br>我们新项目本来打算年后把之前的计划立马落实，争取了在家办公。本来我们办公大多数时候都是各自敲电脑，实行起来也不太艰难。就是在家办公工资被粉碎性打折，但是活还是一样多，还得花自己水电。<br>在家办公三周了，大家都开始有点坐不住了，希望回到公司上班，不然太没有安全感。月底那周老大也通知在外地的同事要回到广州，随时准备回公司上班。</p>
<h2 id="无解"><a href="#无解" class="headerlink" title="无解"></a>无解</h2><p>紧接着就是忽如其来的被辞退。当外地的同事回来准备去上班的时候，收到的是公司要断臂自保的决定。<br>虽然时势艰难，但我们都表示理解，毕竟我们也想去寻找别的机会。接下来就是跟人资不愉快的交流。人资小姐姐年龄不小，不过似乎不会讲道理喜欢胡搅蛮缠，提出合理赔偿的时候，她就用女孩子对男朋友撒泼的招数，一直重复“你怎么这样啊！你别说这种话！”完全无法理智交流。只是提出依法赔偿而已，这是哪种话呢？后来还是被忽悠了，还得继续留下来干活，干的啥也不想提。反正就是赶紧找下家吧。<br>其实是公司想转型，失败，年前已经解雇了一批人，这次借疫情又来一刀。想要用技术脱离传统来个质的飞跃，然而研发是费钱费力，还见效无法预估的。舍不得价钱请个实干的领导者，底下的人再有能力也没法成事。技术研发靠虚假宣传、夸大效果、注重形式主义是骗不了人的。<br>领导者对技术一知半解在国内是很平常的事。这不是最糟糕，最可怕的是他不要你觉得，他只要他觉得。技术研发部全员营销，虚假宣传、无脑画饼，骗得了他自己，骗不过老板。没成果，啥啥都落不了地。老板就是要看结果的。技术员也是要恰饭的呀！</p>
<h2 id="它有它末日，我有我HAPPY"><a href="#它有它末日，我有我HAPPY" class="headerlink" title="它有它末日，我有我HAPPY"></a>它有它末日，我有我HAPPY</h2><p>“如果选择做的事是自己认可的，且有它的意义在，我会把心态调整好再开始。做了又抱怨，这是逻辑问题。”某人说的这句话很平实很有道理。<br>我还是比较情绪化的，喜欢埋怨或许是每个人的常态。抱怨也抱怨过了，再抱怨下去于事无补。虽然事态发展非我所愿，但事已至此，该调整心态，调整计划，走好接下来的路。<br>一直犯懒，老大靠谱、同事相处和谐就觉得不太想变动了，过于安逸了。这次的不快对我来说可能转变成一个好事，被迫加快落实我的个人规划，尽早拨云见日。<br>距上次弄出来个github博客之后一年了，一直都没有更新过。我都忘了看的哪个教程搭出来的了，之前好像因为前端技能没有点亮遇到了瓶颈就丢弃了。这次找到对的方式认真搭好，开始用起来！<br>另外，最近我觉得还是有不少好事的，虽然不是发生在我身上，但都是我朋友的好事，我也觉得很快乐。老同学准备入职新公司了，仙女网友添丁了，沙雕网友拉埋天窗了，留学时的好朋友在当地买房了。跳槽、添丁、结婚、置业，一堆喜事萦绕着我呀~<br>我也希望实现19年末的新年许愿，在2020年找到新工作！<br>$$====== Fin =====$$</p>
]]></content>
  </entry>
</search>
