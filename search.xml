<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>用Python实现SPC统计过程控制</title>
    <url>/2020/05/09/python-spc/</url>
    <content><![CDATA[<h2 id="SPC与六西格玛"><a href="#SPC与六西格玛" class="headerlink" title="SPC与六西格玛"></a>SPC与六西格玛</h2><p>SPC (Statistical Process Control) 统计过程控制，是六西格玛工业管理理论的其中一个重要模块。SPC的控制图 (control chart) 是数据可视化的一个重要手段。而控制图的选择应该根据实际需求来，这里不展开讲控制图，关于控制图的细节可以查找其他资料。（7 种控制图，8 个判异准则。）</p>
<img src='https://pic.downk.cc/item/5eb7b506c2a9a83be53b8e1e.png'>

<p>简单介绍一下六西格玛，就是 6 sigma 的音译，sigma 是什么？接触过统计的人应该会有印象，$\sigma$ 这个符号就是 sigma，一般代表偏差。</p>
<p>先介绍一下生产质量控制中常用的一个概念 DPMO (Defects Per Million Opportunities)，就是在生产过程中每 100 万个机会中出现的缺陷数。比如在 100 万个焊点里，出现了 1 个缺陷，那么 DPMO 就等于 1。可见 DPMO 是越小越好，最好是为 0。当然在大量的生产过程中这种理想状态出现的几率是非常非常小的。DPMO 的取值会作为衡量生产质量的一个重要指标，下面是 DPMO 对应 sigma 水平的对照表。</p>
<table>
<thead>
<tr>
<th align="center">sigma level</th>
<th align="center">DPMO</th>
<th align="center">yield</th>
</tr>
</thead>
<tbody><tr>
<td align="center">6</td>
<td align="center">3.4</td>
<td align="center">99.99966%</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">230</td>
<td align="center">99.977%</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">6210</td>
<td align="center">99.38%</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">66800</td>
<td align="center">93.32%</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">308000</td>
<td align="center">69.15%</td>
</tr>
<tr>
<td align="center">1</td>
<td align="center">690000</td>
<td align="center">30.85%</td>
</tr>
</tbody></table>
<p>如果 DPMO 为 3.4 ，对应的 sigma 水平是 6，也就是可以达到 99.99966% 的合格率。所以 sigma 水平就可以作为生产质量的一个评判标准，达到 6 sigma 水平的生产就是非常牛逼的水准了！</p>
<img src='https://pic.downk.cc/item/5eb7cc68c2a9a83be56d120e.jpg' width=600>

<p>6 个 $\sigma$ 的介绍就到这里，不过六西格玛管理理论远不止于此，除了统计学的知识，大部分是关于精益生产管理。</p>
<h2 id="U-Chart-案例"><a href="#U-Chart-案例" class="headerlink" title="U Chart 案例"></a>U Chart 案例</h2><p>这是当时工作中遇到的实际需求，要观察生产产品的单位缺陷数。拿到检测机器的测试记录，包括每天检测的产品编号、检测时间、残次件的维修时间、维修人员等等。冗余信息非常多，用 Excel 处理，只提取检测的产品数量和缺陷件数量，按月份统计整理。（现在看回头当时为什么会弄了两个表示时间的列也是百思不得其解。。。。）n 列表示当月检测的产品数量，c 列表示当月的缺陷数。</p>
<img src='https://pic.downk.cc/item/5eb81213c2a9a83be51b1811.png'>

<p>因为每个月的产品数量是不同的，所以采用用于可变样本量的 U-chart。如果每个月的产品数量相同，用 C-chart。</p>
<p>Defects per unit: $$u=\frac{c}{n}$$</p>
<p>Central Limit: $$CL=\bar{u}=\frac{\sum c_i}{\sum n_i}$$</p>
<p>Upper Central Limit: $$UCL=\bar{u}+3\sqrt{\frac{\bar{u}}{n_i}}$$</p>
<p>（开方里的是 $\bar{u}$，好像叠起来看不见了）本来还应该算下界的，但是因为 u 肯定是大于 0，且越小越好，所以下界不需要考虑。</p>
<p>用 pandas 导入整理好的数据命名为 defect_test。计算好3个需要展示的数据：u，CL，UCL。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">time = defect_test[<span class="string">'Month'</span>]</span><br><span class="line">u = defect_test[<span class="string">'c'</span>]/defect_test[<span class="string">'n'</span>]</span><br><span class="line">cl = sum(defect_test[<span class="string">'c'</span>])/sum(defect_test[<span class="string">'n'</span>])</span><br><span class="line">ucl = cl + <span class="number">3</span>*np.sqrt(cl/defect_test[<span class="string">'n'</span>])</span><br></pre></td></tr></table></figure>

<p>用 matplotlib 可视化</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cl_s = np.ones(<span class="number">12</span>)*cl</span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">6</span>))</span><br><span class="line">plt.plot(time,u,<span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">10</span>,lw=<span class="number">3</span>,label=<span class="string">'u=c/n'</span>)</span><br><span class="line">plt.plot(time,cl_s,<span class="string">'r'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">plt.plot(time,ucl,<span class="string">'b--'</span>,label=<span class="string">'UCL'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'U chart'</span>);</span><br></pre></td></tr></table></figure>

<img src='https://pic.downk.cc/item/5eb81fc3c2a9a83be53e6989.png'>

<p>从上图可以看出，缺陷数u一直在CL附近波动。大部分的u都在UCL以下，但是有两个月是在控制以外 (out of control)，二月和三月。二月份的u值是0.005277，三月份的是0.006198。对应的DPMO值5277和6198，对应了4 sigma的水平，对应Cpk值（工程能力，要求达到1.33以上）是1.33。总而来说，在二月和三月，缺陷数在控制以外，但Cpk还是达到了应有的期望值1.33。整一年生产过程是达到要求的，但是还是需要注意质量控制，需要保持所有的缺陷数在控制范围内。</p>
<p>对于超出上界限UCL的点判断为异常，在图表上显示出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">uchart_test</span><span class="params">(c,n,time)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    </span><br><span class="line">    u = c/n</span><br><span class="line">    cl = sum(c)/sum(n)</span><br><span class="line">    ucl = cl + <span class="number">3</span> * np.sqrt(cl/n)</span><br><span class="line">    cl_s = np.ones(len(time))*cl</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># out of control points</span></span><br><span class="line">    ofc = u[u &gt;= ucl]</span><br><span class="line">    ofc_ind = list(ofc.index)</span><br><span class="line">    ofc_time = time[ofc_ind]</span><br><span class="line">    print(<span class="string">"Out of Control:"</span>)</span><br><span class="line">    <span class="keyword">if</span> len(ofc) == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">"All under control."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i,j <span class="keyword">in</span> zip(ofc_ind,range(len(ofc))):</span><br><span class="line">            print(str(j+<span class="number">1</span>) + <span class="string">" - "</span> + str(ofc_time[i]) + <span class="string">", "</span> + str(ofc[i]))</span><br><span class="line"></span><br><span class="line">    plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot out of control points</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ofc_ind:</span><br><span class="line">        plt.scatter(ofc_time[i],ofc[i],c=<span class="string">'r'</span>,s=<span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># plot u chart</span></span><br><span class="line">    plt.plot(time,u,<span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">8</span>,lw=<span class="number">3</span>,label=<span class="string">'u=c/n'</span>)</span><br><span class="line">    plt.plot(time,cl_s,<span class="string">'g'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">    plt.plot(time,ucl,<span class="string">'b--'</span>,label=<span class="string">'UCL'</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.ylim((<span class="number">0</span>,max(u)+<span class="number">0.001</span>))</span><br><span class="line">    plt.title(<span class="string">'U chart'</span>);</span><br></pre></td></tr></table></figure>

<p>用定义好的函数 <code>uchart_test</code>，输入残缺件数c，总件数n，时间time，就可以画出 U chart，并且找出异常点：</p>
<img src='https://pic.downk.cc/item/5eb8c0d7c2a9a83be577e9e3.png'>

<h2 id="I-MR-Chart-案例"><a href="#I-MR-Chart-案例" class="headerlink" title="I-MR Chart 案例"></a>I-MR Chart 案例</h2><p>使用 I-MR 控制图 可以在拥有连续数据且这些数据是不属于子组的单个观测值的情况下监视过程的均值和变异。使用此控制图可以监视过程在一段时间内的稳定性，以便您可以标识和更正过程中的不稳定性。</p>
<p>例如，一家医院的管理员想确定门诊疝气手术所用的时间是否稳定，以及手术时间的变异性是否稳定。由于数据不是以子组形式收集的，因此管理员使用 I-MR 控制图监视手术时间的均值和变异性。</p>
<p>实际看看下面这个数据案例：<br><img src='https://pic.downk.cc/item/5eb8c4fec2a9a83be57efa77.png'></p>
<p>I-MR Chart 是有两个图：一个是 I Chart (Individual Chart)，一个是 MR Chart (Moving Range Chart)。<br><img src='https://pic.downk.cc/item/5eb8c67fc2a9a83be5822731.png'></p>
<p>和所有控制图一样，重点是找到 CL，UCL，LCL。</p>
<p><strong>Tabular values for X and range charts</strong></p>
<table>
<thead>
<tr>
<th align="center">Subgroup Size</th>
<th align="center">E2</th>
<th align="center">D4</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">2.660</td>
<td align="center">3.268</td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">2.660</td>
<td align="center">3.268</td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">1.772</td>
<td align="center">2.574</td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">1.457</td>
<td align="center">2.282</td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">1.290</td>
<td align="center">2.114</td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">1.184</td>
<td align="center">2.004</td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">1.109</td>
<td align="center">1.924</td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">1.054</td>
<td align="center">1.864</td>
</tr>
<tr>
<td align="center">9</td>
<td align="center">1.010</td>
<td align="center">1.816</td>
</tr>
<tr>
<td align="center">10</td>
<td align="center">0.975</td>
<td align="center">1.777</td>
</tr>
</tbody></table>
<p>subgroup 的大小可以调整，这里我们默认选 1。</p>
<p>I Chart：$X$</p>
<p>$$CL_X=\bar{X}=\frac{1}{n}\sum_{i=1}^n X_i$$</p>
<p>$$UCL_X=\bar{X}+E_2\times \bar{MR}$$</p>
<p>$$LCL_X=\bar{X}-E_2\times \bar{MR}$$</p>
<p>MR Chart：$MR=|X_i-X_{i-1}|$ （维度比 X 少 1）</p>
<p>$$CL_{MR}=\frac{1}{n-1}\sum_{i=2}^n (X_i-X_{i-1})$$</p>
<p>$$UCL_{MR}=D_4\times \bar{MR}$$</p>
<p>$$LCL_{MR}=0$$</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">xs = data.iloc[:,[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>]]</span><br><span class="line">observation = data.iloc[:,[<span class="number">0</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">6</span>,<span class="number">8</span>]]</span><br><span class="line">x = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(xs.shape[<span class="number">1</span>]):</span><br><span class="line">    x.extend(xs.iloc[:,i])</span><br><span class="line">x = np.array(x)</span><br><span class="line">    </span><br><span class="line">obs = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(observation.shape[<span class="number">1</span>]):</span><br><span class="line">    obs.extend(observation.iloc[:,i])</span><br><span class="line">obs = np.array(obs)</span><br><span class="line"></span><br><span class="line">x_bar = x.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># move range</span></span><br><span class="line">x_1 = x[:<span class="number">-1</span>]</span><br><span class="line">x_2 = x[<span class="number">1</span>:]</span><br><span class="line">mr = np.array(np.array(list(abs(v - u) <span class="keyword">for</span> u, v <span class="keyword">in</span> zip(x, x[<span class="number">1</span>:]))))</span><br><span class="line"></span><br><span class="line">mr_bar = mr.mean()</span><br><span class="line"></span><br><span class="line"><span class="comment"># E2 = 2.660</span></span><br><span class="line">x_ucl = x_bar + <span class="number">2.660</span> * mr_bar</span><br><span class="line">x_ucl_c = x_bar + <span class="number">2.660</span> * mr_bar * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">x_ucl_b = x_bar + <span class="number">2.660</span> * mr_bar * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line">x_lcl = x_bar - <span class="number">2.660</span> * mr_bar</span><br><span class="line">x_lcl_c = x_bar - <span class="number">2.660</span> * mr_bar * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">x_lcl_b = x_bar - <span class="number">2.660</span> * mr_bar * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># D4 = 3.268</span></span><br><span class="line">mr_ucl = <span class="number">3.268</span> * mr_bar</span><br><span class="line">mr_ucl_c = mr_bar + (mr_ucl - mr_bar) * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">mr_ucl_b = mr_bar + (mr_ucl - mr_bar) * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line">mr_lcl = <span class="number">0</span></span><br><span class="line">mr_lcl_c = mr_bar - (mr_ucl - mr_bar) * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">mr_lcl_b = mr_bar - (mr_ucl - mr_bar) * (<span class="number">2</span>/<span class="number">3</span>)   <span class="comment"># &lt;0</span></span><br><span class="line"></span><br><span class="line">n = len(x)</span><br><span class="line"></span><br><span class="line">x_ucl = np.ones(n)*x_ucl</span><br><span class="line">x_ucl_c = np.ones(n)*x_ucl_c</span><br><span class="line">x_ucl_b = np.ones(n)*x_ucl_b</span><br><span class="line">x_lcl = np.ones(n)*x_lcl</span><br><span class="line">x_lcl_c = np.ones(n)*x_lcl_c</span><br><span class="line">x_lcl_b = np.ones(n)*x_lcl_b</span><br><span class="line"></span><br><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x, <span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">5</span>,lw=<span class="number">2</span>,label=<span class="string">'X'</span>)</span><br><span class="line">plt.plot(obs,np.ones(n)*x_bar,<span class="string">'r'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">plt.plot(obs,x_ucl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x_ucl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x_ucl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x_lcl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x_lcl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs,x_lcl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'I chart'</span>)</span><br><span class="line"></span><br><span class="line">mr_ucl = np.ones(n<span class="number">-1</span>)*mr_ucl</span><br><span class="line">mr_ucl_c = np.ones(n<span class="number">-1</span>)*mr_ucl_c</span><br><span class="line">mr_ucl_b = np.ones(n<span class="number">-1</span>)*mr_ucl_b</span><br><span class="line">mr_lcl = np.ones(n<span class="number">-1</span>)*mr_lcl</span><br><span class="line">mr_lcl_c = np.ones(n<span class="number">-1</span>)*mr_lcl_c</span><br><span class="line">mr_lcl_b = np.ones(n<span class="number">-1</span>)*mr_lcl_b</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr, <span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">5</span>,lw=<span class="number">2</span>,label=<span class="string">'MR'</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_bar,<span class="string">'r'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr_ucl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr_ucl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr_ucl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr_lcl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.plot(obs[<span class="number">1</span>:],mr_lcl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.title(<span class="string">'MR chart'</span>);</span><br></pre></td></tr></table></figure>

<img src='https://pic.downk.cc/item/5eb9122fc2a9a83be5183c86.png'>

<p>判异的规则有 8 条：</p>
<table>
<thead>
<tr>
<th align="center">Rule</th>
<th align="center">Rule Name</th>
<th align="left">Pattern</th>
<th>&emsp; &emsp; &emsp; &emsp; &emsp; &emsp; Example&emsp; &emsp; &emsp; &emsp; &emsp; &emsp;</th>
</tr>
</thead>
<tbody><tr>
<td align="center">1</td>
<td align="center">Beyond Limits</td>
<td align="left">1个点落在A区外</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/a/a0/Rule_1_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">2</td>
<td align="center">Zone A</td>
<td align="left">连续3点中有2点落在中心线同一侧的Zone B以外</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/2/20/Rule_5_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">3</td>
<td align="center">Zone B</td>
<td align="left">连续5点有4点落在中心线同一侧的Zone C以外</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/7/7b/Rule_6_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">4</td>
<td align="center">Zone C</td>
<td align="left">连续9个以上的点落在中心线同一侧（Zone C或以外）</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/0/0e/Rule_2_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">5</td>
<td align="center">Trend</td>
<td align="left">连续7点递增或递减</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/e/e0/Rule_3_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">6</td>
<td align="center">Mixture</td>
<td align="left">连续8点无一点落在Zone C</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/b/b7/Rule_8_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">7</td>
<td align="center">Stratification</td>
<td align="left">连续15点落在中心线两侧的Zone C内</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/a/ae/Rule_7_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
<tr>
<td align="center">8</td>
<td align="center">Over-control</td>
<td align="left">连续14点相邻交替上下</td>
<td><img src='https://upload.wikimedia.org/wikipedia/commons/3/35/Rule_4_-_Control_Charts_for_Nelson_Rules.svg'></td>
</tr>
</tbody></table>
<p>加入8条判异规则：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rules</span><span class="params">(data, obs, cl, ucl, ucl_b, ucl_c, lcl, lcl_b, lcl_c)</span>:</span></span><br><span class="line">    n = len(data)</span><br><span class="line">    ind = np.array(range(n))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 1</span></span><br><span class="line">    ofc1 = data[(data &gt; ucl) | (data &lt; lcl)]</span><br><span class="line">    ofc1_obs = obs[(data &gt; ucl) | (data &lt; lcl)]    </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 2</span></span><br><span class="line">    ofc2_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-2</span>):</span><br><span class="line">        d = data[i:i+<span class="number">3</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">3</span>]</span><br><span class="line">        <span class="keyword">if</span> ((d&gt;ucl_b).sum()==<span class="number">2</span>) | ((d&lt;lcl_b).sum()==<span class="number">2</span>):</span><br><span class="line">            ofc2_ind.extend(index[(d&gt;ucl_b) | (d&lt;lcl_b)])</span><br><span class="line">    ofc2_ind = list(sorted(set(ofc2_ind)))</span><br><span class="line">    ofc2 = data[ofc2_ind]</span><br><span class="line">    ofc2_obs = obs[ofc2_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 3</span></span><br><span class="line">    ofc3_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-4</span>):</span><br><span class="line">        d = data[i:i+<span class="number">5</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">5</span>]</span><br><span class="line">        <span class="keyword">if</span> ((d&gt;ucl_c).sum()==<span class="number">4</span>) | ((d&lt;lcl_c).sum()==<span class="number">4</span>):</span><br><span class="line">            ofc3_ind.extend(index[(d&gt;ucl_c) | (d&lt;lcl_c)])</span><br><span class="line">    ofc3_ind = list(sorted(set(ofc3_ind)))</span><br><span class="line">    ofc3 = data[ofc3_ind]</span><br><span class="line">    ofc3_obs = obs[ofc3_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 4</span></span><br><span class="line">    ofc4_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-8</span>):</span><br><span class="line">        d = data[i:i+<span class="number">9</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">9</span>]</span><br><span class="line">        <span class="keyword">if</span> ((d&gt;cl).sum()==<span class="number">9</span>) | ((d&lt;cl).sum()==<span class="number">9</span>):</span><br><span class="line">            ofc4_ind.extend(index)</span><br><span class="line">    ofc4_ind = list(sorted(set(ofc4_ind)))</span><br><span class="line">    ofc4 = data[ofc4_ind]</span><br><span class="line">    ofc4_obs = obs[ofc4_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 5</span></span><br><span class="line">    ofc5_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-6</span>):</span><br><span class="line">        d = data[i:i+<span class="number">7</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">7</span>]</span><br><span class="line">        <span class="keyword">if</span> all(u &lt;= v <span class="keyword">for</span> u, v <span class="keyword">in</span> zip(d, d[<span class="number">1</span>:])) | all(u &gt;= v <span class="keyword">for</span> u, v <span class="keyword">in</span> zip(d, d[<span class="number">1</span>:])):</span><br><span class="line">            ofc5_ind.extend(index)</span><br><span class="line">    ofc5_ind = list(sorted(set(ofc5_ind)))</span><br><span class="line">    ofc5 = data[ofc5_ind]</span><br><span class="line">    ofc5_obs = obs[ofc5_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 6</span></span><br><span class="line">    ofc6_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-7</span>):</span><br><span class="line">        d = data[i:i+<span class="number">8</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">8</span>]</span><br><span class="line">        <span class="keyword">if</span> (all(d&gt;ucl_c) | all(d&lt;lcl_c)):</span><br><span class="line">            ofc6_ind.extend(index)</span><br><span class="line">    ofc6_ind = list(sorted(set(ofc6_ind)))</span><br><span class="line">    ofc6 = data[ofc6_ind]</span><br><span class="line">    ofc6_obs = obs[ofc6_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 7</span></span><br><span class="line">    ofc7_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-14</span>):</span><br><span class="line">        d = data[i:i+<span class="number">15</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">15</span>]</span><br><span class="line">        <span class="keyword">if</span> all(lcl_c&lt;d) <span class="keyword">and</span> all(d&lt;ucl_c):</span><br><span class="line">            ofc7_ind.extend(index)</span><br><span class="line">    ofc7_ind = list(sorted(set(ofc7_ind)))</span><br><span class="line">    ofc7 = data[ofc7_ind]</span><br><span class="line">    ofc7_obs = obs[ofc7_ind]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># rule 8</span></span><br><span class="line">    ofc8_ind = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-13</span>):</span><br><span class="line">        d = data[i:i+<span class="number">14</span>]</span><br><span class="line">        index = ind[i:i+<span class="number">14</span>]</span><br><span class="line">        diff = list(v - u <span class="keyword">for</span> u, v <span class="keyword">in</span> zip(d, d[<span class="number">1</span>:]))</span><br><span class="line">        <span class="keyword">if</span> all(u*v&lt;<span class="number">0</span> <span class="keyword">for</span> u,v <span class="keyword">in</span> zip(diff,diff[<span class="number">1</span>:])):</span><br><span class="line">            ofc8_ind.extend(index)</span><br><span class="line">    ofc8_ind = list(sorted(set(ofc8_ind)))</span><br><span class="line">    ofc8 = data[ofc8_ind]</span><br><span class="line">    ofc8_obs = obs[ofc8_ind]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ofc1, ofc1_obs, ofc2, ofc2_obs, ofc3, ofc3_obs, ofc4, ofc4_obs, ofc5, ofc5_obs,ofc6, ofc6_obs, ofc7, ofc7_obs, ofc8, ofc8_obs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">imrchart_test</span><span class="params">(obs,x,subgroup=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># depend on subgroup value</span></span><br><span class="line">    E2 = [<span class="number">2.660</span>, <span class="number">2.660</span>, <span class="number">1.772</span>, <span class="number">1.457</span>, <span class="number">1.290</span>, <span class="number">1.184</span>, <span class="number">1.109</span>, <span class="number">1.054</span>, <span class="number">1.010</span>, <span class="number">0.975</span>]</span><br><span class="line">    D4 = [<span class="number">3.268</span>, <span class="number">3.268</span>, <span class="number">2.574</span>, <span class="number">2.282</span>, <span class="number">2.114</span>, <span class="number">2.004</span>, <span class="number">1.924</span>, <span class="number">1.864</span>, <span class="number">1.816</span>, <span class="number">1.777</span>]</span><br><span class="line">    e2 = E2[subgroup<span class="number">-1</span>]</span><br><span class="line">    d4 = D4[subgroup<span class="number">-1</span>]</span><br><span class="line">    </span><br><span class="line">    n = len(x)</span><br><span class="line">    </span><br><span class="line">    x_bar = x.mean()</span><br><span class="line">    mr = np.array(list(abs(v - u) <span class="keyword">for</span> u, v <span class="keyword">in</span> zip(x, x[<span class="number">1</span>:])))</span><br><span class="line">    mr_bar = mr.mean()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># I chart</span></span><br><span class="line">    x_ucl = x_bar + e2 * mr_bar</span><br><span class="line">    x_ucl_c = x_bar + e2 * mr_bar * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    x_ucl_b = x_bar + e2 * mr_bar * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line">    x_lcl = x_bar - e2 * mr_bar</span><br><span class="line">    x_lcl_c = x_bar - e2 * mr_bar * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    x_lcl_b = x_bar - e2 * mr_bar * (<span class="number">2</span>/<span class="number">3</span>) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MR chart</span></span><br><span class="line">    mr_ucl = d4 * mr_bar</span><br><span class="line">    mr_ucl_c = mr_bar + (mr_ucl - mr_bar) * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    mr_ucl_b = mr_bar + (mr_ucl - mr_bar) * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line">    mr_lcl = <span class="number">0</span></span><br><span class="line">    mr_lcl_c = mr_bar - (mr_ucl - mr_bar) * (<span class="number">1</span>/<span class="number">3</span>)</span><br><span class="line">    mr_lcl_b = mr_bar - (mr_ucl - mr_bar) * (<span class="number">2</span>/<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">if</span> mr_lcl_c &lt; <span class="number">0</span>: mr_lcl_c = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> mr_lcl_b &lt; <span class="number">0</span>: mr_lcl_b = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 8 rules</span></span><br><span class="line">    x_ofc = rules(x, obs, x_bar, x_ucl, x_ucl_b, x_ucl_c, x_lcl, x_lcl_b, x_lcl_c)</span><br><span class="line">    print(<span class="string">"========== I chart test =========="</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        print(<span class="string">"Against Rule %d:"</span> %(r+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> len(x_ofc[r*<span class="number">2</span>]) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"None."</span>)</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">for</span> num,i,j <span class="keyword">in</span> zip(range(len(x_ofc[r*<span class="number">2</span>])), x_ofc[r*<span class="number">2</span>+<span class="number">1</span>], x_ofc[r*<span class="number">2</span>]):</span><br><span class="line">                print(<span class="string">"\t%d -&gt; %s, %f"</span> %(num+<span class="number">1</span>, str(i), j))</span><br><span class="line">    </span><br><span class="line">    mr_ofc = rules(mr, obs[<span class="number">1</span>:], mr_bar, mr_ucl, mr_ucl_b, mr_ucl_c, mr_lcl, mr_lcl_b, mr_lcl_c)</span><br><span class="line">    print(<span class="string">"========== MR chart test =========="</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        print(<span class="string">"Against Rule %d:"</span> %(r+<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">if</span> len(mr_ofc[r*<span class="number">2</span>]) == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"None."</span>)</span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">for</span> num,i,j <span class="keyword">in</span> zip(range(len(mr_ofc[r*<span class="number">2</span>])), mr_ofc[r*<span class="number">2</span>+<span class="number">1</span>], mr_ofc[r*<span class="number">2</span>]):</span><br><span class="line">                print(<span class="string">"\t%d -&gt; %s, %f"</span> %(num+<span class="number">1</span>, str(i), j))      </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># plot</span></span><br><span class="line">    plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,x, <span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">5</span>,lw=<span class="number">2</span>,label=<span class="string">'X'</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_bar,<span class="string">'r'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_ucl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_ucl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_ucl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_lcl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_lcl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs,np.ones(n)*x_lcl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(x_ofc[r*<span class="number">2</span>])):</span><br><span class="line">            plt.scatter(x_ofc[r*<span class="number">2</span>+<span class="number">1</span>][i],x_ofc[r*<span class="number">2</span>][i],c=<span class="string">'r'</span>,s=<span class="number">70</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">'I chart'</span>)</span><br><span class="line"></span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],mr, <span class="string">'k'</span>,marker=<span class="string">'o'</span>,markersize=<span class="number">5</span>,lw=<span class="number">2</span>,label=<span class="string">'MR'</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_bar,<span class="string">'r'</span>,label=<span class="string">'CL'</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_ucl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_ucl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_ucl_b,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_lcl,<span class="string">'b'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    plt.plot(obs[<span class="number">1</span>:],np.ones(n<span class="number">-1</span>)*mr_lcl_c,<span class="string">'b-.'</span>,lw=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> r <span class="keyword">in</span> range(<span class="number">8</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(mr_ofc[r*<span class="number">2</span>])):</span><br><span class="line">            plt.scatter(mr_ofc[r*<span class="number">2</span>+<span class="number">1</span>][i],mr_ofc[r*<span class="number">2</span>][i],c=<span class="string">'r'</span>,s=<span class="number">70</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.title(<span class="string">'MR chart'</span>);</span><br><span class="line"></span><br><span class="line">imrchart_test(obs,x)</span><br></pre></td></tr></table></figure>

<img src='https://pic.downk.cc/item/5eb9699dc2a9a83be5dcf194.png'>
<img src='https://pic.downk.cc/item/5eb96a11c2a9a83be5de1c15.png'>
<img src='https://pic.downk.cc/item/5eb9693cc2a9a83be5dbf8ef.png'>

<p>针对每个控制图，按照计算公式编写对应的代码，确定上下界和判异规则，将异常点在图上用红点表示出来。</p>
]]></content>
      <tags>
        <tag>Projet</tag>
      </tags>
  </entry>
  <entry>
    <title>概率 &amp; 统计 &amp; 逻辑 &amp; 数据结构</title>
    <url>/2020/05/05/math-logique/</url>
    <content><![CDATA[<ol>
<li><p><strong>N个人中有两个人是同一天生日的概率。（一年365天，N&lt;365）</strong></p>
<ul>
<li><p>每个人的生日都有365个可能：$365^N$</p>
</li>
<li><p>每一个人的生日都不重复：</p>
<ul>
<li>第一个人：365种选择</li>
<li>第二个人：除去第一个人的生日，有364种选择</li>
<li>第三个人：除去前面两个人的生日，有363种选择</li>
</ul>
<p>共 $365\times364\times\cdots\times(365-(N-1))$个可能</p>
</li>
<li><p>至少有两个人是同一天生日的概率：$\displaystyle 1-\frac{365\times364\times\cdots\times(365-(N-1))}{365^N}$</p>
</li>
<li><p>仅有两个人是同一天生日：</p>
<ul>
<li>前N-1个人生日都不同：$365\times364\times\cdots\times(365-(N-2))$种</li>
<li>最后一个人生日同前面某个人：$N-1$种</li>
</ul>
<p>概率：$\displaystyle \frac{365\times364\times\cdots\times(365-(N-2))(N-1)}{365^N}$</p>
</li>
</ul>
</li>
</ol>
<ol start="2">
<li><p><strong>有2堆宝石，A和B一起玩游戏，假设俩人足够聪明，规则是每个人只能从一堆选走1个或2个或3个宝石，最后全部取玩的人获胜，假设2堆宝石的数目为12和13，请问A怎么可以必胜？<br> A. 让A先取<br> B. 让B先取<br> C. 没有策略能够让A必胜<br> D. 说法都不正确</strong></p>
<p> 让A先取，先把13取走一个，让两堆数量相同。B再在哪堆取，A就在同一堆取：如果B取1个，A在同堆取3个；如果B取2个，A取同堆2个；如果B取3个，A取同堆1个。如此保证A每次取完后每堆剩余的数目一直是4的倍数，B无法全部取走。</p>
</li>
</ol>
<ol start="3">
<li><p><strong>从数字集合{1,2,3,4,… ,20}中选出3个数字的子集，如果不允许两个相连的数字出现在同一集合中，那么能够形成多少个这种子集？<br> A. 816<br> B. 220<br> C. 340<br> D. 620</strong></p>
<p> 插空法，把3个球插入17个球中间，就是有18个空位来放这3个球：$C^3_{18}=816$，选A。</p>
</li>
</ol>
<ol start="4">
<li><p><strong>将3个不一样的球随机放入4个杯子中，则杯子中球的最大个数为2的概率是?<br> A. 9/16<br> B. 3/4<br> C. 3/8<br> D. 3/16</strong></p>
<p> 所有的情况，每个球都有4种方法：$4\times 4\times 4=64$<br> 选2个放在其中一个杯子$C^2_3$，这个杯子有$C^1_4$个选择，剩1个球放在其他三个杯子之一$C^1_3$：$C^2_3\times 4\times 3=36$<br> 最终概率为：$36/64 = 9/16$，选A。</p>
</li>
</ol>
<ol start="5">
<li><p><strong>在数理统计中， 一般通过增加抽样次数取平均来使得预估误差减小， 在机器学习中也有类似的模型处理， 如随机森林， 通过引入随机样本并且增加决策树的数据，对于随机森林主要降低预估的哪个方面值<br> A. 预估偏差<br> B. 预估方差<br> C. 噪音<br> D. 全部</strong></p>
<p> 选B。增加数据是降低由数据的不稳定性所带来的方差，增加模型复杂度是降低偏差，另外噪音是无法避免的。</p>
</li>
</ol>
<ol start="6">
<li><p><strong>一个快递公司对同一年龄段的员工，进行汽车，三轮车，二轮车平均送件量的比较，结果给出sig.=0.034，说明<br> A. 三类交通工具送件量有差别的可能性是0.034<br> B. 三类交通工具送件量没有差别的可能性是0.034<br> C. 交通工具对送件量没影响。<br> D. 按照0.05显著性水平，拒绝H0，说明三类交通工具送件量有显著差异</strong></p>
<p> 选D。</p>
<p> SIG.=significance，显著性，后面的值就是统计出的P值：</p>
<ul>
<li>如果$p&lt;\alpha$，则在显著性水平为$\alpha$下拒绝$H_0$。</li>
<li>如果$p&gt;\alpha$，则在显著性水平为$\alpha$下接受$H_0$。</li>
</ul>
</li>
</ol>
<ol start="7">
<li><p><strong>小明在一次班干部二人竞选中，支持率为百分之五十五，而置信水平0.95以上的置信区间为百分之五十到百分之六十，请问小明未当选的可能性有可能是<br> A. 40%<br> B. 50%<br> C. 5%<br> D. 3%</strong></p>
<p> 这里小明在一次班干部二人竞选中，支持率为55%（真实支持率），而置信水平0.95以上的置信区间为50%到60%。也就是说他的真实支持率只有95%的几率落在50%到60%，因此它的真实支持率不足一半的可能性小于5%。这里是两人竞选，未当选也就是支持率不足一半，因此小明未当选的可能性是小于5%的，可能是3%。</p>
</li>
</ol>
<ol start="8">
<li><p>**设${x_n}$服从独立同分布，$E[x_n] = 0, Var(x_n)=1$，则当n趋向于无穷大时，下式值为：$$\frac{x_1^2+x_2x_3+x_4^2+x_5x_6+\cdots+x_{3n-2}^2+x_{3n-1}x_{3n}}{n}$$</p>
<p> A. 无穷大<br> B. 0<br> C. 1<br> D. 2**</p>
<p> 选C。分两部分用大数定律：<br> $$\frac{x_1^2+x_4^2+\cdots}{n}\longrightarrow E(x_1^2)=Var(x_1)+E(x_1)^2=1$$</p>
<p> $$\frac{x_2x_3+x_5x_6+\cdots}{n}\longrightarrow E(x_2x_3)=E(x_2)E(x_3)=0$$</p>
</li>
</ol>
<ol start="9">
<li><p><strong>一个盒子中有三个大小相同的球，这三个球可能是红和蓝两种颜色，并且一个球是红的还是蓝的是等可能的。已知其中有一个是红色的，那么至少有一个球是蓝色的概率是多少（ ）<br> A. 7/8<br> B. 3/4<br> C. 6/7<br> D. 1/3</strong></p>
<p> 选C。共有4种情况，每种情况概率如下：<br> 3R - $(1/2)^3=1/8$<br> 2R1B - $C_3^2 (1/2)^2(1/2)=3/8$<br> 1R2B - $C_3^1 (1/2)(1/2)^2=3/8$<br> 3B - $(1/2)^3=1/8$<br> $$P(至少1B|1R)=\frac{P(至少1B, 1R)}{P(1R)}=\frac{6/8}{7/8}=6/7$$</p>
</li>
</ol>
<ol start="10">
<li><p><strong>在5张卡片上按顺序写上laval这五个字母，并依次放入5个盒中，有人从中任意取出两张卡片使用，但是在放回时，忘记了两张卡片各自的位置，求此人将卡片随意放回两个空盒子后卡片顺序仍为laval的概率 ( )<br>A. 3/5<br>B. 1/2<br>C. 1/5<br>D. 2/5</strong></p>
<p>选A。总共$C_5^2=10$个选择，两种情况会不变</p>
<ul>
<li>选中两个相同的字母，放回不变概率是1：$2\times 1$</li>
<li>选中两个不同的字母，放回不变概率是0.5：$8\times 0.5$</li>
</ul>
<p>$$\frac{2+4}{10}=3/5$$</p>
</li>
</ol>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
  </entry>
  <entry>
    <title>MySQL学习笔记</title>
    <url>/2020/05/01/mysql-notes/</url>
    <content><![CDATA[<h2 id="笔记"><a href="#笔记" class="headerlink" title="笔记"></a>笔记</h2><ol>
<li><strong>更改数据库名字。</strong><br> 例如 db_1 想要改成 db_2：<br> 在Navicat打开数据库db_1，右键-&gt;转储SQL文件-&gt;结构和数据，选择存储的文件夹位置，存为db_1.sql文件；<br> 记事本打开db_1.sql，更改<code>Source Database: db_1</code>为<code>Source Database: db_2</code><br> 在Navicat新建数据库db_2，右键-&gt;运行SQL文件，选择刚才的db_1.sql文件，运行可copy刚才db_1的数据，完成后删除db_1即可。</li>
</ol>
<ol start="2">
<li><strong>导入csv文件，选了utf8编码依然显示乱码。</strong><br> 将csv文件用记事本打开另存为utf8编码，或者是用Excel打开另存为csv的utf8编码格式，再导入即可解决。</li>
</ol>
<ol start="3">
<li><strong>从客户端拖拽导入大量级的数据文件很慢。</strong><br> 在MySQL终端用coding导入更快。 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; load data local infile &#39;G:\\practice01\\order_info_utf.csv&#39; into table practice01.</span><br><span class="line">orderinfo</span><br><span class="line">    -&gt; fields terminated by &#39;,&#39;;</span><br><span class="line">Query OK, 539414 rows affected, 65535 warnings (34.19 sec)</span><br><span class="line">Records: 539414  Deleted: 0  Skipped: 0  Warnings: 77610</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="4">
<li><strong>在开始菜单点击MySQL的终端（MySQL 8.0 Command Line Client -  Unicode），运行以上语句导入数据报错：<br> ERROR 3948 (42000): Loading local data is disabled; this must be enabled on both the client and server sides</strong><br> 在cmd进入MySQL的bin文件夹，运行MySQL，设置 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql&gt; set global local_infile&#x3D;on;</span><br></pre></td></tr></table></figure>
 可用。</li>
</ol>
<ol start="5">
<li><p><strong>时间函数</strong><br> <code>DATE(data)</code>：返回日期<br> <code>DATE_FORMAT(date,format)</code>：返回自定义format的日期</p>
<table>
<thead>
<tr>
<th>格式</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td>%Y</td>
<td>年，4 位</td>
</tr>
<tr>
<td>%y</td>
<td>年，2 位</td>
</tr>
<tr>
<td>%M</td>
<td>月名，March之类</td>
</tr>
<tr>
<td>%m</td>
<td>月，数值(00-12)</td>
</tr>
</tbody></table>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> paidTime, <span class="built_in">DATE</span>(paidTime), <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m'</span>) <span class="keyword">FROM</span> orderinfo</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5eac3968c2a9a83be5afed3d.png'>

<p> <code>DATE_ADD(data,INTERVAL 1 DAY)</code>：数字+DAY/WEEK/MONTH/YEAR</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> paidTime, <span class="keyword">DATE_ADD</span>(paidTime,<span class="built_in">INTERVAL</span> <span class="number">1</span> <span class="keyword">DAY</span>) <span class="keyword">FROM</span> orderinfo</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5eac475dc2a9a83be5c00beb.png'>



</li>
</ol>
<ol start="6">
<li><strong>IF函数</strong><br> <code>IF(expr,value1,value2)</code>：如果expr是True，返回value1，否则返回value2</li>
</ol>
<ol start="7">
<li><strong>COUNT函数</strong><br> <code>COUNT(expr)</code><br> 或者 <code>COUNT(1)</code> <code>COUNT(*)</code>，默认对行进行计数</li>
</ol>
<ol start="8">
<li><strong>LIMIT</strong><br> 限制提取的数据量<br> <code>LIMIT 5</code>：0<del>4<br> <code>LIMIT 5,10</code>：6</del>15<br> <code>LIMIT 50,-1</code>：51~最后</li>
</ol>
<hr>
<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><ol>
<li>哪个不是DDL(数据库定义语言)语句？<br> A. ALTER<br> B. CREATE<br> C. RENAME<br> D. GRANT <details>
 <summary><u>答案：</u></summary>
 D.
 DDL（Data Denifition Language）：数据定义语言关键字：create、drop、alter、rename创建、删除、更改表结构、rename table 旧表名 to 新表名
 DML（Data Manipulation Language）：数据操作语言关键字：insert、delete、update插入、删除、更改数据
 DQL（Data Query Language）：数据查询语言关键字：select查询数据
 TCL（Trasactional Conrtol Language）：事务控制语言关键字：commit、rollback用来提交和回滚事务
 DCL（Data Conrtol Language）：数据控制语言关键字：grant、revoke用来设置或更改数据库用户或角色权限
 </details>



</li>
</ol>
<ol start="2">
<li>对于SQL语句select * from t where a=100 and b=200，哪个索引可以使用到？<br> A. 索引idx_b(b)<br> B. 索引idx_b_a(b,a)<br> C. 索引idx_a_b(a,b)<br> D. 都可以 <details>
 <summary><u>答案：</u></summary>
 D.
 </details>



</li>
</ol>
<ol start="3">
<li>若要在员工信息表EMP中增加一列WANGYI_NO（网易id），可用（ ）。<br> A. ADD TABLE EMP (WANGYI_NO CHAR (10))<br> B. ADD TABLE EMP ALTER (WANGYI_NO CHAR (10))<br> C. ALTER TABLE EMP ADD (WANGYI_NO CHAR (10))<br> D. ALTER TABLE EMP (ADD WANGYI_NO CHAR (10)) <details>
 <summary><u>答案：</u></summary>
 C.
 </details>






</li>
</ol>
<hr>
<h2 id="Practice01"><a href="#Practice01" class="headerlink" title="Practice01"></a>Practice01</h2><ol>
<li>统计不同月份的下单人数 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m'</span>), <span class="keyword">COUNT</span>(<span class="keyword">DISTINCT</span> userId) <span class="keyword">FROM</span> orderinfo</span><br><span class="line"><span class="keyword">WHERE</span> isPaid = <span class="string">'已支付'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m'</span>)</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5eacd5c8c2a9a83be5396f02.png'>



</li>
</ol>
<ol start="2">
<li><p>统计用户三月份的复购率和回购率<br> <strong>衡量用户的消费欲望的指标</strong><br> 复购率：一段时间内多次消费的用户占总消费用户的比率。如，4月有1000位用户消费，其中500位消费了两次以上，复购率为50%。衡量较高频的消费欲望，如外卖。<br> 回购率：一段时间内消费过的用户，在下一段时间内仍旧消费的占比。如，4月的消费用户数1000位，其中600位在5月继续消费，回购率为60%。衡量较中频的消费欲望，如本月购买，下月再会购买。</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(num_user), <span class="keyword">COUNT</span>(<span class="keyword">IF</span>(num_user&gt;<span class="number">1</span>,<span class="number">1</span>,<span class="literal">NULL</span>)) <span class="keyword">FROM</span></span><br><span class="line">    <span class="comment"># COUNT(num_user): 全部的用户数</span></span><br><span class="line">    (<span class="keyword">SELECT</span> userId, <span class="keyword">COUNT</span>(userId) <span class="keyword">AS</span> num_user <span class="keyword">FROM</span> orderinfo</span><br><span class="line">        <span class="keyword">WHERE</span>	isPaid = <span class="string">'已支付'</span></span><br><span class="line">        <span class="keyword">AND</span> <span class="keyword">DATE_FORMAT</span>(paidTime, <span class="string">'%Y-%m'</span>) = <span class="string">'2016-03'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> userId) t</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5eace1d0c2a9a83be546560c.png'>
 复购率：16916/54799=30.87%

 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> t1.mon, <span class="keyword">COUNT</span>(t1.mon), <span class="keyword">COUNT</span>(t2.mon) <span class="keyword">FROM</span></span><br><span class="line">    (<span class="keyword">SELECT</span> userId, <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m-01'</span>) <span class="keyword">AS</span> mon <span class="keyword">FROM</span> orderinfo</span><br><span class="line">    <span class="keyword">WHERE</span> isPaid = <span class="string">'已支付'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> userId, <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m-01'</span>)) t1</span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">JOIN</span></span><br><span class="line">    (<span class="keyword">SELECT</span> userId, <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m-01'</span>) <span class="keyword">AS</span> mon <span class="keyword">FROM</span> orderinfo</span><br><span class="line">    <span class="keyword">WHERE</span> isPaid = <span class="string">'已支付'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> userId, <span class="keyword">DATE_FORMAT</span>(paidTime,<span class="string">'%Y-%m-01'</span>)) t2</span><br><span class="line"><span class="keyword">ON</span> t1.userId=t2.userId <span class="keyword">AND</span> t1.mon=<span class="keyword">DATE_SUB</span>(t2.mon, <span class="built_in">INTERVAL</span> <span class="number">1</span> <span class="keyword">MONTH</span>)</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> t1.mon</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5ead291cc2a9a83be58e68d5.png'>

<p> 说明：self join 得到的结果</p>
 <img src='https://pic.downk.cc/item/5ead28bfc2a9a83be58e25a7.png'>
 如果t2中减去一个月的日期没和t1匹配上，是NULL，那就表示本月用户在上月并没有消费。



</li>
</ol>
<ol start="3">
<li>统计男女用户的消费频次是否有差异 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> sex, <span class="keyword">AVG</span>(<span class="keyword">num</span>) <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> o.userId,sex,<span class="keyword">COUNT</span>(<span class="number">1</span>) <span class="keyword">AS</span> <span class="keyword">num</span> <span class="keyword">FROM</span> orderinfo o</span><br><span class="line">    <span class="keyword">JOIN</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> * <span class="keyword">FROM</span> userinfo</span><br><span class="line">        <span class="keyword">WHERE</span> sex &lt;&gt; <span class="string">''</span>) u</span><br><span class="line">    <span class="keyword">ON</span> o.userId=u.userId</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> o.userId, sex </span><br><span class="line">    <span class="comment"># 同一用户存在多次消费，所以先group by userId</span></span><br><span class="line">) t</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> sex</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5ead2d5bc2a9a83be5916241.png'>



</li>
</ol>
<ol start="4">
<li>统计多次消费的用户，第一次和最后一次消费间隔是多少 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> userId, <span class="keyword">DATEDIFF</span>(<span class="keyword">max</span>(paidTime),<span class="keyword">min</span>(paidTime)) <span class="keyword">AS</span> <span class="keyword">DAYS</span> <span class="keyword">FROM</span> orderinfo</span><br><span class="line"><span class="keyword">WHERE</span> isPaid = <span class="string">'已支付'</span></span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> userId</span><br><span class="line"><span class="keyword">HAVING</span> <span class="keyword">COUNT</span>(<span class="number">1</span>)&gt;<span class="number">1</span></span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5ead321bc2a9a83be5954cfc.png'>



</li>
</ol>
<ol start="5">
<li>统计不同年龄段，用户的消费金额是否有差异 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> age, <span class="keyword">AVG</span>(price) <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> o.userId,age, price <span class="keyword">FROM</span> orderinfo o</span><br><span class="line">    <span class="keyword">JOIN</span> (</span><br><span class="line">        <span class="keyword">SELECT</span> userId, <span class="keyword">CEIL</span>((<span class="keyword">YEAR</span>(<span class="keyword">NOW</span>())-<span class="keyword">YEAR</span>(birth))/<span class="number">10</span>) <span class="keyword">AS</span> age <span class="keyword">FROM</span> userinfo</span><br><span class="line">        <span class="keyword">WHERE</span> birth&gt;<span class="string">'1901-00-00'</span></span><br><span class="line">    ) u</span><br><span class="line">    <span class="keyword">ON</span> o.userId=u.userId</span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> o.userId, age</span><br><span class="line">    <span class="keyword">WHERE</span> isPaid = <span class="string">'已支付'</span></span><br><span class="line">) t</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> age</span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> age</span><br></pre></td></tr></table></figure>
 <img src='https://pic.downk.cc/item/5ead3a1fc2a9a83be59dfeaa.png'>



</li>
</ol>
<ol start="6">
<li><p>统计消费的二八法则，消费的top20%用户，贡献了多少额度<br> 有多少用户：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(userId),<span class="keyword">SUM</span>(total) <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> userId, <span class="keyword">SUM</span>(price) <span class="keyword">AS</span> total <span class="keyword">FROM</span> orderinfo</span><br><span class="line">    <span class="keyword">WHERE</span> isPaid=<span class="string">'已支付'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> userId</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> total <span class="keyword">DESC</span></span><br><span class="line">) t</span><br></pre></td></tr></table></figure>

 <imr src='https://pic.downk.cc/item/5ead3dfec2a9a83be5a1ebd7.png'>

<p> 85649*20%=17130，加多个LIMIT限制只取前20%：</p>
 <figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="keyword">COUNT</span>(userId),<span class="keyword">SUM</span>(total) <span class="keyword">FROM</span> (</span><br><span class="line">    <span class="keyword">SELECT</span> userId, <span class="keyword">SUM</span>(price) <span class="keyword">AS</span> total <span class="keyword">FROM</span> orderinfo</span><br><span class="line">    <span class="keyword">WHERE</span> isPaid=<span class="string">'已支付'</span></span><br><span class="line">    <span class="keyword">GROUP</span> <span class="keyword">BY</span> userId</span><br><span class="line">    <span class="keyword">ORDER</span> <span class="keyword">BY</span> total <span class="keyword">DESC</span></span><br><span class="line">    <span class="keyword">LIMIT</span> <span class="number">17130</span></span><br><span class="line">) t</span><br></pre></td></tr></table></figure>

 <img src='https://pic.downk.cc/item/5ead3ed0c2a9a83be5a2a97d.png'></li>
</ol>
]]></content>
      <categories>
        <category>Data</category>
      </categories>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>主成分分析 (PCA) 和因子分析 (FA)</title>
    <url>/2020/04/29/pca-and-fa/</url>
    <content><![CDATA[<h2 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h2><p>Principal Components Analysis</p>
<h3 id="基变换"><a href="#基变换" class="headerlink" title="基变换"></a>基变换</h3><ul>
<li>标准正交基 $(\vec{i},\vec{j})\rightarrow(\vec{u},\vec{v})$<br>$P_0=(\vec{i},\vec{j})=\begin{pmatrix}1&amp;0\\0&amp;1\end{pmatrix}, \ \vec{i}=\begin{pmatrix}1\\0\end{pmatrix},\vec{j}=\begin{pmatrix}0\\1\end{pmatrix}$<br>设 $P=(\vec{u},\vec{v})$，那么<br>$PX_{(u,v)}=P_0X_{(i,j)} \ \Longrightarrow \ X_{(u,v)}=P^{-1}X_{(i,j)}$<br>而 $P^{-1}=P^{T}$（因为标准正交基$PP^{T}=E$）<br>所以基变换后的坐标计算就是 $X_{(u,v)}=P^{T}X_{(i,j)}$</li>
</ul>
<hr>
<p>CodingLabs<br>keep coding, keep foolish<br>首页<br>|<br>标签<br>|<br>关于我<br>|<br>+订阅<br>|<br>微博<br>PCA的数学原理<br>作者 张洋 | 发布于 2013-06-22<br>机器学习 线性代数 PCA<br>PCA（Principal Component Analysis）是一种常用的数据分析方法。PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。网上关于PCA的文章有很多，但是大多数只描述了PCA的分析过程，而没有讲述其中的原理。这篇文章的目的是介绍PCA的基本数学原理，帮助读者了解PCA的工作机制是什么。</p>
<p>当然我并不打算把文章写成纯数学文章，而是希望用直观和易懂的方式叙述PCA的数学原理，所以整个文章不会引入严格的数学推导。希望读者在看完这篇文章后能更好的明白PCA的工作原理。</p>
<p>数据的向量表示及降维问题<br>一般情况下，在数据挖掘和机器学习中，数据被表示为向量。例如某个淘宝店2012年全年的流量及交易情况可以看成一组记录的集合，其中每一天的数据是一条记录，格式如下：</p>
<p>(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)</p>
<p>其中“日期”是一个记录标志而非度量值，而数据挖掘关心的大多是度量值，因此如果我们忽略日期这个字段后，我们得到一组记录，每条记录可以被表示为一个五维向量，其中一条看起来大约是这个样子：</p>
<p>$$(500,240,25,13,2312.15)^\mathsf{T}$$</p>
<p>注意这里我用了转置，因为习惯上使用列向量表示一条记录（后面会看到原因），本文后面也会遵循这个准则。不过为了方便有时我会省略转置符号，但我们说到向量默认都是指列向量。</p>
<p>我们当然可以对这一组五维向量进行分析和挖掘，不过我们知道，很多机器学习算法的复杂度和数据的维数有着密切关系，甚至与维数呈指数级关联。当然，这里区区五维的数据，也许还无所谓，但是实际机器学习中处理成千上万甚至几十万维的情况也并不罕见，在这种情况下，机器学习的资源消耗是不可接受的，因此我们必须对数据进行降维。</p>
<p>降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。</p>
<p>举个例子，假如某学籍数据有两列M和F，其中M列的取值是如何此学生为男性取值1，为女性取值0；而F列是学生为女性取值1，男性取值0。此时如果我们统计全部学籍数据，会发现对于任何一条记录来说，当M为1时F必定为0，反之当M为0时F必定为1。在这种情况下，我们将M或F去掉实际上没有任何信息的损失，因为只要保留一列就可以完全还原另一列。</p>
<p>当然上面是一个极端的情况，在现实中也许不会出现，不过类似的情况还是很常见的。例如上面淘宝店铺的数据，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。</p>
<p>这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。</p>
<p>上面给出的是降维的朴素思想描述，可以有助于直观理解降维的动机和可行性，但并不具有操作指导意义。例如，我们到底删除哪一列损失的信息才最小？亦或根本不是单纯删除几列，而是通过某些变换将原始数据变为更少的列但又使得丢失的信息最小？到底如何度量丢失信息的多少？如何根据原始数据决定具体的降维操作步骤？</p>
<p>要回答上面的问题，就要对降维问题进行数学化和形式化的讨论。而PCA是一种具有严格数学基础并且已被广泛采用的降维方法。下面我不会直接描述PCA，而是通过逐步分析问题，让我们一起重新“发明”一遍PCA。</p>
<p>向量的表示及基变换<br>既然我们面对的数据被抽象为一组向量，那么下面有必要研究一些向量的数学性质。而这些数学性质将成为后续导出PCA的理论基础。</p>
<p>内积与投影<br>下面先来看一个高中就学过的向量运算：内积。两个维数相同的向量的内积被定义为：</p>
<p>$$(a_1,a_2,\cdots,a_n)^\mathsf{T}\cdot (b_1,b_2,\cdots,b_n)^\mathsf{T}=a_1b_1+a_2b_2+\cdots+a_nb_n$$</p>
<p>内积运算将两个向量映射为一个实数。其计算方式非常容易理解，但是其意义并不明显。下面我们分析内积的几何意义。假设A和B是两个n维向量，我们知道n维向量可以等价表示为n维空间中的一条从原点发射的有向线段，为了简单起见我们假设A和B均为二维向量，则$A=(x_1,y_1)$，$B=(x_2,y_2)$。则在二维平面上A和B可以用两条发自原点的有向线段表示，见下图：</p>
<p>好，现在我们从A点向B所在直线引一条垂线。我们知道垂线与B的交点叫做A在B上的投影，再设A与B的夹角是a，则投影的矢量长度为$|A|cos(a)$，其中$|A|=\sqrt{x_1^2+y_1^2}$是向量A的模，也就是A线段的标量长度。</p>
<p>注意这里我们专门区分了矢量长度和标量长度，标量长度总是大于等于0，值就是线段的长度；而矢量长度可能为负，其绝对值是线段长度，而符号取决于其方向与标准方向相同或相反。</p>
<p>到这里还是看不出内积和这东西有什么关系，不过如果我们将内积表示为另一种我们熟悉的形式：</p>
<p>$$A\cdot B=|A||B|cos(a)$$</p>
<p>现在事情似乎是有点眉目了：A与B的内积等于A到B的投影长度乘以B的模。再进一步，如果我们假设B的模为1，即让$|B|=1$，那么就变成了：</p>
<p>$$A\cdot B=|A|cos(a)$$</p>
<p>也就是说，设向量B的模为1，则A与B的内积值等于A向B所在直线投影的矢量长度！这就是内积的一种几何解释，也是我们得到的第一个重要结论。在后面的推导中，将反复使用这个结论。</p>
<p>基<br>下面我们继续在二维空间内讨论向量。上文说过，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一个有向线段。例如下面这个向量：</p>
<p>在代数表示方面，我们经常用线段终点的点坐标表示向量，例如上面的向量可以表示为(3,2)，这是我们再熟悉不过的向量表示。</p>
<p>不过我们常常忽略，只有一个(3,2)本身是不能够精确表示一个向量的。我们仔细看一下，这里的3实际表示的是向量在x轴上的投影值是3，在y轴上的投影值是2。也就是说我们其实隐式引入了一个定义：以x轴和y轴上正方向长度为1的向量为标准。那么一个向量(3,2)实际是说在x轴投影为3而y轴的投影为2。注意投影是一个矢量，所以可以为负。</p>
<p>更正式的说，向量(x,y)实际上表示线性组合：</p>
<p>$$x(1,0)^\mathsf{T}+y(0,1)^\mathsf{T}$$</p>
<p>不难证明所有二维向量都可以表示为这样的线性组合。此处(1,0)和(0,1)叫做二维空间中的一组基。</p>
<p>所以，要准确描述向量，首先要确定一组基，然后给出在基所在的各个直线上的投影值，就可以了。只不过我们经常省略第一步，而默认以(1,0)和(0,1)为基。</p>
<p>我们之所以默认选择(1,0)和(0,1)为基，当然是比较方便，因为它们分别是x和y轴正方向上的单位向量，因此就使得二维平面上点坐标和向量一一对应，非常方便。但实际上任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不在一条直线上的向量。</p>
<p>例如，(1,1)和(-1,1)也可以成为一组基。一般来说，我们希望基的模是1，因为从内积的意义可以看到，如果基的模是1，那么就可以方便的用向量点乘基而直接获得其在新基上的坐标了！实际上，对应任何一个向量我们总可以找到其同方向上模为1的向量，只要让两个分量分别除以模就好了。例如，上面的基可以变为$(\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$和$(-\frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}})$。</p>
<p>现在，我们想获得(3,2)在新基上的坐标，即在两个方向上的投影矢量值，那么根据内积的几何意义，我们只要分别计算(3,2)和两个基的内积，不难得到新的坐标为$(\frac{5}{\sqrt{2}},-\frac{1}{\sqrt{2}})$。下图给出了新的基以及(3,2)在新基上坐标值的示意图：</p>
<p>另外这里要注意的是，我们列举的例子中基是正交的（即内积为0，或直观说相互垂直），但可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的。不过因为正交基有较好的性质，所以一般使用的基都是正交的。</p>
<p>基变换的矩阵表示<br>下面我们找一种简便的方式来表示基变换。还是拿上面的例子，想一下，将(3,2)变换为新基上的坐标，就是用(3,2)与第一个基做内积运算，作为第一个新的坐标分量，然后用(3,2)与第二个基做内积运算，作为第二个新坐标的分量。实际上，我们可以用矩阵相乘的形式简洁的表示这个变换：</p>
<p>$$\begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix} \begin{pmatrix} 3 \\ 2 \end{pmatrix} = \begin{pmatrix} 5/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix}$$</p>
<p>太漂亮了！其中矩阵的两行分别为两个基，乘以原向量，其结果刚好为新基的坐标。可以稍微推广一下，如果我们有m个二维向量，只要将二维向量按列排成一个两行m列矩阵，然后用“基矩阵”乘以这个矩阵，就得到了所有这些向量在新基下的值。例如(1,1)，(2,2)，(3,3)，想变换到刚才那组基上，则可以这样表示：</p>
<p>$$\begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix} \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 1 &amp; 2 &amp; 3 \end{pmatrix} = \begin{pmatrix} 2/\sqrt{2} &amp; 4/\sqrt{2} &amp; 6/\sqrt{2} \\ 0 &amp; 0 &amp; 0 \end{pmatrix}$$</p>
<p>于是一组向量的基变换被干净的表示为矩阵的相乘。</p>
<p>一般的，如果我们有M个N维向量，想将其变换为由R个N维向量表示的新空间中，那么首先将R个基按行组成矩阵A，然后将向量按列组成矩阵B，那么两矩阵的乘积AB就是变换结果，其中AB的第m列为A中第m列变换后的结果。</p>
<p>数学表示为：</p>
<p>$$\begin{pmatrix} p_1 \\ p_2 \\ \vdots \\ p_R \end{pmatrix} \begin{pmatrix} a_1 &amp; a_2 &amp; \cdots &amp; a_M \end{pmatrix} = \begin{pmatrix} p_1a_1 &amp; p_1a_2 &amp; \cdots &amp; p_1a_M \\ p_2a_1 &amp; p_2a_2 &amp; \cdots &amp; p_2a_M \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ p_Ra_1 &amp; p_Ra_2 &amp; \cdots &amp; p_Ra_M \end{pmatrix}$$</p>
<p>其中$p_i$是一个行向量，表示第i个基，$a_j$是一个列向量，表示第j个原始数据记录。</p>
<p>特别要注意的是，这里R可以小于N，而R决定了变换后数据的维数。也就是说，我们可以将一N维数据变换到更低维度的空间中去，变换后的维度取决于基的数量。因此这种矩阵相乘的表示也可以表示降维变换。</p>
<p>最后，上述分析同时给矩阵相乘找到了一种物理解释：两个矩阵相乘的意义是将右边矩阵中的每一列列向量变换到左边矩阵中每一行行向量为基所表示的空间中去。更抽象的说，一个矩阵可以表示一种线性变换。很多同学在学线性代数时对矩阵相乘的方法感到奇怪，但是如果明白了矩阵相乘的物理意义，其合理性就一目了然了。</p>
<p>协方差矩阵及优化目标<br>上面我们讨论了选择不同的基可以对同样一组数据给出不同的表示，而且如果基的数量少于向量本身的维数，则可以达到降维的效果。但是我们还没有回答一个最最关键的问题：如何选择基才是最优的。或者说，如果我们有一组N维向量，现在要将其降到K维（K小于N），那么我们应该如何选择K个基才能最大程度保留原有的信息？</p>
<p>要完全数学化这个问题非常繁杂，这里我们用一种非形式化的直观方法来看这个问题。</p>
<p>为了避免过于抽象的讨论，我们仍以一个具体的例子展开。假设我们的数据由五条记录组成，将它们表示成矩阵形式：</p>
<p>$$\begin{pmatrix} 1 &amp; 1 &amp; 2 &amp; 4 &amp; 2 \\ 1 &amp; 3 &amp; 3 &amp; 4 &amp; 4 \end{pmatrix}$$</p>
<p>其中每一列为一条数据记录，而一行为一个字段。为了后续处理方便，我们首先将每个字段内所有值都减去字段均值，其结果是将每个字段都变为均值为0（这样做的道理和好处后面会看到）。</p>
<p>我们看上面的数据，第一个字段均值为2，第二个字段均值为3，所以变换后：</p>
<p>$$\begin{pmatrix} -1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \end{pmatrix}$$</p>
<p>我们可以看下五条数据在平面直角坐标系内的样子：</p>
<p>现在问题来了：如果我们必须使用一维来表示这些数据，又希望尽量保留原始的信息，你要如何选择？</p>
<p>通过上一节对基变换的讨论我们知道，这个问题实际上是要在二维平面中选择一个方向，将所有数据都投影到这个方向所在直线上，用投影值表示原始记录。这是一个实际的二维降到一维的问题。</p>
<p>那么如何选择这个方向（或者说基）才能尽量保留最多的原始信息呢？一种直观的看法是：希望投影后的投影值尽可能分散。</p>
<p>以上图为例，可以看出如果向x轴投影，那么最左边的两个点会重叠在一起，中间的两个点也会重叠在一起，于是本身四个各不相同的二维点投影后只剩下两个不同的值了，这是一种严重的信息丢失，同理，如果向y轴投影最上面的两个点和分布在x轴上的两个点也会重叠。所以看来x和y轴都不是最好的投影选择。我们直观目测，如果向通过第一象限和第三象限的斜线投影，则五个点在投影后还是可以区分的。</p>
<p>下面，我们用数学方法表述这个问题。</p>
<p>方差<br>上文说到，我们希望投影后投影值尽可能分散，而这种分散程度，可以用数学上的方差来表述。此处，一个字段的方差可以看做是每个元素与字段均值的差的平方和的均值，即：</p>
<p>$$Var(a)=\frac{1}{m}\sum_{i=1}^m{(a_i-\mu)^2}$$</p>
<p>由于上面我们已经将每个字段的均值都化为0了，因此方差可以直接用每个元素的平方和除以元素个数表示：</p>
<p>$$Var(a)=\frac{1}{m}\sum_{i=1}^m{a_i^2}$$</p>
<p>于是上面的问题被形式化表述为：寻找一个一维基，使得所有数据变换为这个基上的坐标表示后，方差值最大。</p>
<p>协方差<br>对于上面二维降成一维的问题来说，找到那个使得方差最大的方向就可以了。不过对于更高维，还有一个问题需要解决。考虑三维降到二维问题。与之前相同，首先我们希望找到一个方向使得投影后方差最大，这样就完成了第一个方向的选择，继而我们选择第二个投影方向。</p>
<p>如果我们还是单纯只选择方差最大的方向，很明显，这个方向与第一个方向应该是“几乎重合在一起”，显然这样的维度是没有用的，因此，应该有其他约束条件。从直观上说，让两个字段尽可能表示更多的原始信息，我们是不希望它们之间存在（线性）相关性的，因为相关性意味着两个字段不是完全独立，必然存在重复表示的信息。</p>
<p>数学上可以用两个字段的协方差表示其相关性，由于已经让每个字段均值为0，则：</p>
<p>$$Cov(a,b)=\frac{1}{m}\sum_{i=1}^m{a_ib_i}$$</p>
<p>可以看到，在字段均值为0的情况下，两个字段的协方差简洁的表示为其内积除以元素数m。</p>
<p>当协方差为0时，表示两个字段完全独立。为了让协方差为0，我们选择第二个基时只能在与第一个基正交的方向上选择。因此最终选择的两个方向一定是正交的。</p>
<p>至此，我们得到了降维问题的优化目标：将一组N维向量降为K维（K大于0，小于N），其目标是选择K个单位（模为1）正交基，使得原始数据变换到这组基上后，各字段两两间协方差为0，而字段的方差则尽可能大（在正交的约束下，取最大的K个方差）。</p>
<p>协方差矩阵<br>上面我们导出了优化目标，但是这个目标似乎不能直接作为操作指南（或者说算法），因为它只说要什么，但根本没有说怎么做。所以我们要继续在数学上研究计算方案。</p>
<p>我们看到，最终要达到的目的与字段内方差及字段间协方差有密切关系。因此我们希望能将两者统一表示，仔细观察发现，两者均可以表示为内积的形式，而内积又与矩阵相乘密切相关。于是我们来了灵感：</p>
<p>假设我们只有a和b两个字段，那么我们将它们按行组成矩阵X：</p>
<p>$$X=\begin{pmatrix} a_1 &amp; a_2 &amp; \cdots &amp; a_m \\ b_1 &amp; b_2 &amp; \cdots &amp; b_m \end{pmatrix}$$</p>
<p>然后我们用X乘以X的转置，并乘上系数1/m：</p>
<p>$$\frac{1}{m}XX^\mathsf{T}=\begin{pmatrix} \frac{1}{m}\sum_{i=1}^m{a_i^2} &amp; \frac{1}{m}\sum_{i=1}^m{a_ib_i} \\ \frac{1}{m}\sum_{i=1}^m{a_ib_i} &amp; \frac{1}{m}\sum_{i=1}^m{b_i^2} \end{pmatrix}$$</p>
<p>奇迹出现了！这个矩阵对角线上的两个元素分别是两个字段的方差，而其它元素是a和b的协方差。两者被统一到了一个矩阵的。</p>
<p>根据矩阵相乘的运算法则，这个结论很容易被推广到一般情况：</p>
<p>设我们有m个n维数据记录，将其按列排成n乘m的矩阵X，设$C=\frac{1}{m}XX^\mathsf{T}$，则C是一个对称矩阵，其对角线分别个各个字段的方差，而第i行j列和j行i列元素相同，表示i和j两个字段的协方差。</p>
<p>协方差矩阵对角化<br>根据上述推导，我们发现要达到优化目前，等价于将协方差矩阵对角化：即除对角线外的其它元素化为0，并且在对角线上将元素按大小从上到下排列，这样我们就达到了优化目的。这样说可能还不是很明晰，我们进一步看下原矩阵与基变换后矩阵协方差矩阵的关系：</p>
<p>设原始数据矩阵X对应的协方差矩阵为C，而P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们推导一下D与C的关系：</p>
<p>$$\begin{array}{l l l} D &amp; = &amp; \frac{1}{m}YY^\mathsf{T} \\ &amp; = &amp; \frac{1}{m}(PX)(PX)^\mathsf{T} \\ &amp; = &amp; \frac{1}{m}PXX^\mathsf{T}P^\mathsf{T} \\ &amp; = &amp; P(\frac{1}{m}XX^\mathsf{T})P^\mathsf{T} \\ &amp; = &amp; PCP^\mathsf{T} \end{array}$$</p>
<p>现在事情很明白了！我们要找的P不是别的，而是能让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足$PCP^\mathsf{T}$是一个对角矩阵，并且对角元素按从大到小依次排列，那么P的前K行就是要寻找的基，用P的前K行组成的矩阵乘以X就使得X从N维降到了K维并满足上述优化条件。</p>
<p>至此，我们离“发明”PCA还有仅一步之遥！</p>
<p>现在所有焦点都聚焦在了协方差矩阵对角化问题上，有时，我们真应该感谢数学家的先行，因为矩阵对角化在线性代数领域已经属于被玩烂了的东西，所以这在数学上根本不是问题。</p>
<p>由上文知道，协方差矩阵C是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：</p>
<p>1）实对称矩阵不同特征值对应的特征向量必然正交。</p>
<p>2）设特征向量$\lambda$重数为r，则必然存在r个线性无关的特征向量对应于$\lambda$，因此可以将这r个特征向量单位正交化。</p>
<p>由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量，设这n个特征向量为$e_1,e_2,\cdots,e_n$，我们将其按列组成矩阵：</p>
<p>$$E=\begin{pmatrix} e_1 &amp; e_2 &amp; \cdots &amp; e_n \end{pmatrix}$$</p>
<p>则对协方差矩阵C有如下结论：</p>
<p>$$E^\mathsf{T}CE=\Lambda=\begin{pmatrix} \lambda_1 &amp; &amp; &amp; \\ &amp; \lambda_2 &amp; &amp; \\ &amp; &amp; \ddots &amp; \\ &amp; &amp; &amp; \lambda_n \end{pmatrix}$$</p>
<p>其中$\Lambda$为对角矩阵，其对角元素为各特征向量对应的特征值（可能有重复）。</p>
<p>以上结论不再给出严格的数学证明，对证明感兴趣的朋友可以参考线性代数书籍关于“实对称矩阵对角化”的内容。</p>
<p>到这里，我们发现我们已经找到了需要的矩阵P：</p>
<p>$$P=E^\mathsf{T}$$</p>
<p>P是协方差矩阵的特征向量单位化后按行排列出的矩阵，其中每一行都是C的一个特征向量。如果设P按照$\Lambda$中特征值的从大到小，将特征向量从上到下排列，则用P的前K行组成的矩阵乘以原始数据矩阵X，就得到了我们需要的降维后的数据矩阵Y。</p>
<p>至此我们完成了整个PCA的数学原理讨论。在下面的一节，我们将给出PCA的一个实例。</p>
<p>算法及实例<br>为了巩固上面的理论，我们在这一节给出一个具体的PCA实例。</p>
<p>PCA算法<br>总结一下PCA的算法步骤：</p>
<p>设有m条n维数据。</p>
<p>1）将原始数据按列组成n行m列矩阵X</p>
<p>2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值</p>
<p>3）求出协方差矩阵$C=\frac{1}{m}XX^\mathsf{T}$</p>
<p>4）求出协方差矩阵的特征值及对应的特征向量</p>
<p>5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P</p>
<p>6）$Y=PX$即为降维到k维后的数据</p>
<p>实例<br>这里以上文提到的</p>
<p>$$\begin{pmatrix} -1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \end{pmatrix}$$</p>
<p>为例，我们用PCA方法将这组二维数据其降到一维。</p>
<p>因为这个矩阵的每行已经是零均值，这里我们直接求协方差矩阵：</p>
<p>$$C=\frac{1}{5}\begin{pmatrix} -1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \end{pmatrix}\begin{pmatrix} -1 &amp; -2 \\ -1 &amp; 0 \\ 0 &amp; 0 \\ 2 &amp; 1 \\ 0 &amp; 1 \end{pmatrix}=\begin{pmatrix} \frac{6}{5} &amp; \frac{4}{5} \\ \frac{4}{5} &amp; \frac{6}{5} \end{pmatrix}$$</p>
<p>然后求其特征值和特征向量，具体求解方法不再详述，可以参考相关资料。求解后特征值为：</p>
<p>$$\lambda_1=2,\lambda_2=2/5$$</p>
<p>其对应的特征向量分别是：</p>
<p>$$c_1\begin{pmatrix} 1 \\ 1 \end{pmatrix},c_2\begin{pmatrix} -1 \\ 1 \end{pmatrix}$$</p>
<p>其中对应的特征向量分别是一个通解，$c_1$和$c_2$可取任意实数。那么标准化后的特征向量为：</p>
<p>$$\begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix},\begin{pmatrix} -1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}$$</p>
<p>因此我们的矩阵P是：</p>
<p>$$P=\begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}$$</p>
<p>可以验证协方差矩阵C的对角化：</p>
<p>$$PCP^\mathsf{T}=\begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}\begin{pmatrix} 6/5 &amp; 4/5 \\ 4/5 &amp; 6/5 \end{pmatrix}\begin{pmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}=\begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2/5 \end{pmatrix}$$</p>
<p>最后我们用P的第一行乘以数据矩阵，就得到了降维后的表示：</p>
<p>$$Y=\begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}\begin{pmatrix} -1 &amp; -1 &amp; 0 &amp; 2 &amp; 0 \\ -2 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \end{pmatrix}=\begin{pmatrix} -3/\sqrt{2} &amp; -1/\sqrt{2} &amp; 0 &amp; 3/\sqrt{2} &amp; -1/\sqrt{2} \end{pmatrix}$$</p>
<p>降维投影结果如下图：</p>
<p>进一步讨论<br>根据上面对PCA的数学原理的解释，我们可以了解到一些PCA的能力和限制。PCA本质上是将方差最大的方向作为主要特征，并且在各个正交方向上将数据“离相关”，也就是让它们在不同正交方向上没有相关性。</p>
<p>因此，PCA也存在一些限制，例如它可以很好的解除线性相关，但是对于高阶相关性就没有办法了，对于存在高阶相关性的数据，可以考虑Kernel PCA，通过Kernel函数将非线性相关转为线性相关，关于这点就不展开讨论了。另外，PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>
<p>最后需要说明的是，PCA是一种无参数技术，也就是说面对同样的数据，如果不考虑清洗，谁来做结果都一样，没有主观参数的介入，所以PCA便于通用实现，但是本身无法个性化的优化。</p>
<p>希望这篇文章能帮助朋友们了解PCA的数学理论基础和实现原理，借此了解PCA的适用场景和限制，从而更好的使用这个算法。</p>
<p>290<br>Copyright (c) 2011-2019 CodingLabs 本博客内容采用知识共享署名 3.0 中国大陆许可协议进行许可</p>
<h2 id="因子分析"><a href="#因子分析" class="headerlink" title="因子分析"></a>因子分析</h2><p>Factors Analysis</p>
<p>那么因子分析呢，又存在两个方向，一个是探索性因子分析（exploratory factor analysis）。另一个是验证性因子分析（confirmatory factor analysis）。探索性因子分析是不确定一堆自变量背后有几个因子，我们通过这种方法试图寻找到这几个因子。而验证性因子分析是已经假设自变量背后有几个因子，试图通过这种方法去验证一下这种假设是否正确。</p>
<h3 id="EFA"><a href="#EFA" class="headerlink" title="EFA"></a>EFA</h3><h3 id="CFA"><a href="#CFA" class="headerlink" title="CFA"></a>CFA</h3>]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>主成分分析</tag>
        <tag>因子分析</tag>
      </tags>
  </entry>
  <entry>
    <title>『地平线上这个世界』</title>
    <url>/2020/04/06/voyage-blog0/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>Voyage</category>
      </categories>
  </entry>
  <entry>
    <title>NLP CheatSheet</title>
    <url>/2020/04/03/nlp-cheatsheet/</url>
    <content><![CDATA[<ol>
<li><p>请大致对比下plsa和LDA的区别。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <a href="https://zhuanlan.zhihu.com/p/30781802" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/30781802</a> #21</p>
 </details>



</li>
</ol>
<ol start="2">
<li><p>下列哪个不属于CRF模型相对HMM和MEMM模型的优势：<br> A. 特征灵活<br> B. 速度快<br> C. 可容纳较多上下文信息<br> D. 全局最优</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选B。首先，CRF，HMM，MEMM都常用来做序列标注的建模。<br> HMM一个最大的缺点就是由于其输出独立性假设，导致其不能考虑上下文的特征，限制了特征的选择。<br> MEMM则解决了HMM的问题，可以任意选择特征，但由于其在每一节点都要进行归一化，所以只能找到局部的最优值，同时也带来了标记偏见的问题，即凡是训练语料中未出现的情况全都忽略掉。<br> CRF则很好的解决了这一问题，他并不在每一个节点进行归一化，而是所有特征进行全局归一化，因此可以求得全局的最优值。</p>
 </details>



</li>
</ol>
<ol start="3">
<li><p>在HMM中，如果已知观察序列和产生观察序列的状态序列,那么可用以下哪种方法直接进行参数估计<br> A. EM算法<br> B. 维特比算法<br> C. 前向后向算法<br> D. 极大似然估计</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选D。</p>
<p> A. EM算法：只有观测序列，无状态序列时来学习模型参数，即Baum-Welch算法<br> B. 维特比算法：用动态规划解决HMM的预测问题，不是参数估计<br> C. 前向后向算法：用来算概率<br> D. 极大似然估计：即观测序列和相应的状态序列都存在时的监督学习算法，用来估计参数</p>
<p> 注意的是在给定观测序列和<strong>对应的状态序列</strong>估计模型参数，可以利用<strong>极大似然估计</strong>。如果给定观测序列，<strong>没有对应的状态序列</strong>，才用<strong>EM</strong>，将状态序列看作隐数据。</p>
 </details>

</li>
</ol>
]]></content>
      <categories>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep Learning CheatSheet</title>
    <url>/2020/04/03/dl-cheatsheet/</url>
    <content><![CDATA[<p>七月在线 - BAT机器学习面试1000题系列</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><ol>
<li>考虑某个具体问题时，你可能只有少量数据来解决这个问题。不过幸运的是你有一个类似问题已经预先训练好的神经网络。可以用下面哪种方法来利用这个预先训练好的网络？<br> A. 把除了最后一层外所有的层都冻住，重新训练最后一层<br> B. 对新数据重新训练整个模型<br> C. 只对最后几层进行调参(fine tune)<br> D. 对每一层模型进行评估，选择其中的少数来用 <details>
 <summary><u>答案：</u></summary>
 选C。如果有个pre-trained的神经网络，就相当于网络各参数有个很靠谱的先验代替随机初始化。若新的少量数据来自于pre-trained数据（或者pre-trained数据量很好地描述了数据分布，而新数据采样自完全相同的分布），则冻结前面所有层而重新训练最后一层即可；但一般情况下，新数据分布跟先前训练集分布有所偏差，所以先验网络不足以完全拟合新数据时，可以冻结大部分前层网络，只对最后几层进行训练调参（fine tune）。
 </details>



</li>
</ol>
<ol start="2">
<li>下列哪一项在神经网络中引入了非线性？<br> A. 随机梯度下降<br> B. 修正线性单元（ReLU）<br> C. 卷积函数<br> D. 以上都不正确 <details>
 <summary><u>答案：</u></summary>
 选B。
 </details>



</li>
</ol>
<ol start="3">
<li><p>简单说下sigmoid激活函数。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> $$\sigma(z)=\frac{1}{1+e^{-z}}$$ 导函数 $$\sigma’(z)=\sigma(1-\sigma)$$ sigmoid函数对输入的数据做非线性变换，压缩在0<del>1之间。sigmoid函数的缺点：当输入数据过大或者过小时，导数值非常小，在BP算法训练数据时导数连乘会出现梯度消失的问题，导致参数值无法更新。所以为了避免梯度消失，现在很少使用sigmoid作为隐藏层的激活函数。而因为sigmoid的函数值在0</del>1之间，当最后的输出是做分类任务时，输出层的激活函数就用sigmoid。</p>
 </details>



</li>
</ol>
<ol start="4">
<li><p>在用sigmoid作为激活函数的时候，为什么要用交叉熵损失函数，而不用均方误差损失函数？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> $$z=wx+b\hspace{2em}a=\sigma(z)$$</p>
<p> MSE：$C=\displaystyle\frac{1}{2n}\sum_x (a-y)^2$<br> $$\begin{cases}\displaystyle\frac{\partial C}{\partial w}=(a-y)\sigma’(z)x\\\displaystyle\frac{\partial C}{\partial b}=(a-y)\sigma’(z)\end{cases}$$ 参数更新迭代：$$\begin{cases}w=w-\alpha(a-y)\sigma’(z)x\\b=b-\alpha(a-y)\sigma’(z)\end{cases}$$</p>
<p> Cross Entropy：$C=\displaystyle\frac{1}{n}\sum_x \left[y\ln a+(1-y)\ln(1-a)\right]$<br> $$\begin{cases}\displaystyle\frac{\partial C}{\partial w}=(a-y)x\\\displaystyle\frac{\partial C}{\partial b}=(a-y)\end{cases}$$ 参数更新迭代：$$\begin{cases}w=w-\alpha(a-y)x\\b=b-\alpha(a-y)\end{cases}$$</p>
<p> 两者的参数更新迭代公式差别就在：MSE有 $\sigma’$ 这一项，Cross Entropy没有。sigmoid函数的导数，当变量值比较大或比较小的时候，导数是接近于零的，这时用MSE做损失的更新是非常缓慢的。用Cross Entropy的好处是，当误差大的时候，权重更新快；当误差小的时候，权重更新慢。</p>
 </details>





</li>
</ol>
<h2 id="RNN-amp-LSTM"><a href="#RNN-amp-LSTM" class="headerlink" title="RNN &amp; LSTM"></a>RNN &amp; LSTM</h2><ol>
<li><p>LSTM结构推导，为什么比RNN好？</p>
 <details>
 <summary><u>答案：</u></summary>

 <a href="/2020/04/02/rnn-lstm/" title="LSTM 公式推导">LSTM 公式推导</a>
<p> 因为LSTM有进有出且当前的cell informaton是通过input gate控制之后叠加的，RNN是叠乘，因此LSTM可以防止梯度消失或者爆炸。</p>
 </details>



</li>
</ol>
<ol start="2">
<li>简述RNN？ <details>
 <summary><u>答案：</u></summary>


</li>
</ol>
<pre><code>&lt;/details&gt;</code></pre><h2 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h2><ol>
<li><p>什么是卷积？</p>
 <details>
 <summary><u>答案：</u></summary>

 <img src="https://pic4.zhimg.com/v2-fe4569fe1348a47119c1c1f6d509029b_r.jpg" width=400>
 对图像（不同的数据窗口数据）和滤波矩阵（一组固定的权重：因为每个神经元的多个权重固定，所以又可以看做一个恒定的filter）做内积（逐个元素相乘再求和）的操作就是所谓的『卷积』操作，也是卷积神经网络的名字来源。
 </details>



</li>
</ol>
<ol start="2">
<li><p>什么是CNN的池化pool层？</p>
 <details>
 <summary><u>答案：</u></summary>

 <img src="https://pic3.zhimg.com/v2-ecf2c6a85a22c13cb6122ae3e959bfd2_r.jpg" width=400>
 池化（pooling），简言之，即取区域平均（average pooling）或最大（max pooling）。池化层是为了保留主要的特征同时减少参数（降维），防止过拟合，提高模型泛化能力。
 </details>



</li>
</ol>
<ol start="3">
<li><p>输入图片大小为200×200，依次经过一层卷积（kernel size 5×5，padding 1，stride 2），pooling（kernel size 3×3，padding 0，stride 1），又一层卷积（kernel size 3×3，padding 1，stride 1）之后，输出特征图大小为(？) (*)</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 计算尺寸不被整除只在GoogLeNet中遇到过。卷积向下取整，池化向上取整。<br> 本题 （200-5+2<em>1）/2+1 为99.5，取99<br> （99-3）/1+1 为97<br> （97-3+2</em>1）/1+1 为97<br> 研究过网络的话看到stride为1的时候，当kernel为 3 padding为1或者kernel为5 padding为2 一看就是卷积前后尺寸不变。计算GoogLeNet全过程的尺寸也一样。</p>
 </details>



</li>
</ol>
<h2 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h2><ol>
<li><p>简述TensorFlow计算图。</p>
 <details>
 <summary><u>答案：</u></summary>
 Tensorflow框架，通过计算图（Graph）的形式来表述运算。计算图是一种有向图，TensorFlow的每一个节点都是计算图上的一个Tensor，而节点之间的边描述了计算之间的依赖关系（定义时）和数学操作（运算时）。如下两图表示：

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = x*y; b = a+z; c = tf.reduce_sum(b);</span><br></pre></td></tr></table></figure>
 <img src="https://pic.downk.cc/item/5e7c76f8504f4bcb04a1c87b.png" width=100>

<p> Tensorflow计算的过程就是利用的Tensor来建立一个计算图，然后使用Session会话来启动计算，最后得到结果的过程。<br> 不同于一般的编程语言，变量计算后，比如c=a+b，TF不会得到c的值。TF先构建一个计算图出来，然后启用一个会话（Session）来把数据作为输入，通过这个图规定的计算步骤计算，最后得到结果。</p>
 </details>
</li>
<li><p>简述TensorFlow的会话管理，使用会话的两种方式。</p>
 <details>
 <summary><u>答案：</u></summary>
 会话用来执行定义好的运算，它拥有和管理程序运行时的所有资源。当计算完成之后，需要通过关闭会话来帮助系统回收资源，否则可能导致资源泄露的问题。在TensorFlow中使用会话有两种方式。

<ul>
<li><p>需要明确调用会话<strong>生成</strong>函数和<strong>关闭</strong>会话函数</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = tf.constant([<span class="number">1.</span>,<span class="number">2.</span>],name=<span class="string">"a"</span>)  </span><br><span class="line">b = tf.constant([<span class="number">2.0</span>,<span class="number">3.0</span>],name=<span class="string">"b"</span>)  </span><br><span class="line">result = tf.add(a,b,name=<span class="string">"add"</span>)  </span><br><span class="line"><span class="comment"># 创建session  </span></span><br><span class="line">sess = tf.Session()  </span><br><span class="line"><span class="comment"># 获取运算结果  </span></span><br><span class="line">sess.run(result)  </span><br><span class="line"><span class="comment"># 关闭会话,释放资源  </span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure>
<p>  当程序因为异常而退出的时候，<code>sess.close()</code>有可能不会执行从而导致资源泄露问题的发生。</p>
</li>
<li><p>通过上下文管理器来创建Session，管理会话，不需要调用<code>sess.close()</code>来关闭会话，当上下文退出的时候会话会自动关闭和释放资源</p>
  <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:  </span><br><span class="line">    sess.run(result)</span><br></pre></td></tr></table></figure>
</details>
</li>
</ul>
</li>
<li><p>简述TensorFlow框架如何构建神经网络。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> Tensorflow通过Graph和Session的设定，对神经网络模型进行了三重解构：</p>
<ul>
<li>第一层解构是将数据与神经网络模型进行了分离，所以要先设计神经网络模型，再导入数据进行训练，从而得出神经网络节点参数；</li>
<li>第二层解构是通Graph将神经网络模型进行结构化分区，通过结构化分区把复杂的神经网络进行了解构，研究人员可以按Graph的结果对特定的神经网络模型组成部分进行局部微调，通过局部微调实现全局复杂神经网络的组建；</li>
<li>第三层次的解构就是通过Session的设计，将神经网络的运算拆解到相关的算力中心，这就导致大规模的算力组合训练复杂的人工神经网络模型成为了可能性。</details>


















</li>
</ul>
</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>rnn_lstm</title>
    <url>/2020/04/02/rnn-lstm/</url>
    <content><![CDATA[<h2 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h2><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2>]]></content>
  </entry>
  <entry>
    <title>Python CheatSheet</title>
    <url>/2020/03/30/Python-CheatSheet/</url>
    <content><![CDATA[<details>
<summary><u>答案：</u></summary>

</details>

<ol>
<li> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python my.py v1 v2</span><br></pre></td></tr></table></figure>
<p> 命令运行脚本，通过 <code>from sys import argv</code> 如何获得v2的参数值?<br> A. argv[0]<br> B. argv[1]<br> C. argv[2]<br> D. argv[3]</p>
 <details>
 <summary><u>答案：</u></summary>
 选C。sys.argv是传递给python脚本的命令行参数【字符串】列表，<b>argv[0]为该脚本自身路径</b>，其余为命令行参数。
 argv[0]: [path]/my.py
 argv[1]: v1
 argv[2]: v2
 </details>
</li>
<li><p>下面输出是<code>False</code>，怎么解释？</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.2</span> - <span class="number">1.0</span> == <span class="number">0.2</span></span><br></pre></td></tr></table></figure>
<p> A. Python的实现有错误<br> B. 浮点数无法精确表示<br> C. 布尔运算不能用于浮点数比较<br> D. Python将非0数视为False</p>
 <details>
 <summary><u>答案：</u></summary>
 选B。因为精度问题，十进制数转换为二进制进行计算的时候产生的误差：

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="number">1.2</span> - <span class="number">1.0</span></span><br></pre></td></tr></table></figure>
<pre><code>输出：
0.19999999999999996</code></pre><p> 解决办法：用decimal模块</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> decimal <span class="keyword">import</span> Decimal</span><br><span class="line">print(Decimal(<span class="string">'1.2'</span>)-Decimal(<span class="string">'1.0'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>输出：
0.2</code></pre> </details>
</li>
<li><p>如何遍历一个字典？</p>
 <details>
 <summary><u>答案：</u></summary>
 3种方式：

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = &#123;<span class="string">'num'</span>: <span class="number">1</span>, <span class="string">'name'</span>: <span class="string">'lizz'</span>, <span class="string">'age'</span>: <span class="number">16</span>&#125;</span><br></pre></td></tr></table></figure>
<p> #1. 遍历key</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> d.keys():</span><br><span class="line">    print(k)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
num
name
age</code></pre><p> #2. 遍历value</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> d.values():</span><br><span class="line">    print(v)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
1
lizz
16</code></pre><p> #3. 同时遍历key和value</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> k,v <span class="keyword">in</span> d.items():</span><br><span class="line">    print(k,v)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
num 1
name lizz
age 16</code></pre> </details>
</li>
<li><p>列表的增删改查</p>
 <details>
 <summary><u>答案：</u></summary>

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l = list((<span class="number">2</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">-5</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p> 增</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l.append(<span class="number">1000</span>)    <span class="comment"># 在最后增加</span></span><br><span class="line">l</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[2, 3, 7, -5, 3, 1000]</code></pre> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l.insert(<span class="number">1</span>,<span class="string">'new'</span>)    <span class="comment"># 在位置 1 后插入</span></span><br><span class="line">l</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[2, &apos;new&apos;, 3, 7, -5, 3, 1000]</code></pre><p> 查</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l[<span class="number">1</span>]    <span class="comment"># 索引</span></span><br></pre></td></tr></table></figure>
<pre><code>输出：
&apos;new&apos;</code></pre> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l.index(<span class="string">'new'</span>)    <span class="comment"># 返回所在位置</span></span><br></pre></td></tr></table></figure>
<pre><code>输出：
1</code></pre><p> 改</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l[<span class="number">0</span>] = <span class="number">250</span></span><br><span class="line">l</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[250, &apos;new&apos;, 3, 7, -5, 3, 1000]</code></pre><p> 删</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l.remove(<span class="number">3</span>)    <span class="comment"># 删除第一个3</span></span><br><span class="line">l</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[250, &apos;new&apos;, 7, -5, 3, 1000]</code></pre> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l.pop(<span class="number">2</span>)    <span class="comment"># 删除第3个元素，并显示该元素</span></span><br></pre></td></tr></table></figure>
<pre><code>输出：
7</code></pre> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">l</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[250, &apos;new&apos;, -5, 3, 1000]</code></pre> </details>



</li>
</ol>
<ol start="5">
<li><p>请用python编写函数find_string，从文本中搜索并打印内容，要求支持通配符星号和问号。<br> 例子：<br> <code>find_string(&#39;hello\nworld\n&#39;,&#39;wor&#39;)</code><br> [‘wor’]<br> <code>find_string(&#39;hello\nworld\n&#39;,&#39;l*d&#39;)</code><br> [‘ld’]<br> <code>find_string(&#39;hello\nworld\n&#39;,&#39;o.&#39;)</code><br> [‘or’]</p>
 <details>
 <summary><u>答案：</u></summary>

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_string</span><span class="params">(str, pat)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> re</span><br><span class="line">    <span class="keyword">return</span> re.findall(pat, str, re.I)</span><br></pre></td></tr></table></figure>
<p> <code>findall(pattern, string, flags=0)</code> 返回string中所有与pattern相匹配的全部字符串，<code>re.I</code>表示使匹配忽略大小写</p>
 </details>



</li>
</ol>
<ol start="6">
<li><p>现在有 a 到 z 26 个元素， 编写程序打印 a 到 z 中任取 3 个元素的组合（比如：打印 a b c ，d y z等）。</p>
 <details>
 <summary><u>答案：</u></summary>

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">print_letters</span><span class="params">(num=<span class="number">3</span>)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> string</span><br><span class="line">    alphabet = string.ascii_lowercase</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">    ind = np.random.randint(<span class="number">0</span>,<span class="number">26</span>,num)   <span class="comment"># choose randomly integers as index</span></span><br><span class="line">    letters = [alphabet[i] <span class="keyword">for</span> i <span class="keyword">in</span> ind]    <span class="comment"># choose letters from alphabet by index</span></span><br><span class="line">    s = <span class="string">''</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> s.join(letters)</span><br></pre></td></tr></table></figure>
 </details>



</li>
</ol>
<ol start="7">
<li><p>Python是如何进行内存管理的？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 从三个方面来说：</p>
<p> <strong>一、对象的引用计数机制</strong></p>
<p> Python内部使用引用计数，来保持追踪内存中的对象，所有对象都有引用计数。</p>
 <ul>
     <li>引用计数增加的情况：
     <ol>
         <li>一个对象分配一个新名称
         <li>将其放入一个容器中（如列表、元组或字典）
     </ol>
     <li>引用计数减少的情况：
     <ol>
         <li>使用del语句对对象别名显示的销毁
         <li>引用超出作用域或被重新赋值
     </ol>
 </ul>

<p> <code>sys.getrefcount( )</code>函数可以获得对象的当前引用计数<br> 多数情况下，引用计数比你猜测得要大得多。对于不可变数据（如数字和字符串），解释器会在程序的不同部分共享内存，以便节约内存。</p>
<p> <strong>二、垃圾回收</strong></p>
 <ol>
     <li>当一个对象的引用计数归零时，它将被垃圾收集机制处理掉。
     <li>当两个对象a和b相互引用时，del语句可以减少a和b的引用计数，并销毁用于引用底层对象的名称。然而由于每个对象都包含一个对其他对象的应用，因此引用计数不会归零，对象也不会销毁。（从而导致内存泄露）。为解决这一问题，解释器会定期执行一个循环检测器，搜索不可访问对象的循环并删除它们。
 </ol>

<p> <strong>三、内存池机制</strong></p>
<p> Python提供了对内存的垃圾收集机制，但是它将不用的内存放到内存池而不是返回给操作系统。</p>
 <ol>
     <li>Pymalloc机制。为了加速Python的执行效率，Python引入了一个内存池机制，用于管理对小块内存的申请和释放。
     <li>Python中所有小于256个字节的对象都使用pymalloc实现的分配器，而大的对象则使用系统的malloc。
     <li>对于Python对象，如整数，浮点数和List，都有其独立的私有内存池，对象间不共享他们的内存池。也就是说如果你分配又释放了大量的整数，用于缓存这些整数的内存就不能再分配给浮点数。
 </ol>
 </details>



</li>
</ol>
<ol start="8">
<li><p>请写出一段Python代码实现删除一个list里面的重复元素。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <code>set</code>函数：Build an unordered collection of unique elements.</p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">set(a)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
{1, 2, 3, 4, 5}</code></pre> </details>



</li>
</ol>
<ol start="9">
<li><p>编程用sort进行排序，然后从最后一个元素开始判断 a=[1,2,4,2,4,5,7,10,5,5,7,8,9,0,3]。</p>
 <details>
 <summary><u>答案：</u></summary>

 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">10</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">0</span>,<span class="number">3</span>]</span><br><span class="line">a.sort()     <span class="comment"># 排序：a=[0,1,2,2,3,4,4,5,5,5,7,7,8,9,10]</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)<span class="number">-1</span>,<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">if</span> a[i] &lt; a[i<span class="number">-1</span>]:</span><br><span class="line">        print(<span class="string">"Error: position %d greater than position %d"</span> %(i<span class="number">-1</span>,i))</span><br><span class="line">    print(<span class="string">"Sorted list checked."</span>)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
Sorted list checked.</code></pre> <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">last = a[<span class="number">-1</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(a)<span class="number">-2</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">    <span class="keyword">if</span> last == a[i]:</span><br><span class="line">        <span class="keyword">del</span> a[i]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        last = a[i]</span><br><span class="line">print(a)</span><br></pre></td></tr></table></figure>
<pre><code>输出：
[0, 1, 2, 3, 4, 5, 7, 8, 9, 10]</code></pre> </details>



</li>
</ol>
<ol start="10">
<li><p>Python里面如何生成随机数？</p>
<details>
<summary><u>答案：</u></summary>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.randint(<span class="number">0</span>,<span class="number">10</span>)         <span class="comment"># 返回一个 0~9 之间的整数</span></span><br><span class="line">random.randrange(<span class="number">0</span>,<span class="number">10</span>,<span class="number">2</span>)     <span class="comment"># 第三个参数是 step，默认值是 1</span></span><br><span class="line"></span><br><span class="line">random.random()              <span class="comment"># 返回一个 0~1 之间的浮点数</span></span><br><span class="line">random.uniform(<span class="number">0</span>,<span class="number">10</span>)         <span class="comment"># 返回一个 0~9 之间的浮点数</span></span><br></pre></td></tr></table></figure>
</details>



</li>
</ol>
<ol start="11">
<li>以下关于Python数据结构说法正确的是<br>A. Python中list可以动态的更新，但是不容许嵌套<br>B. Python中tuple可以动态更新，但是不容许嵌套<br>C. Python中dict保存键值对，并且键值对是有序的<br>D. Python中list的元素可以是tuple<details>
<summary><u>答案：</u></summary>
D.
</details>


</li>
</ol>
]]></content>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>Machine Learning CheatSheet</title>
    <url>/2020/03/26/ml-cheatsheet/</url>
    <content><![CDATA[<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><ol>
<li><p>KNN中的K如何选取的？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> #1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；</p>
<p> #2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</p>
<p> #3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。</p>
<p> 在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。</p>
 </details>








</li>
</ol>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><ol>
<li><p>什么是最小二乘法？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 一般我们所说的最小二乘法，指的是普通最小二乘法（Ordinary Least Squares），只适用于线性回归问题。通过求解目标函数对参数的偏导函数，使得偏导函数为零，求出使得残差平方和最小化的最优解 $$\hat{\beta}^{\text{OLS}}=(X^TX)^{-1}X^Ty$$ 只有符合线性回归假设的问题才能用OLS求解。</p>
 </details>



</li>
</ol>
<ol start="2">
<li>以下关于最小二乘法正确的是<br> A. 最小二乘估计是线性有偏估计中方差最小的<br> B. 最小二乘估计是线性无偏估计中方差最小的<br> C. 最小二乘估计是线性有偏估计中方差最大的<br> D. 最小二乘估计是线性无偏估计中方差最大的 <details>
 <summary><u>答案：</u></summary>
 B.
 </details>   




</li>
</ol>
<h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><ol>
<li><p>逻辑斯特回归为什么要对特征进行离散化？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：</p>
 <ol>
 <li>离散特征的增加和减少都很容易，易于模型的快速迭代；
 <li>稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；
 <li>离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
 <li>逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
 <li>离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
 <li>特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
 <li>特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。
 </ol>

<p> 李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。</p>
 </details>



</li>
</ol>
<ol start="2">
<li><p>简单介绍下logistics回归？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> $n$ 个样本：$\left( \mathbf{x}_1,y_1 \right) ,\cdots ,\left( \mathbf{x}_n,y_n \right)$<br> x 有p个特征：$\mathbf{x}_i=\left( x_{i}^{\left( 1 \right)},\cdots ,x_{i}^{\left( p \right)} \right) ,\ i=1,\cdots ,n$<br> y 只有两种取值0和1：$y_i=\left\{ 0,1 \right\}$ </p>
<p> Sigmoid函数：$$\sigma(z)=\frac{1}{1+e^{-z}}=\frac{e^z}{1+e^z}$$</p>
<p> 逻辑回归，就是在线性回归的基础上，用sigmoid函数做非线性变换，使得预测值在0~1之间，作为分类 $y=1$ 的概率值。<br> $$\begin{aligned}\hat{y}_i&amp;=\mathbb{P}\left( y_i=1\ |\ \mathbf{x}_i \right) =\frac{1}{1+e^{-\mathbf{x}_i\boldsymbol{\beta }}}\\1-\hat{y}_i&amp;=\mathbb{P}\left( y_i=0\ |\ \mathbf{x}_i \right) =\frac{e^{-\mathbf{x}_i\boldsymbol{\beta }}}{1+e^{-\mathbf{x}_i\boldsymbol{\beta }}}\end{aligned}$$</p>
<p> Loss function $$L\left( \hat{y}_i,y_i \right) =-\left( y_i\log \hat{y}_i+\left( 1-y_i \right) \log \left( 1-\hat{y}_i \right) \right)$$</p>
<p> Cost function $$\begin{aligned}J\left( \boldsymbol{\beta } \right) &amp;=\frac{1}{n}\sum_{i=1}^n{L\left( \hat{y}_i,y_i \right)}\\\<br> &amp;=-\frac{1}{n}\sum_{i=1}^n{\left( y_i\log \hat{y}_i+\left( 1-y_i \right) \log \left( 1-\hat{y}_i \right) \right)}\\\<br> &amp;=-\frac{1}{n}\sum_{i=1}^n{\left( y_i\left( \mathbf{x}_i\boldsymbol{\beta } \right) -\log \left( 1+e^{\mathbf{x}_i\boldsymbol{\beta }} \right) \right)}\end{aligned}$$</p>
<p> 梯度下降法 $$\boldsymbol{\beta} ^{\left[ j+1 \right]}=\boldsymbol{\beta} ^{\left[ j \right]}-\alpha \nabla _{\boldsymbol{\beta }}J\left( \boldsymbol{\beta } \right)$$ $$\nabla _{\boldsymbol{\beta }}J\left( \boldsymbol{\beta } \right) =\frac{1}{n}\boldsymbol{X}^T\left( \sigma \left( \boldsymbol{X\beta } \right) -\mathbf{y} \right)$$</p>
 </details>



</li>
</ol>
<ol start="3">
<li><p>关于逻辑回归和 SVM 不正确的是<br> A. 逻辑回归目标函数是最小化后验概率<br> B. 逻辑回归可以用于预测事件发生概率的大小<br> C. SVM目标是结构风险最小化<br> D. SVM可以有效避免模型过拟合</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。<br> 逻辑回归目标是似然函数最大化，并不是后验概率最大化，加上正则项的逻辑回归才可以看作是后验概率最大化。<br> SVM的软间隔相当于是在经验风险最小化的基础上添加了正则化项，所以SVM应该是结构风险最小化。</p>
<ul>
<li>经验风险最小化（empirical risk minimization, ERM）$$\min_{f\in F}\frac{1}{n}\sum^n_{i=1}L(y_i,f(x_i))$$</li>
<li>结构风险最小化（structural minimization, SRM）$$\min_{f\in F}\left[\frac{1}{n}\sum^n_{i=1}L(y_i,f(x_i))+\lambda J(f)\right]$$</details>






</li>
</ul>
</li>
</ol>
<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><ol>
<li>下列不是SVM核函数的是：<br> A. 多项式核函数<br> B. logistic核函数<br> C. 径向基核函数<br> D. Sigmoid核函数 <details>
 <summary><u>答案：</u></summary>
 选B。
 </details>



</li>
</ol>
<ol start="2">
<li><p>介绍一下核函数。</p>
 <details>
 <summary><u>答案：</u></summary>
 核技巧是构造一个核函数，可以表示输入低维空间的向量，经过某个变换后在高维空间里的向量内积。满足了Mercer条件的函数，都可以作为核函数，表示成向量内积。以下是几个常用的核函数：

<ul>
<li><p><b>线性核</b>：$K(x_i,x_j)=x_i^Tx_j$<br>$\hspace{1em}$这是SVM的原始形式那个内积，没做高维映射。</p>
</li>
<li><p><b>多项式核</b>：$K(x_i,x_j)=(\gamma x_i^Tx_j+b)^d$<br>$\hspace{1em}$类似于多项式回归，d是多项式的阶数。如果d较大计算量增大，所以阶数d不能取太高。</p>
</li>
<li><p><b>RBF核</b>，又叫径向基核或者高斯核：$K(x_i,x_j)=\exp(-\gamma\lVert x_i-x_j\rVert^2)$<br>$\hspace{1em}$应用最多的核函数，好处是只有一个参数，将样本映射到无穷维，对大样本或者小样本性能都较好。$\gamma$越大，高斯分布越窄；$\gamma$越小，高斯分布越宽。$\gamma$相当于调整模型的复杂度，$\gamma$越小模型复杂度越低；$\gamma$越高，模型复杂度越大。</p>
</li>
<li><p><b>sigmoid核</b>：$K(x_i,x_j)=\tanh(\gamma x_i^Tx_j+b)$<br>$\hspace{1em}$相当于神经网络。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>关于支持向量机SVM，下列说法错误的是<br> A. L2正则项，作用是最大化分类间隔，使得分类器拥有更强的泛化能力<br> B. Hinge 损失函数，作用是最小化经验分类错误<br> C. 分类间隔为 $1/\lVert w\rVert$，$\lVert w\rVert$ 代表向量的模<br> D. 当参数C越小时，分类间隔越大，分类错误越多，趋于欠学习</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。间隔是 $2/\lVert w\rVert$<br> A. 软间隔，有更强的泛化能力。<br> D. 软间隔加的正则项惩罚系数C（下图的 $\lambda$），即对误差的宽容度。C越高，说明越不能容忍出现误差，容易过拟合。C越小，容易欠拟合。</p>
 <img src="https://pic.downk.cc/item/5e8e87b8504f4bcb04e0e87d.png">
 </details>






</li>
</ol>
<h2 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h2><ol>
<li><p>经常在网上搜索东西的朋友知道，当你不小心输入一个不存在的单词时，搜索引擎会提示你是不是要输入某一个正确的单词，这叫做拼写检查。根据谷歌一员工写的文章 (<a href="http://norvig.com/spell-correct.html" target="_blank" rel="noopener">http://norvig.com/spell-correct.html</a>) 显示，Google的拼写检查基于贝叶斯方法。请说说你的理解，具体Google是怎么利用贝叶斯方法，实现“拼写检查”的功能。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 用户输入一个单词时，可能拼写正确，也可能拼写错误。如果把拼写正确的情况记做 $c$（correct），拼写错误的情况记做 $w$（wrong），那么“拼写检查”要做的事情就是：在发生 $w$ 的情况下，试图推断出 $c$。换言之：已知 $w$，然后在若干个备选方案中，找出可能性最大的那个 $c$，也就是求 $P(c|w)$ 的最大值。<br> 根据贝叶斯定理，有：$$P(c|w)=\frac{P(w|c)P(c)}{P(w)}$$ 由于对于所有备选的 $c$ 来说，对应的都是同一个 $w$，所以它们的 $P(w)$ 是相同的，因此我们只要最大化 $P(w|c)P(c)$即可。其中：$P(c)$ 表示某个正确的词的出现“概率”，它可以用“频率”代替。如果我们有一个足够大的文本库，那么这个文本库中每个单词的出现频率，就相当于它的发生概率。某个词的出现频率越高，$P(c)$ 就越大。比如在你输入一个错误的词“Julw”时，系统更倾向于去猜测你可能想输入的词是“July”，而不是“Jult”，因为“July”更常见。<br> $P(w|c)$ 表示在试图拼写 $c$ 的情况下，出现拼写错误 $w$ 的概率。为了简化问题，假定两个单词在字形上越接近，就有越可能拼错，$P(w|c)$ 就越大。举例来说，相差一个字母的拼法，就比相差两个字母的拼法，发生概率更高。你想拼写单词July，那么错误拼成Julw（相差一个字母）的可能性，就比拼成Jullw高（相差两个字母）。值得一提的是，一般把这种问题称为“编辑距离”，参见<a href="http://blog.csdn.net/v_july_v/article/details/8701148#t4" target="_blank" rel="noopener">http://blog.csdn.net/v_july_v/article/details/8701148#t4</a><br> 所以，我们比较所有拼写相近的词在文本库中的出现频率，再从中挑出出现频率最高的一个，即是用户最想输入的那个词。</p>
 </details>



</li>
</ol>
<ol start="2">
<li>朴素贝叶斯“朴素”在哪里？ <details>
 <summary><u>答案：</u></summary>
 Naive Bayes的假设：所有特征都是相互独立的。而这个假设在现实中是不存在的，是很naive的假设。
 </details>



</li>
</ol>
<ol start="3">
<li><p>简单说说贝叶斯定理。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 贝叶斯公式 $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$</p>
<ul>
<li>先验概率 $P(A)$：预估事件A发生的概率</li>
<li>后验概率 $P(A|B)$：收集到在A条件下相关事件B发生的概率，借助这个实际发生的概率来“纠正”A的概率</li>
<li>标准似然度 $\displaystyle\frac{P(B|A)}{P(B)}$：<ul>
<li>标准似然度大于1，意味着“先验概率”被增强，事件A发生的可能性变大；</li>
<li>标准似然度等于1，意味着事件B无助于判断事件A的可能性；</li>
<li>标准似然度小于1，意味着“先验概率”被削弱，事件A的可能性变小。</details>



</li>
</ul>
</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p>Naive Bayes是一种特殊的Bayes分类器，特征变量是X，类别标签是C，它的一个假定是:<br> A. 各类别的先验概率P(C)是相等的<br> B. 以0为均值，sqr(2)/2为标准差的正态分布<br> C. 特征变量X的各个维度是类别条件独立随机变量<br> D. P(X|C)是高斯分布</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>假定某同学使用Naive Bayesian（NB）分类模型时，不小心将训练数据的两个维度搞重复了，那么关于NB的说法中正确的是：<br> A. 这个被重复的特征在模型中的决定作用会被加强<br> B. 模型效果相比无重复特征的情况下精确度会降低<br> C. 如果所有特征都被重复一遍，得到的模型预测结果相对于不重复的情况下的模型预测结果一样。<br> D. 当两列特征高度相关时，无法用两列特征相同时所得到的结论来分析问题<br> E. NB可以用来做最小二乘回归<br> F. 以上说法都不正确</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选BD。NB的核心在于它假设向量的所有特征之间是独立的。重复特征破坏了这个假设，使得模型变差。</p>
 </details>



</li>
</ol>
<h2 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h2><h2 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习 (*)"></a>集成学习 (*)</h2><ol>
<li><p>随机森林、Boosting、AdaBoost、GBDT和XGBoost的区别。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <a href="https://xijunlee.github.io/2017/06/03/集成学习总结/" target="_blank" rel="noopener">https://xijunlee.github.io/2017/06/03/集成学习总结/</a></p>
 </details>



</li>
</ol>
<ol start="2">
<li>为什么XGBoost要用泰勒展开，优势在哪里？ <details>
 <summary><u>答案：</u></summary>
 目标函数难求解，运用泰勒公式展开原目标函数从而近似求解，只需要求一阶和二阶导，大大简化计算，更快速更准确。
 </details>



</li>
</ol>
<ol start="3">
<li>XGBoost如何寻找最优特征？是又放回还是无放回的呢？ <details>
 <summary><u>答案：</u></summary>
 XGBoost在生成树的时候，类似决策树的生成，不同的是以目标函数（也就是损失函数）作为特征选取标准。XGBoost利用梯度优化模型算法，样本是不放回的，但xgboost支持子采样，也就是每轮计算可以不使用全部样本。
 </details>



</li>
</ol>
<ol start="4">
<li><p>说一下Adaboost，权值更新公式。当弱分类器是Gm时，每个样本的的权重是w1，w2…，请写出最终的决策公式。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> <a href="https://blog.csdn.net/v_july_v/article/details/40718799" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/40718799</a></p>
 </details>



</li>
</ol>
<h2 id="聚类"><a href="#聚类" class="headerlink" title="聚类"></a>聚类</h2><ol>
<li><p>影响聚类算法效果的主要原因中，不正确的是：<br> A. 特征选取<br> B. 模式相似性测度<br> C. 分类准则<br> D. 已知类别的样本质量</p>
 <details>
 <summary><u>答案：</u></summary>
 选D。聚类是对无类别的数据进行聚类，不使用已经标记好的数据。
 </details>
</li>
<li><p> 2.1. 为了初步观察不同性别说话人的语音特征的区别，甲尝试用聚类的方法对所获得的语音特征进行分析。在对语音数据做了简单的处理后，甲得到了部分对话中一共M句话的特征x1, x2, …, xM，其中每个xi均为N维实数向量。然后甲通过K-Means算法将这M句话聚类为两类。试写出针对该问题的K-Means聚类代码或伪代码</p>
 <details>
 <summary><u>答案：</u></summary>
 #1. Input: $X=(\mathbf{x}_1,\cdots,\mathbf{x}_N)$, $K$<br>
 #2. Initialize $K$ centroids: $c_1^{(0)},\cdots,c_K^{(0)}$<br>
 #3. for $i=1,\cdots,N$：<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $d_{ij}=\text{distance}(\mathbf{x}_i, c_j)$<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $k=\left\{j: \min\{d_{ij}\}\right\}$<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $\mathbf{x}_i\in$ cluster $k$<br>
 #4. for $j=1,\cdots,K$：<br>
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;- $c_j^{new}=\text{mean}(\mathbf{x}_i, \text{for } \mathbf{x}_i\in \text{cluster }j)$<br>
 #5. repeat #3 #4, till $|{c^{old}-c^{new}}|<\varepsilon$, END。
 </details>

<p> 2.2. 聚类完成后，甲通过训练好的聚类模型对另外挑选的1000句话（包括800句男声说话与200句女声说话）的说话人性别进行了预测，预测结果如下：800句男声说话中预测正确了600句，200句女声说话中预测正确了100句。请从不同角度评价该分类结果的表现，并提出改进的建议。</p>
 <details>
 <summary><u>答案：</u></summary>
 1). 准确率：$\displaystyle\frac{600+100}{1000}=0.7$。

<p> 2). 两类样本的比例是8/2，分类不均衡。以男声作为正类 (1)，女声作为负类 (0)，混淆矩阵如下：</p>
 <img src="https://pic.downk.cc/item/5e82e9e4504f4bcb04fb800a.png"  width=250>
 召回率：$R=\displaystyle\frac{600}{600+200}=\displaystyle\frac{3}{4}$

<p> 精确率：$P=\displaystyle\frac{600}{600+100}=\displaystyle\frac{6}{7}$</p>
<p> 召回率R和精确率P都是越大越好。但往往当R较大，P会较小，反之亦然。而F1-score可以平衡两者$\displaystyle\frac{1}{F_1}=\displaystyle\frac{1}{2}(\displaystyle\frac{1}{P}+\displaystyle\frac{1}{R})$<br> $$F_1=\displaystyle\frac{2PR}{P+R}=0.8$$</p>
<p> 3). 真正例率：$TPR=\displaystyle\frac{600}{600+200}=\displaystyle\frac{3}{4}$，男声中，预测正确的概率是75%。<br> 假正例率：$FPR=\displaystyle\frac{100}{100+100}=\displaystyle\frac{1}{2}$，女声中，预测错误的概率是50%。<br> 对于女声样本，这个分类器的效果很差，把女声判断为男声和女声的概率是相等的。</p>
<p> 改进：由于样本分类不均衡使得模型效果不佳，可以在本身算法上做集成学习，若干个弱分类器集成可以增强分类器效果。</p>
 </details>

<p> 2.3. 甲认为上述聚类结果并不能作为很好的识别说话人性别的参考标准，所以决定采用神经网络构造说话人性别分类模型。他设计得网络结构如下：第一层Input Layer为输入层，输入x为N1维实向量；第二、第三层为隐藏层Hidden Layer，其中第二层有N2个节点，第三层有2个节点，每个节点均为一个实数；最后一层为输出层Output Layer，输出2维向量y=(Prob0, Prob1)，Prob0和Prob1分别代表对应输入x的语句说话人为女 (0) 和男 (1) 的概率。激活函数使用sigmoid函数，代价函数使用交叉熵函数。<br> 现在甲有M个训练样本(xi, Yi), i=1,2,…,M。其中xi为N1维向量，Yi为对应句子的实际说话人性别，若Yi=(0,1)表示说话人为男，若Yi=(1,0)表示说话人为女。试根据甲设计得网络结构，构造函数返回对这M个样本的单次前向传播后的平均代价函数值。</p>
 <details>
 <summary><u>答案：</u></summary>
 <img src="https://pic.downk.cc/item/5e82fc7d504f4bcb0409e049.png"  width=350>
 Input：$\mathbf{x}=(x_1,\cdots,x_{N1})$，Output：$y$是男声的概率Prob1，Prob0=1-Prob1
 $$\begin{aligned}
 &z^{[1]}=W^{[1]}\mathbf{x}+b^{[1]}, &&a^{[1]}=\sigma(z^{[1]})\\\\
 &z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}, &&a^{[2]}=\sigma(z^{[2]})\\\\
 &z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}, &&a^{[3]}=\sigma(z^{[3]})
 \end{aligned}
 $$
 单个样本$\mathbf{x}_i$的代价函数
 $$L(a^{[3](i)},y_i)=-(y_i\log a^{[3](i)}+(1-y_i)\log(1-a^{[3](i)}))$$
 M个样本的平均代价函数
 $$J=\frac{1}{M}\sum^M_{i=1}L(a^{[3](i)},y_i)$$
 </details>



</li>
</ol>
<ol start="3">
<li><p>K-means的复杂度？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> K-means算法：</p>
<blockquote>
<p>选择K个点作为初始质心<br> repeat：<br> $\hspace{2em}$将每个点指派到最近的质心，形成K个簇<br> $\hspace{2em}$重新计算每个簇的质心<br> until 簇不发生变化或达到最大迭代次数</p>
</blockquote>
<p> 时间复杂度：$O(tKmn)$，其中 t 为迭代次数，K 为簇的数目，m 为样本数，n 为维数<br> 空间复杂度：$O\left((m+K)n\right)$，其中 K 为簇的数目，m 为样本数，n 为维数</p>
<p> 关于时间复杂度和空间复杂度：<a href="https://zhuanlan.zhihu.com/p/50479555" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/50479555</a></p>
 </details>




</li>
</ol>
<h2 id="关联规则"><a href="#关联规则" class="headerlink" title="关联规则"></a>关联规则</h2><ol>
<li><p>通常可以通过关联规则挖掘来发现啤酒和尿布的关系， 那么如果对于一条规则A →B, 如果同时购买A和B的顾客比例是4/7, 而购买A的顾客当中也购买了B的顾客比例是1/2, 而购买B的顾客当中也购买了A的顾客比例是1/3,则以下对于规则A →B的支持度(support)和置信度(confidence)分别是多少？<br> A. 4/7，1/3<br> B. 3/7，1/2<br> C. 4/7，1/2<br> D. 4/7，2/3</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选C。A-&gt;B<br> support = P(AB)<br> confidence = P(B|A)<br> lift = P(B|A)/P(B)</p>
 </details>



</li>
</ol>
<h2 id="监督，回归，分类"><a href="#监督，回归，分类" class="headerlink" title="监督，回归，分类"></a>监督，回归，分类</h2><ol>
<li>下列算法中，均为监督学习算法的为：<br> A. CRF，SVM，Logistic Regression<br> B. K-Means，KNN，决策树<br> C. 决策树，KNN，朴素贝叶斯<br> D. K-Means，SVM，朴素贝叶斯 <details>
 <summary><u>答案：</u></summary>
 选AC。聚类是对无类别的数据进行聚类，不使用已经标记好的数据。
 </details>



</li>
</ol>
<ol start="2">
<li>在分类问题中，我们经常会遇到正负样本数据量不等的情况，比如正样本为10w条数据，负样本只有1w条数据，以下最不合适的处理方法是：<br> A. 将负样本重复10次，生成10w样本量，打乱顺序参与分类<br> B. 直接进行分类，可以最大限度利用数据<br> C. 从10w正样本中随机抽取1w参与分类<br> D. 将负样本每个权重设置为10，正样本权重为1，参与训练过程 <details>
 <summary><u>答案：</u></summary>
 选B。A. 重采样，通过多次复制小样本，改变数据分布消除不平衡，可能导致过拟合；C. 欠采样，通过随机抽样减少多样本规模，提高少数类的分类性能，可能丢失多数类的重要信息；D. 权值调整。
 </details>



</li>
</ol>
<ol start="3">
<li><p>在K-Means或KNN，我们常用欧氏距离来计算最近的邻居之间的距离，有时也用曼哈顿距离，请对比下这两种距离的差别。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 两个n维样本$\mathbf{x}=(x_1,\cdots,x_n), \mathbf{y}=(y_1,\cdots,y_n)$</p>
<p> 欧氏距离 $$d=\sqrt{\sum^n_{i=1}(x_i-y_i)^2}$$<br> 曼哈顿距离 $$d=\sum^n_{i=1}|x_i-y_i|$$<br> 在2维平面，欧氏距离就是两点间的直线距离，曼哈顿距离就是在x轴和y轴的距离之和。</p>
<p> 这两种距离都是明可夫斯基距离的特殊情况 $$d=\left(\sum^n_{i=1}|x_i-y_i|^p\right)^{1/p}$$<br> p=1时是曼哈顿距离，p=2时是欧氏距离。</p>
 </details>



</li>
</ol>
<ol start="4">
<li><p>线性分类器与非线性分类器的区别以及优劣。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。<br> 常见的线性分类器有：LR，贝叶斯分类，单层感知机，原始SVM<br> 常见的非线性分类器：决策树，RF，GBDT，多层感知机，SVM+非线性核</p>
<p> 线性分类器速度快、编程方便，但是可能拟合效果不会很好<br> 非线性分类器编程复杂，但是效果拟合能力强</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>以下哪些方法不可以直接来对文本分类？<br> A. Kmeans<br> B. 决策树<br> C. 支持向量机<br> D. KNN</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。A是聚类，其他都是分类算法。</p>
 </details>



</li>
</ol>
<ol start="6">
<li><p>在统计模式分类问题中，当先验概率未知时，可以使用 (*)<br> A. 最小最大损失准则<br> B. 最小误判概率准则<br> C. 最小损失准则<br> D. N-P判决</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选AD。</p>
<p> 先验概率未知，也就是说不能用生成模型，只能用判别模型。</p>
<p> A. <strong>最小最大损失准则</strong><br> 考虑p(wi)变化的条件下，是风险最小。</p>
<p> B. <strong>最小误判概率准则</strong><br> 判断p(w1|x)和p(w2|x)哪个大，x为特征向量，w1和w2为两分类，根据贝叶斯公式，需要用到先验知识。</p>
<p> C. <strong>最小损失准则</strong><br> 在B的基础之上，还要求出p(w1|x)和p(w2|x)的期望损失，因为B需要先验概率，所以C也需要先验概率。</p>
<p> D. <strong>N-P判决</strong><br> 限定一类错误率条件下使另一类错误率为最小的两类别决策，即在一类错误率固定的条件下，求另一类错误率的极小值的问题，直接计算p(x|w1)和p(x|w2)的比值，不需要用到贝叶斯公式。<br> 在贝叶斯决策中，对于先验概率p(y)，分为已知和未知两种情况。 </p>
<ol>
<li>p(y)已知，直接使用贝叶斯公式求后验概率即可； </li>
<li>p(y)未知，可以使用聂曼-皮尔逊决策(N-P决策)来计算决策面。</details>



</li>
</ol>
</li>
</ol>
<ol start="7">
<li>以下不属于非监督学习的为<br> A. 关联规则<br> B. Kmeans<br> C. Word2vec<br> D. KNN <details>
 <summary><u>答案：</u></summary>
 选D。
 </details>





</li>
</ol>
<h2 id="过拟合，正则化"><a href="#过拟合，正则化" class="headerlink" title="过拟合，正则化"></a>过拟合，正则化</h2><ol>
<li><p>Overfitting怎么解决？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><b>Regularization</b><br>  l1正则化：（通过假设参数的先验分布为拉普拉斯分布，由最大后验概率估计导出。）<br>  $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}|\beta_j|\leq C$$<br>  l2正则化：（通过假设参数的先验分布为高斯分布，由最大后验概率估计导出。）<br>  $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}\beta_j^2\leq C$$ 假设只有两个参数$\beta_0, \beta_1$，可以在二维平面表示成这样  <img src="https://pic.downk.cc/item/5e845dd8504f4bcb040d5354.png" width=350>
  正则化对参数的限制：l1正则化是让个别参数近似于零，从而消除这些变量达到降维的效果，同时包含了特征选择和正则化；l2正则化则是控制每个参数的权重来调整参数，使得所有参数趋向零但不为零。</li>
<li><b>Dropout</b><br>  随机drop掉某些神经元，防止过拟合。<br>  Training：让神经元以$p$的概率被激活（也就是$1-p$的概率被设置为0），这样每个w都可以随机参与训练，模型泛化能力更强。  <img src="https://pic.downk.cc/item/5e8453d0504f4bcb0406611e.png" width=250>
  Test：在测试集不会dropout，要用所有的神经元，每个神经元需要乘上p作为权重。</li>
<li><b>Batch Normalization</b><br>  这个方法给每层的输出都做一次归一化（纵向规范化），使得下一层的输入接近高斯分布。因为在深度学习里，无论一开始的input有没有做归一化处理，随着层数的加深，隐藏层接收到的input value（上一层的output）也会产生大幅度的波动，BN就是scaling这些值。<br>  $$\begin{aligned}\mu_B&amp;=\frac{1}{m}\sum^m_{i=1}x_i\\\<br>  \sigma_B^2&amp;=\frac{1}{m}\sum^m_{i=1}(x_i-\mu_B)^2\\\<br>  \hat{x}_i&amp;=\frac{x_i-\mu_B}{\sqrt{\sigma_B^2+\varepsilon}}\\\<br>  y_i&amp;=\gamma\hat{x}_i+\beta\end{aligned}$$ $\varepsilon$是为了让分母不为零，避免出现计算错误。$\gamma, \beta$两个参数对最后的结果进行缩放和平移，需要被学习。</li>
<li><b>交叉验证</b></li>
<li><b>Early Stopping</b><br>  结合cross-validation使用，当在测试集上连续的几次迭代中都出现损失上升，则训练停止。</li>
<li><b>数据集扩增</b></li>
<li><b>特征选择/特征降维</b></details>



</li>
</ul>
</li>
</ol>
<ol start="2">
<li><p>L1和L2的区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> L1正则化：（通过假设参数的先验分布为拉普拉斯分布，由最大后验概率估计导出。）<br> $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}|\beta_j|\leq C$$<br> L2正则化：（通过假设参数的先验分布为高斯分布，由最大后验概率估计导出。）<br> $$\min_\beta L(\beta) \hspace{3em}\text{s.t.} \sum^p_{j=1}\beta_j^2\leq C$$ 正则化对参数的限制：L1正则化是让个别参数近似于零，从而消除这些变量达到降维的效果，同时包含了特征选择和正则化；L2正则化则是控制每个参数的权重来调整参数，使得所有参数趋向零但不为零。    </p>
<p> 几何含义：<br> 假设只有两个参数$\beta_0, \beta_1$，可以在二维平面表示成这样</p>
 <img src="https://pic.downk.cc/item/5e845dd8504f4bcb040d5354.png" width=350>
 红色的等高线代表损失函数，等高线中心的红点代表损失函数取到最小值。

<p> 左图（L1正则化）：灰底方块表示惩罚项（绝对值之和）$$|\beta_0|+|\beta_1|\leq C$$<br> 右图（L2正则化）：灰底圆形表示惩罚项（平方和）$$\beta_0^2+\beta_1^2\leq C$$<br> 黑点（圆点）表示惩罚项取到最小值（$|\beta_0|+|\beta_1|=0$ 或 $\beta_0^2+\beta_1^2=0$），也就是零。方块（或圆形）与等高线相切的点，也就是蓝点，同时使得损失函数与惩罚项之和最小。 </p>
 </details>



</li>
</ol>
<ol start="3">
<li><p>L1和L2正则先验分别服从什么分布？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> L1是Laplace先验分布，L2是高斯先验分布。<br> <a href="https://blog.csdn.net/m0_38045485/article/details/82147817" target="_blank" rel="noopener">https://blog.csdn.net/m0_38045485/article/details/82147817</a>  </p>
 </details>






</li>
</ol>
<h2 id="梯度下降法，EM算法，牛顿法，拟牛顿法"><a href="#梯度下降法，EM算法，牛顿法，拟牛顿法" class="headerlink" title="梯度下降法，EM算法，牛顿法，拟牛顿法"></a>梯度下降法，EM算法，牛顿法，拟牛顿法</h2><ol>
<li><p>SGD中 S (stochastic) 代表什么?</p>
 <details>
 <summary><u>答案：</u></summary>

<p> BGD (Batch Gradient Descent) 每次迭代都用全部样本来计算。而SGD (Stochastic Gradient Descent) 每次迭代只随机选取一个样本来计算。<br> BGD的优点：全局最优解；易于并行实现；缺点：当样本数目很多时，训练过程会很慢。<br> SGD的优点：训练速度快；缺点：准确度下降，并不是全局最优；不易于并行实现。</p>
 </details>



</li>
</ol>
<ol start="2">
<li><p>请简要说说EM算法。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 有时候因为样本的产生和隐含变量有关（隐含变量不能被观察），求模型的参数时一般采用极大似然估计，由于有隐含变量，所以对似然函数参数求导是求不出来的，这时可以采用EM算法来求模型的参数，EM算法一般分为2步：</p>
<ul>
<li><p>E步：选取一组参数，求出在该参数下隐含变量的条件概率值；</p>
</li>
<li><p>M步：结合E步求出的隐含变量条件概率，求出似然函数下界函数（本质上是某个期望函数）的最大值。<br>重复上面2步直至收敛。</p>
<p>(E-step) For i: $$Q_i(z^{(i)}):=p(z^{(i)}|x^{(i)};\theta)$$ (M-step) $$\theta:=\arg\max_\theta\sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$$</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>说说梯度下降法。</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法的步骤，是让参数值往梯度下降最快的方向迭代，从而快速达到目标函数的最小值。<br> 比如我们要最小化 cost function $J(\theta)$，梯度下降法的步骤是先选取参数的初始值，还要设定一个超参数步长$\alpha$，每次迭代用当前的参数减去梯度乘以$\alpha$值，$\alpha$的大小决定往梯度下降最快的方向跨多大一步，直到收敛。<br> #1. 选定$\alpha$，初始化参数值$\theta^{(0)}$<br> #2. 迭代参数值 $\theta^{(t+1)}:=\theta^{(t)}-\alpha\nabla_\theta J$<br> 直到 $|\theta^{(t+1)}-\theta^{(t)}|&lt;\varepsilon$</p>
<p> 缺点：</p>
<ul>
<li>对于步长的选择需要慎重，$\alpha$过大，参数值动荡很大，会错过最小值；如果$\alpha$过小，计算的时间会变长。</li>
<li>当样本量很大时，梯度的计算非常耗时。</details>



</li>
</ul>
</li>
</ol>
<ol start="4">
<li><p>梯度下降法找到的一定是下降最快的方向么？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法并不是下降最快的方向，它只是目标函数在当前的点的切平面（二维参数）上下降最快的方向。<br> 牛顿方向（Hessian矩阵）才一般被认为是下降最快的方向，可以达到Superlinear的收敛速度。梯度下降类的算法的收敛速度一般是Linear甚至Sublinear的（在某些带复杂约束的问题）。<br> 梯度下降法比牛顿法更常用，是因为数据较大时要计算Hessian矩阵非常耗时，而梯度下降法的收敛速度已经足够了。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>牛顿法和梯度下降法有什么不同？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 梯度下降法迭代公式： $\theta^{(t+1)}:=\theta^{(t)}-\alpha\nabla_\theta J$<br> 牛顿法迭代公式： $\theta^{(t+1)}:=\theta^{(t)}-\alpha H^{-1}\nabla_\theta J$</p>
<p> 梯度下降法和牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法是用二阶的Hessian矩阵的逆矩阵求解。相对而言，使用牛顿法收敛更快（迭代更少次数）。但是每次迭代的时间比梯度下降法长，因为要计算Hessian矩阵的逆。梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。</p>
 </details>



</li>
</ol>
<ol start="6">
<li><p>什么是拟牛顿法？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。<br> <a href="https://zhuanlan.zhihu.com/p/37524275" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/37524275</a></p>
 </details>



</li>
</ol>
<ol start="7">
<li><p>函数 $f(\theta_0,\theta_1)$ 输出一个数。$f$ 未知，且平滑。假设我们用梯度下降法求解 $\min f(\theta_0,\theta_1)$。下面哪些描述是对的：<br> A. 如果开始的几次迭代使得函数值不减反增，可能是因为步长 $\alpha$ 设置得太大<br> B. 如果 $\theta_0,\theta_1$ 初始值就在函数的全局最小，那迭代不会改变它们的值<br> C. 把 $\alpha$ 设得很小没什么坏处，只会加快收敛速度<br> D. 无论 $\theta_0,\theta_1$ 初始值是多少，只要 $\alpha$ 足够小，梯度下降法就能收敛到同一个结果</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选AB。<br> C. $\alpha$ 设得很小会使得收敛速度过慢。<br> D. 初始值选取不同，可能会收敛到不同的局部最小值。</p>
 </details>



</li>
</ol>
<ol start="8">
<li><p>假设对于线性回归问题 $h_\theta(x)=\theta_0+\theta_1x$，训练某个训练集能找到使得 $J(\theta_0,\theta_1)=0$ 的 $\theta_0,\theta_1$。下面描述正确的是：<br> A. 可以用一条直线完美拟合训练集<br> B. A成立的条件，必须有 $y^{(i)}=0, \forall i$<br> C. 梯度下降法可能会到达一个局部最小值，到不了全局最小值<br> D. C成立的条件，必须有 $\theta_0=0,\theta_1=0$ 使得 $h_\theta(x)=0$</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 选A。<br> A. 对于完美无噪音的训练集可能存在。<br> C. $J$ 是凸函数，只有一个最小值，求到的就是全局最优解。</p>
 </details>



</li>
</ol>
<ol start="9">
<li><p>说说共轭梯度法？（*）</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 共轭梯度法是介于梯度下降法（最速下降法）与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hessian矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有逐步收敛性，稳定性高，而且不需要任何外来参数。<br> <a href="https://blog.csdn.net/qq547276542/article/details/78186050" target="_blank" rel="noopener">https://blog.csdn.net/qq547276542/article/details/78186050</a></p>
 </details>








</li>
</ol>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li><p>谈谈判别式模型和生成式模型？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><p>判别式模型：由数据直接学习决策函数 $Y = f(X)$，或者由条件分布概率 $P(Y|X)$ 作为预测模型，即判别模型。比如KNN、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、LR、boosting、CRF。</p>
</li>
<li><p>生成式模型：由数据学习联合概率密度分布函数 $P(X,Y)$，然后求出条件概率分布 $P(Y|X)$ 作为预测的模型，即生成模型。比如NB、HMM、高斯混合模型、LDA主题模型、限制玻尔兹曼机。</p>
<p>由生成模型可以得到判别模型，但由判别模型得不到生成模型。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="2">
<li><p>机器学习中，为何要对数据做归一化？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> #1. 归一化能提高梯度下降法求解<br> #2. 归一化有可能提高精度，特别对于要计算距离的KNN、K-Means这种算法</p>
<ul>
<li><p>（归一化）最值归一化：把所有数据映射到 0-1 之间（适用于有明显边界的情况）$$x_{scale}=\frac{x-\min}{\max-\min}$$ <code>sklearn.preprocessing.MinMaxScaler</code></p>
</li>
<li><p>（标准化）均值方差归一化：把所有数据归一到均值为0、方差为1的分布中（没有明显边界；有可能存在极端数据值）$$x_{scale}=\frac{x-\mu}{\sigma}$$ <code>sklearn.preprocessing.StandardScaler</code></p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="3">
<li><p>哪些机器学习算法不需要做归一化处理？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、随机森林。</p>
 </details>



</li>
</ol>
<ol start="4">
<li><p>对于树形结构为什么不需要归一化？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。<br> 另外注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点是通过寻找最优分裂点完成的。</p>
 </details>



</li>
</ol>
<ol start="5">
<li><p>协方差和相关性有什么区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li><p>协方差 $$Cov(X,Y)=\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]$$ 用来衡量两个变量是正相关，还是负相关：</p>
<ul>
<li>正相关：X变大，Y也变大，协方差为正；</li>
<li>负相关：X变大，Y变小，协方差为负。</li>
</ul>
<p>$Cov(X,Y)=0 \Longrightarrow$ X，Y不相关<br>X，Y相互独立 $\Longrightarrow Cov(X,Y)=0$</p>
</li>
<li><p>相关系数 $$\rho=\frac{Cov(X,Y)}{\sigma_X\sigma_Y}$$ 可以看做是协方差的标准化，取值范围是 $[-1,1]$。根据定义和协方差一样，正值表示正相关，负值表示负相关。$|\rho|$ 越大（接近1）相关性越强，越小（接近0）则越不相关。</p>
</details>



</li>
</ul>
</li>
</ol>
<ol start="6">
<li><p>熵、联合熵、条件熵、相对熵、互信息的定义。</p>
 <details>
 <summary><u>答案：</u></summary>

<ul>
<li>熵：衡量不确定性（熵越大，不确定性越大，包含信息量越多） $$H(X)=-\sum_xp(x)\log p(x)$$</li>
<li>联合熵：(X,Y)一起出现的不确定性 $$H(X,Y)=-\sum_{x,y}p(x,y)\log p(x,y)$$</li>
<li>条件熵：X确定时，Y的不确定性 $$\begin{aligned}H(Y|X)<br>&amp;=\sum_xp(x)H(Y|X=x)\\\<br>&amp;=-\sum_xp(x)\sum_yp(x|y)\log p(y|x)\\\<br>&amp;=-\sum_x\sum_yp(x,y)\log p(y|x)\\\<br>&amp;=-\sum_{x,y}p(x,y)\log p(y|x)\\\<br>&amp;=-\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)}<br>\end{aligned}$$ $$H(Y|X)=H(X,Y)-H(X)$$</li>
<li>互信息 $$I(X,Y)=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}$$ $$I(X,Y)=H(X)+H(Y)-H(X,Y)$$</li>
<li>交叉熵：衡量两个概率分布的相似度 $$H(p,q)=-\sum_xp(x)\log q(x)$$</li>
<li>相对熵（KL散度）：衡量两个概率分布的距离（不相似度） $$D(p||q)=\sum_xp(x)\log\frac{p(x)}{q(x)}$$ $$H(p,q)=H(p)+D(p||q)$$</details>




</li>
</ul>
</li>
</ol>
<ol start="7">
<li><p>什么是分布式数据库？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 分布式数据库系统是在集中式数据库系统成熟技术的基础上发展起来的，但不是简单地把集中式数据库分散地实现，它具有自己的性质和特征。集中式数据库系统的许多概念和技术，如数据独立性、数据共享和减少冗余度、并发控制、完整性、安全性和恢复等在分布式数据库系统中都有了不同的、更加丰富的内容。</p>
 </details>



</li>
</ol>
<ol start="8">
<li><p>#include和#include“filename.h”有什么区别？</p>
 <details>
 <summary><u>答案：</u></summary>

<p> 用 #include 格式来引用标准库的头文件（编译器将从标准库目录开始搜索）。<br> 用 #include “filename.h” 格式来引用非标准库的头文件（编译器将从用户的工作目录开始搜索）。</p>
 </details>



</li>
</ol>
<ol start="9">
<li>某超市研究销售纪录数据后发现，买啤酒的人很大概率也会购买尿布，这种属于数据挖掘的哪类问题？<br> A. 关联规则发现<br> B. 聚类<br> C. 分类<br> D. 自然语言处理 <details>
 <summary><u>答案：</u></summary>
 选A。
 </details>



</li>
</ol>
<ol start="10">
<li>将原始数据进行集成、变换、维度规约、数值规约是在以下哪个步骤的任务？<br>A. 频繁模式挖掘<br>B. 分类和预测<br>C. 数据预处理<br>D. 数据流挖掘<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="11">
<li>下面哪种不属于数据预处理的方法？<br>A. 变量代换<br>B. 离散化<br>C. 聚集<br>D. 估计遗漏值<details>
<summary><u>答案：</u></summary>
选D。
</details>



</li>
</ol>
<ol start="12">
<li>什么是KDD？<br>A. 数据挖掘与知识发现<br>B. 领域知识发现<br>C. 文档知识发现<br>D. 动态知识发现<details>
<summary><u>答案：</u></summary>
选A。
</details>



</li>
</ol>
<ol start="13">
<li>当不知道数据所带标签时，可以使用哪种技术促使带同类标签的数据与带其他标签的数据相分离？<br>A. 分类<br>B. 聚类<br>C. 关联分析<br>D. 隐马尔可夫链<details>
<summary><u>答案：</u></summary>
选A。
</details>



</li>
</ol>
<ol start="14">
<li>建立一个模型，通过这个模型根据已知的变量值来预测其他某个变量值属于数据挖掘的哪一类任务？<br>A. 根据内容检索<br>B. 建模描述<br>C. 预测建模<br>D. 寻找模式和规则<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="15">
<li>以下哪种方法不属于特征选择的标准方法：<br>A. 嵌入<br>B. 过滤<br>C. 包装<br>D. 抽样<details>
<summary><u>答案：</u></summary>
选D。
</details>



</li>
</ol>
<ol start="16">
<li><p>说下红黑树的五个性质。</p>
<details>
<summary><u>答案：</u></summary>

<p>#1. 每个结点要么是红的要么是黑的。<br>#2. 根结点是黑的。<br>#3. 每个叶结点（叶结点即指树尾端NIL指针或NULL结点）都是黑的。<br>#4. 如果一个结点是红的，那么它的两个儿子都是黑的。<br>#5. 对于任意结点而言，其到叶结点树尾端NIL指针的每条路径都包含相同数目的黑结点。</p>
<p><a href="https://blog.csdn.net/v_july_v/article/details/6105630" target="_blank" rel="noopener">https://blog.csdn.net/v_july_v/article/details/6105630</a></p>
</details>



</li>
</ol>
<ol start="17">
<li><p>简述下什么是生成对抗网络。（*）</p>
<details>
<summary><u>答案：</u></summary>

<p>GAN之所以是对抗的，是因为GAN的内部是竞争关系，一方叫generator，它的主要工作是生成图片，并且尽量使得其看上去是来自于训练样本的；另一方是discriminator，其目标是判断输入图片是否属于真实训练样本。<br>更直白的讲，将generator想象成假币制造商，而discriminator是警察。generator目的是尽可能把假币造的跟真的一样，从而能够骗过discriminator，即生成样本并使它看上去好像来自于真实训练样本一样。</p>
</details>



</li>
</ol>
<ol start="18">
<li><p>快速排序算法。</p>
<details>
<summary><u>答案：</u></summary>

<p>算法原理参考：<a href="https://wiki.jikexueyuan.com/project/easy-learn-algorithm/fast-sort.html" target="_blank" rel="noopener">https://wiki.jikexueyuan.com/project/easy-learn-algorithm/fast-sort.html</a> （选第一个元素做pivot）<br>代码参考：<a href="https://www.geeksforgeeks.org/python-program-for-quicksort/" target="_blank" rel="noopener">https://www.geeksforgeeks.org/python-program-for-quicksort/</a> （选最后一个元素做pivot）</p>
<p>快排是把一个序列分为较小和较大的2个子序列，然后递归地排序两个子序列。步骤是：</p>
<ol>
<li>挑选基准值：从数列中挑出一个元素，称为“基准”（pivot）；
<li>分割（partition）：重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（与基准值相等的数可以到任何一边）。在这个分割结束之后，对基准值的排序就已经完成；
<li>递归排序子序列：递归地将小于基准值元素的子序列和大于基准值元素的子序列排序。
</ol>

<p>选择基准的方式有几种：</p>
<ul>
<li>第一个元素
<li>最后一个元素
<li>随机选择一个元素
<li>中位数
</ul>    

<p>假如选第一个元素作为pivot：</p>
<img src="https://pic.downk.cc/item/5e8d3e67504f4bcb04d5906a.png" width=350>
<img src="https://pic.downk.cc/item/5e8d3e8f504f4bcb04d5a708.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3e9f504f4bcb04d5b03c.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3eab504f4bcb04d5b750.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3eba504f4bcb04d5c098.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3ec8504f4bcb04d5c976.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3ed4504f4bcb04d5d0bf.png" width=300>
<img src="https://pic.downk.cc/item/5e8d3edf504f4bcb04d5d751.png" width=300>

<p>伪代码：</p>
<pre><code>// low --&gt; Starting index, high --&gt; Ending index
quickSort(arr[], low, high)
{
    if (low &lt; high)
    {
        // pi is partitioning index, arr[pi] is now at right place
        pi = partition(arr, low, high);

        quickSort(arr, low, pi - 1);    // Before pi
        quickSort(arr, pi + 1, high);   // After pi
    }
}</code></pre><p>* 其实Python的<code>sorted</code>函数就可以排序，快得一批。</p>
</details>



</li>
</ol>
<ol start="19">
<li><p>说说常见的损失函数。</p>
<details>
<summary><u>答案：</u></summary>

<ol>
<li><b>0-1 损失函数</b>$$ L\left(Y,f(X)\right)=\left\{
\begin{aligned}
&1, && Y\ne f(X)\\\\
&0, && Y=f(X)
\end{aligned}\right.$$ 0-1 损失是用在分类问题上，也就是分类对了损失为零，分类错了损失为1，但它是非凸函数，难以求解，不太适用。

<li><b>绝对值损失函数</b>$$L\left(y,f(X)\right)=\lvert y-f(X)\rvert$$ 这个也是非凸函数，用得比较少。<br>
绝对平均误差（Mean Absolute Error, MAE）$$\text{MAE}=\frac{1}{n}\sum^n_{i=1}\lvert y_i-f(X_i)\rvert$$ 对异常值的敏感度较低，最小化MAE得到是y的中位数。

<li><b>平方损失函数</b>$$L\left(y,f(X)\right)=\left(y-f(X)\right) ^2$$ 这个是凸函数，在回归模型比较常用。<br>
均方误差（Mean Squared Error, MSE）$$\text{MSE}=\frac{1}{n}\sum^n_{i=1}\left(y_i-f(X_i)\right)^2$$ 最小化MSE得到的是y的均值。

<li><b>log对数损失函数（交叉熵）</b>$$L\left(y,P(y|X)\right)=-\log P(y|X)$$ log损失适用于表示概率分布，适用于分类问题，相对Hinge损失它会对噪声更敏感。逻辑回归的损失函数就是交叉熵 $$\text{cross entropy}=\frac{1}{n}\sum^n_{i=1}\left\{-y_i\log{f(X_i)}-(1-y_i)\log(1-f(X_i))\right\}$$

<li><b>Hinge损失函数</b>$$L\left(y,f(X))\right)=\max(0,1-yf(X))$$ Hinge损失适用于分类问题，如果分类正确损失为0，分类错误损失为 $1-yf(X)$。SVM的损失函数就是用的这个。    
</ol>
</details>



</li>
</ol>
<ol start="20">
<li><p>交叉熵函数与最大似然函数的联系和区别？</p>
<details>
<summary><u>答案：</u></summary>

<p>区别：交叉熵函数使用来描述模型预测值和真实值的差距大小，越大代表越不相近；似然函数的本质就是衡量在某个参数下，整体的估计和真实的情况一样的概率，越大代表越相近。</p>
<p>联系：交叉熵函数可以由最大似然函数在伯努利分布的条件下推导出来，或者说<strong>最小化交叉熵函数的本质就是对数似然函数的最大化。</strong><br>给一个服从Bernoulli分布变量 $X\sim\mathcal{B}(p)$ $$\begin{aligned}&amp;P(X=1)=p\\&amp;P(X=0)=1-p\end{aligned}$$ $$P(X)=p^X(1-p)^{1-X}$$ 假设我们有数据样本 $D$，最大化下面的对数似然函数 $$\begin{aligned}\log P(D)&amp;=\log \prod^N_i P(D_i)=\sum_i\log p(D_i)\\&amp;=\sum_i(D_i\log p+(1-D_i)\log(1-p))\end{aligned}$$ 所以我们是要 $\max\left\{ D_i\log p+(1-D_i)\log(1-p)\right\}$，等同于 $\min\left\{-\left(D_i\log p+(1-D_i)\log(1-p)\right)\right\}$，那就是最小化交叉熵损失函数咯。</p>
</details>



</li>
</ol>
<ol start="21">
<li><p>计算三个稠密矩阵 A、B、C 的乘积 ABC，假定三个矩阵的尺寸分别为 m*n, n*p,p*q，且 m&lt;n&lt;p&lt;q，以下计算效率最高的是<br>A. (AB)C<br>B. A(BC)<br>C. (AC)B<br>D. (BC)A</p>
<details>
<summary><u>答案：</u></summary>

<p>选A。首先C和D是错误的，乘不了的。<br><strong>选项A的计算量</strong><br>(AB)：A的行（每行n个数）乘以B的列（每列n个数）–&gt; n次乘法，相加 –&gt; n-1次加法，总共 2n-1 次运算。然后A的m行乘B的p列，(AB)的运算次数总共是 mp(2n-1)。<br>(AB)得到一个 m*p 的矩阵，再乘C，运算次数是 mq(2p-1)。选项A的计算量是 <strong>mp(2n-1)+mq(2p-1)</strong>。<br><strong>选项B的计算量</strong><br>(BC)的计算量是 nq(2p-1)，得到 n*q 矩阵，与A相乘，计算量是 mq(2n-1)。选项B的计算量是 <strong>nq(2p-1)+mq(2n-1)</strong>。</p>
<p>对比一下，选项B的计算量比较大【mp(2n-1)&lt;mq(2n-1)，mq(2p-1)&lt;nq(2p-1)】，所以选A。</p>
</details>



</li>
</ol>
<ol start="22">
<li><p>已知一组数据的协方差矩阵P，下面关于主分量说法错误的是<br>A. 主分量分析的最佳准则是对一组数据进行按一组正交基分解，在只取相同数量分量的条件下，以均方误差计算截尾误差最小<br>B. 在经主分量分解后，协方差矩阵成为对角矩阵<br>C. 主分量分析就是K-L变换<br>D. 主分量是通过求协方差矩阵的特征值得到</p>
<details>
<summary><u>答案：</u></summary>

<p>选C。K-L变换与PCA变换是不同的概念，PCA的变换矩阵是协方差矩阵，K-L变换的变换矩阵可以有很多种（二阶矩阵、协方差矩阵、总类内离散度矩阵等等）。当K-L变换矩阵为协方差矩阵时，等同于PCA。KLT是一种对于连续或离散的随机过程都可进行的变换，PCA则是KLT处理离散情况的算法；定义上KLT比PCA广泛，而实际上PCA比KLT实用。</p>
</details>



</li>
</ol>
<ol start="23">
<li><p>模式识别中，马式距离较之于欧式距离的优点是<br>A. 平移不变性<br>B. 旋转不变性<br>C. 尺度不变性<br>D. 考虑了模式的分布</p>
<details>
<summary><u>答案：</u></summary>

<p>选CD。<br>欧式距离特性有：平移不变性、旋转不变性。$$d_E(\mathbf{x},\mathbf{y})=\sqrt{(\mathbf{x}-\mathbf{y})^T(\mathbf{x}-\mathbf{y})}$$<br>马式距离特性有：平移不变性、旋转不变性、尺度缩放不变性、不受量纲影响、考虑了模式的分布。$$d_M(\mathbf{x},\mathbf{y})=\sqrt{(\mathbf{x}-\mathbf{y})^T\Sigma^{-1}(\mathbf{x}-\mathbf{y})}$$ $\Sigma$是$\mathbf{x}$和$\mathbf{y}$协方差矩阵。</p>
</details>



</li>
</ol>
<ol start="24">
<li><p>下面程序的功能是输出数组的全排列,选择正确的选项,完成其功能。</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">perm</span><span class="params">(<span class="keyword">int</span> <span class="built_in">list</span>[], <span class="keyword">int</span> k, <span class="keyword">int</span> m)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (▊▊▊)</span><br><span class="line">&#123;</span><br><span class="line">    copy(<span class="built_in">list</span>,<span class="built_in">list</span>+m,ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>,<span class="string">" "</span>));</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i=k; i&lt;=m; i++)</span><br><span class="line">&#123;</span><br><span class="line">    swap(&amp;<span class="built_in">list</span>[k],&amp;<span class="built_in">list</span>[i]);</span><br><span class="line">    (▊▊▊);</span><br><span class="line">    swap(&amp;<span class="built_in">list</span>[k],&amp;<span class="built_in">list</span>[i]);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>A. k!=m 和 perm(list,k+1,m)<br>B. k==m 和 perm(list,k+1,m)<br>C. k!=m 和 perm(list,k,m)<br>D. k==m 和 perm(list,k,m)</p>
<details>
<summary><u>答案：</u></summary>
选B。k输入时为0，递归直到k=m结束。
第一个swap是依次将第k个元素和k~m个元素交换，交换完后进入递归，再全排列其子数组。递归调用，之所以从k+1开始，是因为如果传入的是k的话，递归传进去的参数不变，递归将永无止境的递归下去，因为k永远不会等于m。
第二个swap是将该层交换完后的数组再还原，目的是为了使递归返回后不改变上一层的数组元素顺序，方便下一次交换。
</details>



</li>
</ol>
<ol start="25">
<li>若有33个长度不等的初始归并段，做7路平衡归并排序，为组织最佳归并树，应增加长度为0的初始归并段的个数是<strong>____</strong>。<br>A. 0<br>B. 2<br>C. 4<br>D. 6<details>
<summary><u>答案：</u></summary>
(33+2)/7=5;
(5+2)/7=1;
2+2=4.选C
</details>



</li>
</ol>
<ol start="26">
<li>将一个整数序列整理为升序，两趟处理后序列变为10,12,21,9,7,3,4,25，则采用的排序算法可能是<strong>____</strong>。<br>A. 插入排序<br>B. 选择排序<br>C. 快速排序<br>D. 堆排序<details>
<summary><u>答案：</u></summary>
选A。
B错，排两次应该前两个数最小。
C错，快排每次选一个“基准”，左边全部小于基准，右边全部大于基准。
D错，第一趟就应该使得第一个非叶子节点len/2-1=3 (9) 比 2*3+1=7 (25) 大。
第一个非叶子节点 i=len/2-1，与它的子节点 2i+1, 2i+2 对比。
</details>



</li>
</ol>
<ol start="27">
<li>在机器学习任务中经常假设矩阵为n×n的对称矩阵A， 则以下说法正确的是<br>A. 对称矩阵为满秩矩阵<br>B. 对称矩阵的列向量之间正交<br>C. 对应于A的不同特征值的特征向量之间正交<br>D. 对应于A的相同特征值得特征向量之间正交<details>
<summary><u>答案：</u></summary>
选C。
</details>



</li>
</ol>
<ol start="28">
<li>有一类二叉树用三叉链表来存储的时候除了带有指向左右孩子节点的两个指针，还有指向父节点的指针，那么这样一棵二叉树有2个节点，那么有多少指针指向NULL（注：根节点的父指针指向NULL，对于不存在的节点表示为NULL）？<br>A. 1<br>B. 2<br>C. 3<br>D. 4<details>
<summary><u>答案：</u></summary>
选D。
<img src='https://pic.downk.cc/item/5eb55c59c2a9a83be5b85ede.png' width=400>
</details>



</li>
</ol>
<ol start="29">
<li>下列最短路径算法的叙述中正确的是（）<br>A. Dijkstra算法通常用于求每一对顶点间的最短路径；<br>B. Dijkstra算法不允许图中带有负权值的边，而Floyd算法则可以适用；<br>C. Floyd算法通常用于求某一顶点到其他各顶点的最短路径；<br>D. Floyd算法允许有包含负权值的边组成的回路，而Dijkstra算法不允许；<details>
<summary><u>答案：</u></summary>
选B。
Dijkstra算法是计算图中的一个点到其它点的最小路径.
Floyd算法计算图中任意一对点的最短路径.
</details>



</li>
</ol>
<ol start="30">
<li><p>判断一个数组或序列是正序,倒序还是乱序,需要我们将这个数组完整的遍历一遍通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应的位置并插入的排序算法是（ ）<br>A. 选择排序<br>B. 希尔排序<br>C. 插入排序<br>D. 归并排序</p>
<details>
<summary><u>答案：</u></summary>

<p>选C。<br>选择排序：每次从数组中选出一个最小数（最大数）放到数组最前面，存放在序列的起始位置，直到全部待排序的数据元素排完。<br>希尔排序：设置增量分割数组，逐步进行直接插入排序，增量逐趟减少，并最后使得整个数组基本有序，再对整体进行直接插入排序。<br>插入排序：构建有序序列，未排序数据依次从已排序数据按从后往前比较，插入到合适的位置。<br>归并排序：把序列分成两个长度为n/2的子序列，对这两个子序列分别归并排序（循环将两个数组的第一个值比较，并弹出第一个值，直到数组长度都不存在），将两个排序好的子序列合并成一个最终的排序序列。</p>
</details>



</li>
</ol>
<ol start="31">
<li><p>字符串有5个字符q,w,e,r,t，出现的频率分别为1,2,3,4,5，如果采用Huffman编码对字符串编码，则每个字符编码的平均长度是（）?<br>A. 2.2<br>B. 2.4<br>C. 2.6<br>D. 2.8<br>E. 3.0</p>
<details>
<summary><u>答案：</u></summary>

<p>选B。</p>
<img src='https://pic.downk.cc/item/5eb6b5a3c2a9a83be5273cb8.png' width=250>

<table>
<thead>
<tr>
<th align="center">字符</th>
<th align="center">编码</th>
<th align="center">长度</th>
</tr>
</thead>
<tbody><tr>
<td align="center">q</td>
<td align="center">010</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">w</td>
<td align="center">011</td>
<td align="center">3</td>
</tr>
<tr>
<td align="center">e</td>
<td align="center">00</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">r</td>
<td align="center">10</td>
<td align="center">2</td>
</tr>
<tr>
<td align="center">t</td>
<td align="center">11</td>
<td align="center">2</td>
</tr>
</tbody></table>
<p>(3+3+2+2+2)/5=2.4</p>
</details>




</li>
</ol>
]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>Lagrange_Multipliers</title>
    <url>/2020/03/24/Lagrange-Multipliers/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Support Vector Machine (SVM)</title>
    <url>/2020/03/24/SVM/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>ML</category>
      </categories>
      <tags>
        <tag>数学计算</tag>
        <tag>对偶问题</tag>
        <tag>SVM</tag>
        <tag>拉格朗日乘子法</tag>
        <tag>KKT条件</tag>
        <tag>硬间隔&amp;软间隔</tag>
        <tag>核函数</tag>
      </tags>
  </entry>
  <entry>
    <title>2020春，是世界末日的开端，还是黎明前的黑暗</title>
    <url>/2020/03/20/2020%E6%98%A5%EF%BC%8C%E6%98%AF%E4%B8%96%E7%95%8C%E6%9C%AB%E6%97%A5%E7%9A%84%E5%BC%80%E7%AB%AF%EF%BC%8C%E8%BF%98%E6%98%AF%E9%BB%8E%E6%98%8E%E5%89%8D%E7%9A%84%E9%BB%91%E6%9A%97/</url>
    <content><![CDATA[<h2 id="2020春，被迫进入HARD模式"><a href="#2020春，被迫进入HARD模式" class="headerlink" title="2020春，被迫进入HARD模式"></a>2020春，被迫进入HARD模式</h2><p>开始放春节假了，下午大家都逐渐放慢手脚。收拾好我的小抱枕拿回去洗洗，也没打算拷啥回去做了，想学啥我收藏夹里的东西这辈子都要学不完。好好放个假，两个星期后回来开工！<br>现在过年又不能放鞭炮，我们家又不爱走亲戚，我的关注点都是在贺岁片上。今年的贺岁片都是2D，不搞3D那种花里胡哨的东西，还都挺期待的。<br>20号的时候看到有自媒体总结贺岁片我还兴致勃勃地转发“安排上了”，当时新闻已经报了有传染病病例，但是我们都没有觉得是什么大事。好像那时候是深圳有了病例吧，记不太清了，现在还是各处管制着没有完全解封，但对疫情扩散之前的印象居然有点模糊了。有个在深圳工作的朋友提醒我别乱跑了。我还想这孩子居然忽然这么严肃。<br><img src="https://pic.downk.cc/item/5e60f87398271cb2b8bf58e7.jpg" width=350><br>那晚还是买了贺岁片的电影票，年初一早上一场，晚上一场。<br><img src="https://pic.downk.cc/item/5e60fb8d98271cb2b8c06f53.jpg" width=350><br>很快，我就开始跳脚售票平台不主动给我提供退票渠道了。然后到了23号，贺岁档电影接连宣布退档。<br><img src="https://pic.downk.cc/item/5e60fbab98271cb2b8c07ac1.jpg" width=350><br>再后面支付宝的交易信息是一串的口罩交易记录，付款-退款，付款-退款。现在回头看那段时间真的是昏暗，即使我们不在疫区，身边没有病例，但是大家都受到很大影响。到处买不到口罩，不断收到新确诊病例的消息。明明应该是欢天喜地的春节，却笼罩在一片迷雾中。感觉这种日子看不到尽头。</p>
<h2 id="在家办公初体验"><a href="#在家办公初体验" class="headerlink" title="在家办公初体验"></a>在家办公初体验</h2><p>春节假期快要结束了，大家都很不愿意回去上班。形势很不明朗，病毒传播性很强。各地政府都出文推迟开工时间。我们也如愿推迟了，一开始是推迟两天，再接着是一周，再然后就直接不通知上班时间了，届时再通知。<br>我们新项目本来打算年后把之前的计划立马落实，争取了在家办公。本来我们办公大多数时候都是各自敲电脑，实行起来也不太艰难。就是在家办公工资被粉碎性打折，但是活还是一样多，还得花自己水电。<br>在家办公三周了，大家都开始有点坐不住了，希望回到公司上班，不然太没有安全感。月底那周老大也通知在外地的同事要回到广州，随时准备回公司上班。<br>接下来的一些不快经历……或许人一辈子总会遇到一些渣吧。过眼云烟 ┓( ´∀` )┏</p>
<h2 id="Move-on"><a href="#Move-on" class="headerlink" title="Move on"></a>Move on</h2><p>“如果选择做的事是自己认可的，且有它的意义在，我会把心态调整好再开始。做了又抱怨，这是逻辑问题。”某人说的这句话很平实很有道理。<br>我还是比较情绪化的，喜欢埋怨或许是每个人的常态。抱怨也抱怨过了，再抱怨下去于事无补。该调整心态，调整计划，走好接下来的路。计划本来也是有的，只是之前过得有点安逸了。这次的不快对我来说可能转变成一个好事，被迫加快落实我的个人规划，望尽早拨云见日。</p>
<p>距上次弄出来个github博客之后一年了，一直都没有更新过。我都忘了看的哪个教程搭出来的了，之前好像因为前端技能没有点亮遇到瓶颈就丢弃了。这次找到对的方式认真搭好，开始用起来！<br>另外，最近我觉得还是有不少好事的，虽然不是发生在我身上，但都是我朋友的好事，我也觉得很快乐。老同学准备入职新公司了，仙女网友添丁了，沙雕网友拉埋天窗了，留学时的好朋友在当地买房了。跳槽、添丁、结婚、置业，一堆喜事萦绕着我呀~<br>我也希望实现19年末的新年许愿，在2020年找到新工作！<br><img src="https://pic.downk.cc/item/5e7aca66504f4bcb049f780c.jpg" width=300><br>$$====== Fin =====$$</p>
]]></content>
  </entry>
  <entry>
    <title>Linux CheatSheet</title>
    <url>/2020/03/19/Linux-CheatSheet/</url>
    <content><![CDATA[<ol>
<li><p>cron 后台常驻程序 (daemon) 用于：<br>A. 负责文件在网络中的共享<br>B. 管理打印子系统<br>C. 跟踪管理系统信息和错误<br>D. 管理系统日常任务的调度</p>
<details>
<summary><u>答案：</u></summary>
D.
</details>
</li>
<li><p>在大多数Linux发行版本中，以下哪个属于块设备 (block devices) ？<br>A. 串行口<br>B. 硬盘<br>C. 虚拟终端<br>D. 打印机</p>
<details>
<summary><u>答案：</u></summary>
B.
</details>
</li>
<li><p>下面哪个Linux命令可以一次显示一页内容？<br>A. pause<br>B. cat<br>C. more<br>D. grep</p>
<details>
<summary><u>答案：</u></summary>
C.
</details>


</li>
</ol>
<ol start="4">
<li><p>怎样了解您在当前目录下还有多大空间？<br>A. Use df<br>B. Use du /<br>C. Use du .<br>D. Use df .</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>怎样更改一个文件的权限设置？<br>A. attrib<br>B. chmod<br>C. change<br>D. file</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>假如您需要找出 /etc/my.conf 文件属于哪个包 (package) ，您可以执行：<br>A. rpm -q /etc/my.conf<br>B. rpm -requires /etc/my.conf<br>C. rpm -qf /etc/my.conf<br>D. rpm -q | grep /etc/my.conf</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>假如当前系统是在 level 3 运行，怎样不重启系统就可转换到 level 5 运行？<br>A. Set level = 5<br>B. telinit 5<br>C. run 5<br>D. ALT-F7-5</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>那个命令用于改变 IDE 硬盘的设置？<br>A. hdparam<br>B. ideconfig<br>C. hdparm<br>D. hddparm</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>下面哪个命令可以列出定义在以后特定时间运行一次的所有任务？<br>A. atq<br>B. cron<br>C. batch<br>D. at</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>下面命令的作用是：set PS1=”[\u\w\t]\$“ ; export PS1<br>A. 改变错误信息提示<br>B. 改变命令提示符<br>C. 改变一些终端参数<br>D. 改变辅助命令提示符</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>作为一个管理员，你希望在每一个新用户的目录下放一个文件 .bashrc ，那么你应该在哪个目录下放这个文件，以便于新用户创建主目录时自动将这个文件复制到自己的目录下。<br>A. /etc/skel/<br>B. /etc/default/<br>C. /etc/defaults/<br>D. /etc/profile.d/</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在bash中，export命令的作用是：<br>A. 在子shell中运行命令<br>B. 使在子shell中可以使用命令历史记录<br>C. 为其它应用程序设置环境变量<br>D. 提供NFS分区给网络中的其它系统使用</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>在使用了shadow口令的系统中，/etc/passwd和/etc/shadow两个文件的权限正确的是：<br>A. -rw-r—– , -r——–<br>B. -rw-r–r– , -r–r–r–<br>C. -rw-r–r– , -r——–<br>D. -rw-r–rw- , -r—–r–</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>下面哪个参数可以删除一个用户并同时删除用户的主目录？<br>A. rmuser -r<br>B. deluser -r<br>C. userdel -r<br>D. usermgr -r</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>有一个备份程序mybackup，需要在周一至周五下午1点和晚上8点各运行一次，下面哪条crontab的项可以完成这项工作？<br>A. 0 13,20 <em> </em> 1,5 mybackup<br>B. 0 13,20 <em> </em> 1,2,3,4,5 mybackup<br>C. <em> 13,20 </em> <em> 1,2,3,4,5 mybackup<br>D. 0 13,20 1,5 </em> * mybackup</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>如何从当前系统中卸载一个已装载的文件系统<br>A. umount<br>B. dismount<br>C. mount -u<br>D. 从 /etc/fstab 中删除这个文件系统项</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>如果你的umask设置为022，缺省的你创建的文件的权限为：<br>A. —-w–w-<br>B. -w–w—-<br>C. r-xr-x—<br>D. rw-r–r–</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在一条命令中如何查找一个二进制命令 Xconfigurator 的路径？<br>A. apropos Xconfigurator<br>B. find Xconfigurator<br>C. where Xconfigurator<br>D. which Xconfigurator</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>哪一条命令用来装载所有在 /etc/fstab 中定义的文件系统？<br>A. amount<br>B. mount -a<br>C. fmount<br>D. mount -f</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>运行一个脚本，用户不需要什么样的权限？<br>A. read<br>B. write<br>C. execute<br>D. browse on the directory</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>在Linux中，如何标识接在IDE0上的slave硬盘的第2个扩展分区？<br>A. /dev/hdb2<br>B. /dev/hd1b2<br>C. /dev/hdb6<br>D. /dev/hd1b6</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>在应用程序起动时，如何设置进程的优先级？<br>A. priority<br>B. nice<br>C. renice<br>D. setpri</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>在 bash 中, 在一条命令后加入”1&gt;&amp;2” 意味着：<br>A. 标准错误输出重定向到标准输入<br>B. 标准输入重定向到标准错误输出<br>C. 标准输出重定向到标准错误输出<br>D. 标准输出重定向到标准输入</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>下面哪条命令可以把f1.txt复制为f2.txt?<br>A. cp f1.txt | f2.txt<br>B. cat f1.txt | f2.txt<br>C. cat f1.txt &gt; f2.txt<br>D. copy f1.txt | f2.txt</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>显示一个文件最后几行的命令是：<br>A. tac<br>B. tail<br>C. rear<br>D. last</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>如何快速切换到用户John的主目录下？<br>A. cd @John<br>B. cd #John<br>C. cd &amp;John<br>D. cd ~John</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>　
</li>
<li><p>把一个流中所有字符转换成大写字符，可以使用下面哪个命令？<br>A. tr a-z A-Z<br>B. tac a-z A-Z<br>C.sed /a-z/A-Z<br>D. sed –toupper</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>使用什么命令可以查看Linux的启动信息？<br>A. mesg -d<br>B. dmesg<br>C. cat /etc/mesg<br>D. cat /var/mesg</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>运行级定义在：<br>A. in the kernel<br>B. in /etc/inittab<br>C. in /etc/runlevels<br>D. using the rl command</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>如何装载(mount)上在 /etc/fstab 文件中定义的所有文件系统？<br>A. mount -a<br>B. mount /mnt/*<br>C. mount<br>D. mount /etc/fstab</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>使用ln命令将生成了一个指向文件old的符号链接new，如果你将文件old删除，是否还能够访问文件中的数据？<br>A. 不可能再访问<br>B. 仍然可以访问<br>C. 能否访问取决于文件的所有者<br>D. 能否访问取决于文件的权限</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>xt2fs文件系统中，缺省的为root用户保留多大的空间？<br>A. 3%<br>B. 5%<br>C. 10%<br>D. 15%</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>哪个命令用来显示系统中各个分区中inode的使用情况？<br>A. df -i<br>B. df -H<br>C. free -b<br>D. du -a -c /</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>　
</li>
<li><p>多数Linux发行版本中，图形方式的运行级定义为？<br>A. 1<br>B. 2<br>C. 3<br>D. 5</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在系统文档中找到关于print这个单词的所有说明？<br>A. man print<br>B. which print<br>C. locate print<br>D. apropos print</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>man 5 passwd 含义是？<br>A. 显示 passwd 命令的使用方法<br>B. 显示 passwd 文件的结构<br>C. 显示 passwd 命令的说明的前五行<br>D. 显示关于passwd的前五处说明文档。</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>如何在文件中查找显示所有以”<em>“打头的行？<br>A. find \</em> file<br>B. wc -l <em> &lt; file<br>C. grep -n </em> file<br>D. grep ‘^*’ file</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在ps命令中什么参数是用来显示所有用户的进程的？<br>A. a<br>B. b<br>C. u<br>D. x</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>显示二进制文件的命令是？<br>A. od<br>B. vil<br>C. view<br>D. binview</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>如何显示Linux系统中注册的用户数（包含系统用户）？<br>A. account -l<br>B. nl /etc/passwd |head<br>C. wc –users /etc/passwd<br>D. wc –lines /etc/passwd</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在一行结束位置加上什么符号，表示未结束，下一行继续？<br>A. /<br>B. <br>C. ;<br>D. |</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>命令 kill 9 的含义是：<br>A. kills the process whose PID is 9.<br>B. kills all processes belonging to UID 9.<br>C. sends SIGKILL to the process whose PID is 9.<br>D. sends SIGTERM to the process whose PID IS 9.</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>如何删除一个非空子目录/tmp？<br>A. del /tmp/<em><br>B. rm -rf /tmp<br>C. rm -Ra /tmp/</em><br>D. rm -rf /tmp/*</p>
<details>
<summary><u>答案：</u></summary>
B. rm -rf * 删除当前目录下所有文件
</details>
</li>
<li><p>使用什么命令可以在今天午夜运行命令 cmd1 ？<br>A. at midnight cmd1<br>B. cron -at “00:00” cmd1<br>C. batch -t “00:00” &lt; cmd1<br>D. echo “cmd1” | at midnight</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>你的系统使用增量备份策略，当需要恢复系统时，你需要按什么顺序恢复备份数据？<br>A. 最后一次全备份，然后从最早到最近的增量备份<br>B. 最后一次全备份，然后从最近到最早的增量备份<br>C. 最早到最近的增量备份，然后最后一次全备份<br>D. 最近到最早的增量备份，然后最后一次全备份</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>对所有用户的变量设置，应当放在哪个文件下？<br>A. /etc/bashrc<br>B. /etc/profile<br>C. ~/.bash_profile<br>D. /etc/skel/.bashrc</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>Linux系统中，一般把命令 ls 定义为 ls –color 的别名，以便以不同颜色来标识不同类型的文件。但是，如何能够使用原先的ls命令？<br>A. \ls<br>B. ;ls<br>C. ls $$<br>D. ls –noalias</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在Linux系统中的脚本文件一般以什么开头？<br>A. $/bin/sh<br>B. #!/bin/sh<br>C. use /bin/sh<br>D. set shell=/bin/sh</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>下面哪种写法表示如果cmd1成功执行，则执行cmd2命令？<br>A. cmd1&amp;&amp;cmd2<br>B. cmd1|cmd2<br>C. cmd1;cmd2<br>D. cmd1||cmd2</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在哪个文件中定义网卡的I/O地址？<br>A. cat /proc/modules<br>B. cat /proc/devices<br>C. cat /proc/ioports<br>D. cat /io/dma</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>Linux中，提供TCP/IP包过滤功能的软件叫什么？<br>A. rarp<br>B. route<br>C. iptables<br>D. filter</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>如何暂停一个打印队列？<br>A. lpr<br>B. lpq<br>C. lpc<br>D. lpd</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>在vi中退出不保存的命令是？<br>A. :q<br>B. :w<br>C. :wq<br>D. :q!</p>
<details>
<summary><u>答案：</u></summary>
D. A. :q 退出；B. :w 保存 C. :wq 保存并退出 D. :q! 强制退出
</details>
</li>
<li><p>在 XFree86 3.x 中, 缺省的字体服务器为：<br>A. xfs<br>B. xfserv<br>C. fonts<br>D. xfstt</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>使用什么命令检测基本网络连接？<br>A. ping<br>B. route<br>C. netstat<br>D. ifconfig</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>下面哪个协议使用了二个以上的端口？<br>A. telnet<br>B. FTP<br>C. rsh<br>D. HTTP</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>在PPP协议中，哪个认证协议不以明文传递密码？<br>A. PAM<br>B. PAP<br>C. PGP<br>D. CHAP</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>下面哪个文件系统应该分配最大的空间？<br>A. /usr<br>B. /lib<br>C. /root<br>D. /bin</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>如何在Debian系统中安装rpm包？<br>A. alien pkgname.rpm<br>B. dpkg –rpm pkgname.rpm<br>C. dpkg –alien pkgname.rpm<br>D. alien pkganme.rpm ; dpkg -i pkganme.deb</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在安装软件时下面哪一步需要root权限？<br>A. make<br>B. make deps<br>C. make config<br>D. make install</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>什么命令用来只更新已经安装过的rpm软件包？<br>A. rpm -U <em>.rpm<br>B. rpm -F </em>.rpm<br>C. rpm -e <em>.rpm<br>D. rpm -q </em>.rpm</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>在 windows 与 Linux 双起动的系统中，如果要让LILO 管理引导，则 LILO 应该放在：<br>A. MBR<br>B. /<br>C. root分区的首扇区<br>D. /LILO</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>ldconfig的配置文件是<br>A. /lib/ld.so<br>B. /etc/ld.so.conf<br>C. /etc/ld.so.cache<br>D. /etc/modules.conf</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>下面哪个命令可以压缩部分文件：<br>A. tar -dzvf filename.tgz <em><br>B. tar -tzvf filename.tgz </em><br>C. tar -czvf filename.tgz <em><br>D. tar -xzvf filename.tgz </em></p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>网络服务的daemon是：<br>A. lpd<br>B. netd<br>C. httpd<br>D. inetd</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>Linux与windows 的网上领居互联，需要提供什么daemon?<br>A. bind<br>B. smbd<br>C. nmbd<br>D. shard</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>对于Apache服务器，提供的子进程的缺省的用户是：<br>A. root<br>B. apached<br>C. httpd<br>D. nobody</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>sendmail中缺省的未发出信件的存放位置是：<br>A. /var/mail/<br>B. /var/spool/mail/<br>C. /var/spool/mqueue/<br>D. /var/mail/deliver/</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>apache的主配置文件是：<br>A. httpd.conf<br>B. httpd.cfg<br>C. access.cfg<br>D. apache.conf</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>关于可装载的模块，装载时的参数，如I/O地址等的存放位置是：<br>A. /etc/conf.modules<br>B. /etc/lilo.conf<br>C. /boot/System.map<br>D. /etc/sysconfig</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在 Linux 中，如何关闭邮件提示？<br>A. biff n<br>B. mesg n<br>C. notify off<br>D. set notify=off</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在 bash shell 环境下，当一命令正在执行时，按下 control-Z 会：<br>A. 中止前台任务<br>B. 给当前文件加上 EOF.<br>C. 将前台任务转入后台<br>D. 注销当前用户</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>定义bash环境的用户文件是：<br>A. bash &amp; .bashrc<br>B. bashrc &amp; .bash_conf<br>C. bashrc &amp; bash_profile<br>D. .bashrc &amp; .bash_profile</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>下面哪条命令用来显示一个程序所使用的库文件？<br>A. ldd<br>B. ld so<br>C. modprobe<br>D. ldconfig</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>如何查看一个RPM软件的配置文件的存放位置？<br>A. rpm -qc rpm1<br>B. rpm -Vc rpm1<br>C. rpm –config rpm1<br>D. rpm -qa –config rpm1</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>如何查看一个RPM软件的修改记录？<br>A. rpm -Vc postfix<br>B. rpm -qpil postfix<br>C. rpm –changelog postfix<br>D. rpm -q –changelog postfix</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>通过Makefile来安装已编译过的代码的命令是：<br>A. make<br>B. install<br>C. make depend<br>D. make install</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>什么命令解压缩tar文件？<br>A. tar -czvf filename.tgz<br>B. tar -xzvf filename.tgz<br>C. tar -tzvf filename.tgz<br>D. tar -dzvf filename.tgz</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>在 XF86Config 配置文件中，哪个段用来设置字体文件？<br>A. The Fonts section.<br>B. The Files section.<br>C. The xfsCodes section.<br>D. The Graphics section.</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>8 bit color 指的是：<br>A. 64K colors<br>B. 16K colors<br>C. 256 colors<br>D. 16M colors</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>下面哪个文件用来设置 X window 的显示分辨率？<br>A. xinit<br>B. xinitrc<br>C. XF86Setup<br>D. XF86Config</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>哪个变量用来指定一个远程X应用程序将输出放到哪个X server上？<br>A. DISPLAY<br>B. TERM<br>C. ECHO<br>D. OUTPUT</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>在xdm的配置目录中，哪个文件用来设置在用户通过xdm登录后自动起动的应用程序？<br>A. The Xsession file<br>B. The Xsetup_0 file<br>C. The Xstart_up file<br>D. The GiveConsole file</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>命令 netstat -a 停了很长时间没有响应，这可能是哪里的问题？<br>A. NFS.<br>B. DNS.<br>C. NIS.<br>D. routing.</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>ping使用的协议是：<br>A. TCP<br>B. UDP<br>C. SMB<br>D. ICMP</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>下面哪个命令不是用来查看网络故障的？<br>A. ping<br>B. init<br>C. telnet<br>D. netstat</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>拨号上网使用的协议通常是：<br>A. PPP<br>B. UUCP<br>C. SLIP<br>D. Ethernet</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>TCP/IP中，哪个协议是用来进行IP自动分配的？<br>A. ARP<br>B. NFS<br>C. DHCP<br>D. DNS</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>下面哪个文件定义了网络服务的端口？<br>A. /etc/netport<br>B. /etc/services<br>C. /etc/server<br>D. /etc/netconf</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>下面哪个功能用来生成一个文件的校验码？<br>A. md5<br>B. tar<br>C. crypt<br>D. md5sum</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>缺省的，用户邮件放在：<br>A. ~/mail/<br>B. /var/mail/<br>C. /var/mail/spool/<br>D. /var/spool/mail/</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>下面哪个文件包含了供 NFS daemon 使用的目录列表？<br>A. /etc/nfs<br>B. /etc/nfs.conf<br>C. /etc/exports<br>D. /etc/netdir</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>如何停止一台机器的telnet服务？<br>A. Put NONE in /etc/telnet.allow<br>B. Put a line ‘ALL:ALL’ in /etc/hosts.deny<br>C. Comment the telnet entry in /etc/inittab<br>D. Comment the telnet entry in /etc/xinetd.conf</p>
<details>
<summary><u>答案：</u></summary>
D. 
</details>
</li>
<li><p>在哪个文件中保存了sendmail的别名？<br>A. /etc/aliases<br>B. /etc/mailaliases<br>C. /etc/sendmail.aliases<br>D. /etc/sendmail/aliases</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>smbd and nmbddaemons 的配置文件是：<br>A. /etc/exports<br>B. /etc/smb.conf<br>C. /etc/samba/config<br>D. /usr/local/samba.cfg</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>下面哪个命令用来卸载一个内核模块？<br>A. rmmod<br>B. unmod<br>C. delmod<br>D. modprobe</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>什么情况下必须运行lilo<br>A. once a day from cron<br>B. once a week from cron<br>C. after installing a new kernel<br>D. after installing a new module</p>
<details>
<summary><u>答案：</u></summary>
C. 
</details>
</li>
<li><p>什么命令显示所有装载的模块？<br>A. lsmod<br>B. dirmod<br>C. modules<br>D. modlist</p>
<details>
<summary><u>答案：</u></summary>
A. 
</details>
</li>
<li><p>下面哪个命令刷新打印机队列？<br>A. lpflush<br>B. lprm -<br>C. lpclear<br>D. lprm all</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>
</li>
<li><p>下面哪个命令可以查看网卡的中断？<br>A. cat /proc/ioports<br>B. cat /proc/interrupts<br>C. cat /proc/memoryinfo<br>D. which interrupts</p>
<details>
<summary><u>答案：</u></summary>
B. 
</details>



</li>
</ol>
<ol start="101">
<li>将当前命令sh test.sh任务在后台执行，下列最优雅的的做法是<br>A. sh test.sh \&amp;amp;<br>B. nohup sh test.sh<br>C. nohup sh test.sh &amp;amp;<br>D. nohup sh test.sh &amp;amp;&amp;amp;<details>
<summary><u>答案：</u></summary>
C. nohup:表示不挂起，在你退出账户之后，进程继续在后台运行，一般形式为：nohup command &
</details>



</li>
</ol>
<ol start="102">
<li>截取logfile文件中含有suc的行，并且只输出最后一列，下列操作正确的是:<br>A. grep -o ‘suc’ logfile | awk ‘{print $0}’<br>B. grep ‘suc’ logfile | awk ‘{print $0}’<br>C. grep ‘suc’ logfile | awk ‘{print $NF}’<br>D. grep -o ‘suc’ logfile | awk ‘{print $NF}’<details>
<summary><u>答案：</u></summary>
C.
</details>

</li>
</ol>
]]></content>
      <tags>
        <tag>CheatSheet</tag>
      </tags>
  </entry>
  <entry>
    <title>矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵</title>
    <url>/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/</url>
    <content><![CDATA[<h2 id="1-矩阵的秩"><a href="#1-矩阵的秩" class="headerlink" title="1. 矩阵的秩"></a>1. 矩阵的秩</h2><h3 id="1-1-秩的概念"><a href="#1-1-秩的概念" class="headerlink" title="1.1    秩的概念"></a>1.1    秩的概念</h3><p>矩阵的秩Rank是个什么东西，Wiki上面的定义写得挺明白。</p>
<p>Rank (linear algebra) <a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Rank_(linear_algebra)</a></p>
<blockquote>
<p>In linear algebra, the rank of a matrix $A$ is the <font color='red'>dimension of the vector space</font> generated (or spanned) by its <font color='red'>columns</font>. This corresponds to the <font color='red'>maximal number of linearly independent columns</font> of $A$. This, in turn, is identical to the dimension of the vector space spanned by its rows.</p>
</blockquote>
<p>矩阵$A$的秩就是由<font color='red'>矩阵的列</font>构成的<font color='red'>向量空间的维数</font>，也就是线性无关的列的最大数量。<strong>矩阵的列秩 = 行秩。</strong><br>举个栗子：<br>$$<br>A=\begin{pmatrix}<br>    1&amp;        5&amp;        3\\\<br>    2&amp;        10&amp;        6<br>\end{pmatrix} \tag{1}<br>$$<br>这个矩阵$A$的秩是1：后两列$\begin{pmatrix}5\\10\end{pmatrix},\begin{pmatrix}3\\6\end{pmatrix}$都跟第一列$\begin{pmatrix}1\\2\end{pmatrix}$线性相关</p>
<p>如果$A$是这样<br>$$<br>A=\begin{pmatrix}<br>    1&amp;        5&amp;        2\\\<br>    2&amp;        10&amp;        3<br>\end{pmatrix} \tag{2}<br>$$<br>那么他的秩是2：$\begin{pmatrix}1\\2\end{pmatrix}$和$\begin{pmatrix}5\\10\end{pmatrix}$线性相关，但是最后一列$\begin{pmatrix}2\\3\end{pmatrix}$没法表示成前面任意一列的线性组合。</p>
<h3 id="1-2-秩的计算"><a href="#1-2-秩的计算" class="headerlink" title="1.2    秩的计算"></a>1.2    秩的计算</h3><p>矩阵的秩怎么算？高斯消元法做行变换，把矩阵变成行阶梯型矩阵<br>$$<br>\begin{pmatrix}<br>    1&amp;        5&amp;        2\\\<br>    2&amp;        10&amp;        3<br>\end{pmatrix}<br>\xrightarrow{r_2-2r_1\rightarrow r_2}<br>\begin{pmatrix}<br>    1&amp;        5&amp;        2\\\<br>    0&amp;        0&amp;        -1<br>\end{pmatrix}<br>\xrightarrow{(-1)\cdot r_2\rightarrow r_2}<br>\begin{pmatrix}<br>    1&amp;        5&amp;        2\\\<br>    0&amp;        0&amp;        1<br>\end{pmatrix}<br>$$<br>这个行阶梯型矩阵有多少个非零行，矩阵的秩就是多少。</p>
<ul>
<li>计算秩的例子：<a href="https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Computing_the_rank_of_a_matrix" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Rank_(linear_algebra)#Computing_the_rank_of_a_matrix</a></li>
<li>高斯消元法的例子：<a href="https://en.wikipedia.org/wiki/Gaussian_elimination#Example_of_the_algorithm" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Gaussian_elimination#Example_of_the_algorithm</a></li>
</ul>
<p>知道这个计算原理可以了，用Python来算它不香吗！</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array((<span class="number">1</span>,<span class="number">5</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">10</span>,<span class="number">6</span>)).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">np.linalg.matrix_rank(A)</span><br></pre></td></tr></table></figure>
<p>输出：1</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">A = np.array((<span class="number">1</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">10</span>,<span class="number">3</span>)).reshape(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">np.linalg.matrix_rank(A)</span><br></pre></td></tr></table></figure>
<p>输出：2</p>
<h3 id="1-3-满秩矩阵"><a href="#1-3-满秩矩阵" class="headerlink" title="1.3    满秩矩阵"></a>1.3    满秩矩阵</h3><p>秩的一个性质，对于一个m x n的矩阵$A$：<br>$$<br>\text{rank}(A)\leq\min(m,n)\tag{3}<br>$$<br>$A$的秩最大可能等于min(行数, 列数)。</p>
<p>当<br>$$<br>\text{rank}(A)=\min(m,n)\tag{4}<br>$$<br>我们称$A$满秩。</p>
<p>线性回归模型的其中一个基本假设：每个变量$X_j$之间不存在多重共线性（non multicollinearity），可以理解为每个变量之间相互独立。现实当然是没有这么完美的数据，这是模型适用假定的条件。在套用模型之前，我们要检验变量之间是否存在多重共线性（<a href="https://en.wikipedia.org/wiki/Multicollinearity#Detection_of_multicollinearity" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Multicollinearity#Detection_of_multicollinearity</a>），如果存在需要消除多重共线性才可以用线性回归这个模型。<br>基于非多重共线性这个假设，线性回归模型里$\boldsymbol{X}$的秩等于变量的个数$p$，$\text{rank}(\boldsymbol{X})=p$，也就是说满秩。<br>$$<br>\boldsymbol{X}=\begin{pmatrix}<br>           &amp;        &amp;       &amp;         &amp;    &amp;          &amp;     \\\<br>           &amp;        &amp;       &amp;         &amp;    &amp;          &amp;     \\\\<br>    \vec{1}&amp;    X_1 &amp;   X_2 &amp;   \cdots&amp; X_j&amp;    \cdots&amp;  X_p\\\<br>           &amp;        &amp;       &amp;         &amp;    &amp;          &amp;\\\<br>           &amp;        &amp;       &amp;         &amp;    &amp;          &amp;\\\<br>\end{pmatrix}<br>$$</p>
<h3 id="1-4-Gram矩阵"><a href="#1-4-Gram矩阵" class="headerlink" title="1.4    Gram矩阵"></a>1.4    Gram矩阵</h3><p>给定一个实矩阵$A$，矩阵$A^TA$是$A$的列向量的Gram矩阵，而矩阵$AA^T$是$A$的行向量的Gram矩阵。而Gram矩阵的秩是等于原矩阵的秩<br>$$<br>\text{rank}(A^TA)=\text{rank}(AA^T)=\text{rank}(A)=\text{rank}(A^T) \tag{5}<br>$$<br>所以线性回归里的$\boldsymbol{X}^T\boldsymbol{X}$的秩和$\boldsymbol{X}$的秩一样，$\text{rank}(\boldsymbol{X}^T\boldsymbol{X}=p$，而$\boldsymbol{X}^T\boldsymbol{X}$是一个pxp矩阵，所以$\boldsymbol{X}^T\boldsymbol{X}$是满秩的。</p>
<h2 id="2-正定矩阵"><a href="#2-正定矩阵" class="headerlink" title="2. 正定矩阵"></a>2. 正定矩阵</h2><h3 id="2-1-正定矩阵的概念"><a href="#2-1-正定矩阵的概念" class="headerlink" title="2.1    正定矩阵的概念"></a>2.1    正定矩阵的概念</h3><p>对称实矩阵$A\in\mathbb{R}^{K\times K}$是正定的（positive definite），当对于所有非零向量$u$有$u^TAu&gt;0$<br>$$<br>A正定\Longleftrightarrow u^TAu&gt;0,\forall u\in \mathbb{R}^K\setminus 0\tag{6}<br>$$</p>
<h3 id="2-2-Gram矩阵的正定性判断"><a href="#2-2-Gram矩阵的正定性判断" class="headerlink" title="2.2    Gram矩阵的正定性判断"></a>2.2    Gram矩阵的正定性判断</h3><p><strong>定理</strong></p>
<blockquote>
<p>对于一个Gram矩阵$\boldsymbol{G}=G_{ij}=\left&lt; \boldsymbol{v}_{\boldsymbol{i}},\boldsymbol{v}_{\boldsymbol{j}} \right&gt;$，向量$\boldsymbol{v}_{\boldsymbol{i}},\boldsymbol{v}_{\boldsymbol{j}}\left( i,j=1,2,\cdots ,K \right)$<br>$$<br>\boldsymbol{G}=<br>\begin{pmatrix}<br>    \left&lt; \boldsymbol{v}_1,\boldsymbol{v}_1 \right&gt;&amp;        \left&lt; \boldsymbol{v}_1,\boldsymbol{v}_2 \right&gt;&amp;        \cdots&amp;        \left&lt; \boldsymbol{v}_1,\boldsymbol{v}_{\boldsymbol{K}} \right&gt;\\\<br>    \left&lt; \boldsymbol{v}_2,\boldsymbol{v}_1 \right&gt;&amp;        \left&lt; \boldsymbol{v}_2,\boldsymbol{v}_2 \right&gt;&amp;        \cdots&amp;        \left&lt; \boldsymbol{v}_2,\boldsymbol{v}_{\boldsymbol{K}} \right&gt;\\\<br>    \vdots&amp;        \vdots&amp;        \ddots&amp;        \vdots\\\<br>    \left&lt; \boldsymbol{v}_{\boldsymbol{K}},\boldsymbol{v}_1 \right&gt;&amp;        \left&lt; \boldsymbol{v}_{\boldsymbol{K}},\boldsymbol{v}_2 \right&gt;&amp;        \cdots&amp;        \left&lt; \boldsymbol{v}_{\boldsymbol{K}},\boldsymbol{v}_{\boldsymbol{K}} \right&gt;<br>\end{pmatrix}\tag{7}<br>$$<br>$\boldsymbol{G}$是正定矩阵$\Longleftrightarrow \boldsymbol{v}_{\boldsymbol{1}},\boldsymbol{v}_{\boldsymbol{2}},\cdots,\boldsymbol{v}_{\boldsymbol{K}}$这些向量线性无关。</p>
</blockquote>
<p>证明：<br>$\boldsymbol{G}$是对称矩阵，这没什么可说的了。$\boldsymbol{G}=V^TV$<br>$$<br>V=\begin{pmatrix}<br>           &amp;        &amp;       &amp;         \\\<br>           &amp;        &amp;       &amp;         \\\\<br>    \boldsymbol{v}_{\boldsymbol{1}}&amp;    \boldsymbol{v}_{\boldsymbol{2}} &amp;  \cdots&amp; \boldsymbol{v}_{\boldsymbol{K}}\\\<br>           &amp;        &amp;       &amp;       \\\<br>           &amp;        &amp;       &amp;       \\\<br>\end{pmatrix}<br>$$<br>对于任意向量$u\in\mathbb{R}^K$，<br>$$u^T\boldsymbol{G}u=u^TV^TVu=\left( Vu \right) ^TVu=\lVert Vu \rVert ^2\geq 0$$<br>$V$的列向量（$\boldsymbol{v}_{\boldsymbol{1}},\boldsymbol{v}_{\boldsymbol{2}},\cdots,\boldsymbol{v}_{\boldsymbol{K}}$）线性无关 $\Longleftrightarrow$ 只有$u$是零向量的情况，才有$\boldsymbol{v}_1u_1+\boldsymbol{v}_2u_2+\cdots +\boldsymbol{v}_{\boldsymbol{K}}u_K=0$，而根据正定矩阵的定义，$u$非零，所以$Vu\ne 0$，上式等号不成立，$\boldsymbol{G}$是正定矩阵。 $\blacksquare$</p>
<p>根据这个定理，线性回归里的$\boldsymbol{X}^T\boldsymbol{X}$（$\boldsymbol{X}$的Gram矩阵）是正定矩阵，因为$\boldsymbol{X}$的列向量$X_j$线性无关（非多重共线性）<br>$$<br>X=\begin{pmatrix}<br>           &amp;        &amp;       &amp;      &amp;   \\\<br>           &amp;        &amp;       &amp;      &amp;  \\\\<br>    \vec{1}&amp;  X_1&amp;  X_2&amp; \cdots&amp; X_p\\\<br>           &amp;        &amp;       &amp;      &amp; \\\<br>           &amp;        &amp;       &amp;      &amp;\\\<br>\end{pmatrix}<br>$$<br>$$<br>\boldsymbol{X}^T\boldsymbol{X}=<br>\begin{pmatrix}<br>    \left&lt; \vec{1},\vec{1} \right&gt;&amp;        \left&lt; \vec{1},X_1 \right&gt;&amp;        \left&lt; \vec{1},X_2 \right&gt;&amp;        \cdots&amp;        \left&lt; \vec{1},X_p \right&gt;\\\<br>    \left&lt; X_1,\vec{1} \right&gt;&amp;        \left&lt; X_1,X_1 \right&gt;&amp;        \left&lt; X_1,X_2 \right&gt;&amp;        \cdots&amp;        \left&lt; X_1,X_p \right&gt;\\\<br>    \vdots&amp;        \vdots&amp;        \vdots&amp;        \ddots&amp;        \vdots\\\<br>    \left&lt; X_p,\vec{1} \right&gt;&amp;        \left&lt; X_p,X_1 \right&gt;&amp;        \left&lt; X_p,X_2 \right&gt;&amp;        \cdots&amp;        \left&lt; X_p,X_p \right&gt;<br>\end{pmatrix}<br>$$</p>
<h2 id="3-矩阵的逆"><a href="#3-矩阵的逆" class="headerlink" title="3. 矩阵的逆"></a>3. 矩阵的逆</h2><h3 id="3-1-可逆矩阵的概念"><a href="#3-1-可逆矩阵的概念" class="headerlink" title="3.1    可逆矩阵的概念"></a>3.1    可逆矩阵的概念</h3><p>方阵$A\in \mathbb{R}^{K\times K}$可逆：存在一个矩阵$B$，使得$AB=BA=I_K$。</p>
<h3 id="3-2-逆矩阵的计算"><a href="#3-2-逆矩阵的计算" class="headerlink" title="3.2    逆矩阵的计算"></a>3.2    逆矩阵的计算</h3><p>$$A^{-1}=\frac{1}{\det \left( A \right)}A^*\tag{8}$$<br>行列式不等于零，这个矩阵就可逆。<br>$$<br>A=\begin{pmatrix}<br>    1&amp;        2&amp;        3\\\<br>    0&amp;        4&amp;        5\\\<br>    1&amp;        0&amp;        6<br>\end{pmatrix}<br>$$</p>
<p>行列式$\det \left( A \right) =22$，伴随矩阵$A^*=\begin{pmatrix}<br>    24&amp;        -12&amp;        -2\\\<br>    5&amp;        3&amp;        -5\\\<br>    -4&amp;        2&amp;        4<br>\end{pmatrix} $<br>$$<br>A^{-1}=\frac{1}{22}\begin{pmatrix}<br>    24&amp;        -12&amp;        -2\\\<br>    5&amp;        3&amp;        -5\\\<br>    -4&amp;        2&amp;        4<br>\end{pmatrix} =\begin{pmatrix}<br>    \dfrac{12}{11}&amp;        -\dfrac{6}{11}&amp;        -\dfrac{1}{11}\\\<br>    \dfrac{5}{22}&amp;        \dfrac{3}{22}&amp;        -\dfrac{5}{22}\\\<br>    -\dfrac{2}{11}&amp;        \dfrac{1}{11}&amp;        \dfrac{2}{11}<br>\end{pmatrix}<br>$$<br>还是那句话，用Python算不头疼</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">A = np.array((<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">6</span>)).reshape(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">np.linalg.inv(A)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<p>array([[ 1.09090909, -0.54545455, -0.09090909],<br>       [ 0.22727273,  0.13636364, -0.22727273],<br>       [-0.18181818,  0.09090909,  0.18181818]])</p>
<h3 id="3-3-正定矩阵的可逆性判断"><a href="#3-3-正定矩阵的可逆性判断" class="headerlink" title="3.3    正定矩阵的可逆性判断"></a>3.3    正定矩阵的可逆性判断</h3><p><strong>定理：</strong></p>
<blockquote>
<p>$A$正定 $\Longrightarrow$ $A$可逆</p>
</blockquote>
<p>证明：</p>
<ul>
<li>$A$正定 $\Longleftrightarrow$ $A$ 所有的特征值都大于零<details>
<summary><u>点击展开证明</u></summary>
$\forall$特征值$\lambda$对应的特征向量$\mathbf{x}$（非零），根据定义有$$A\mathbf{x}=\lambda \mathbf{x}$$$$\mathbf{x}^TA\mathbf{x}=\mathbf{x}^T\lambda \mathbf{x}=\lambda \mathbf{x}^T\mathbf{x}>0$$$$\mathbf{x}^T\mathbf{x}>0$$所以$\lambda>0$。
</details></li>
<li>$\det(A) =$ $A$所有特征值相乘 $&gt;0$<details>
<summary><u>点击展开证明</u></summary>
$A$的特征多项式$$\begin{array}{l}p_A(\lambda) &=\det(\lambda I-A ) =\left| \begin{matrix}
  \lambda -a_{11}&        -a_{12}&        \cdots&        -a_{1K}\\\\
  -a_{21}&        \lambda -a_{22}&        \cdots&        -a_{2K}\\\\
  \vdots&        \vdots&        \ddots&        \vdots\\\\
  -a_{K1}&        -a_{K2}&        \cdots&        \lambda -a_{KK}
\end{matrix} \right|\\\\
&=\lambda ^K+c_{K-1}\lambda ^{K-1}+\cdots +c_1\lambda +c_0\end{array}
$$ $c_0,c_1,\cdots ,c_{K-1}$是常数。
特征值$\lambda _1,\lambda _2,\cdots ,\lambda _K$是通过让$p_A(\lambda)$等于零求出来的解，所以$p_A(\lambda)$是可以写成$$
p_A\left( \lambda \right) =\left( \lambda -\lambda _1 \right) \left( \lambda -\lambda _2 \right) \cdots \left( \lambda -\lambda _K \right) 
$$$$
p_A\left( 0 \right) =\left( -\lambda _1 \right) \left( -\lambda _2 \right) \cdots \left( -\lambda _K \right) =\left( -1 \right) ^K\lambda _1\lambda _2\cdots \lambda _K
$$$$\begin{array}{l}
p_A\left( 0 \right) &=\det \left( -A \right) =\det \left( \left( -I \right) A \right) \\\\
&=\det \left( -I \right) \cdot \det \left( A \right) =\left( -1 \right) ^K\det \left( A \right) \end{array}
$$所以$\det \left( A \right) =\lambda _1\lambda _2\cdots \lambda _K$。
</details></li>
<li>$\det \left( A \right) \ne 0\ \Longleftrightarrow \ A$可逆。 $\blacksquare$</li>
</ul>
<p>所以线性回归里的$\boldsymbol{X}^T\boldsymbol{X}$是可逆的，因为刚才已经证明它是正定矩阵了。<br>$$===== Fin =====$$</p>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>Math</tag>
        <tag>矩阵</tag>
      </tags>
  </entry>
  <entry>
    <title>普通最小二乘法OLS的手把手求解</title>
    <url>/2020/03/06/OLS/</url>
    <content><![CDATA[<h2 id="1-一元线性回归"><a href="#1-一元线性回归" class="headerlink" title="1. 一元线性回归"></a>1. 一元线性回归</h2><p>模型：</p>
<p>$$\mathbf{y}\approx\beta _0+\beta _1\mathbf{x}$$ </p>
<p>$$\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix},<br>\quad\mathbf{x}=\begin{pmatrix}x_1\\x_2\\\vdots\\x_n\end{pmatrix}$$</p>
<p>对于每个样本：$$y_i=\hat{\beta}_0+\hat{\beta}_1x_i+\varepsilon_i$$ $$\hat{y}_i=\hat{\beta}_0+\hat{\beta}_1x_i$$<br>残差平方和 $$\text{RSS}=\sum^n_{i=1}(y_i-\hat{y}_i)^2$$</p>
<p>记函数$L(\beta_0,\beta_1)$为 $$L(\beta_0,\beta_1)=\sum^n_{i=1}(y_i-\beta_0-\beta_1x_i)^2$$<br>那么 $$(\hat{\beta}_0,\hat{\beta}_1)=\arg\min_{(\beta_0,\beta_1)}L(\beta_0,\beta_1)$$</p>
<h3 id="1-1-OLS第一步：求偏导"><a href="#1-1-OLS第一步：求偏导" class="headerlink" title="1.1 OLS第一步：求偏导"></a>1.1 OLS第一步：求偏导</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    \displaystyle\frac{\partial L}{\partial \beta _0} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)} \\<br>    \displaystyle\frac{\partial L}{\partial \beta _1} =-2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}<br>\end{array}<br>\right.<br>$$</p>
<h3 id="1-2-OLS第二步：让偏导等于零求解"><a href="#1-2-OLS第二步：让偏导等于零求解" class="headerlink" title="1.2 OLS第二步：让偏导等于零求解"></a>1.2 OLS第二步：让偏导等于零求解</h3><p>$$<br>\left \{<br>\begin{array}{l}<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right)}=0 \\<br>    -2\displaystyle\sum_{i=1}^n{\left( y_i-\beta _0-\beta _1x_i \right) x_i}=0<br>\end{array}<br>\right.<br>$$</p>
<p>约掉-2，把求和里的每一项拆出来<br>$$<br>\left \{<br>\begin{array}{c}<br>    \displaystyle\sum_{i=1}^n{y_i}-n\beta _0-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _1&amp;=0 &amp;&amp;&amp;① \\<br>    \displaystyle\sum_{i=1}^n{x_iy_i}-\left( \displaystyle\sum_{i=1}^n{x_i} \right) \beta _0-\left( \displaystyle\sum_{i=1}^n{x_i^2} \right) \beta _1&amp;=0 &amp;&amp;&amp;②<br>\end{array}<br>\right.<br>$$</p>
<p>消元法，把①式的$\beta_0$用$\beta_1$来表示，然后代入②式求解出$\beta_1$<br>$$<br>\beta _0=\frac{1}{n}\sum_{i=1}^n{y_i}-\left( \frac{1}{n}\sum_{i=1}^n{x_i} \right) \beta _1<br>$$ $$<br>\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) \left( \sum_{i=1}^n{y_i} \right) +\frac{1}{n}\left( \sum_{i=1}^n{x_i} \right) ^2\beta _1-\left( \sum_{i=1}^n{x_i^2} \right) \beta _1=0<br>$$<br>求出$\beta_1$的值，这是$\beta_1$的估计值，记为$\hat{\beta_1}$<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{x_iy_i}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) \left( \sum\limits_{i=1}^n{y_i} \right)}{\sum\limits_{i=1}^n{x_i^2}-\frac{1}{n}\left( \sum\limits_{i=1}^n{x_i} \right) ^2}<br>$$ 这个表达式一大坨不好看，还可以简化一下，之后用在数据集上算也简便很多。$\frac{1}{n}\sum\limits_{i=1}^n{x_i}$是$x$的均值，记为$\bar{x}$；同样有$\bar{y}=\sum\limits_{i=1}^n{y_i}$<br>$$<br>\sum_{i=1}^n{x_i}=n\bar{x}，<br>\sum_{i=1}^n{y_i}=n\bar{y}<br>$$<br>先变分子，<br>$$<br>\begin{aligned}\text{分子}<br>&amp;=\sum_{i=1}^n{x_iy_i}-\frac{1}{n}\left( n\bar{x} \right) \left( n\bar{y} \right) \\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-2n\bar{x}\bar{y}+n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( n\bar{y} \right) -\bar{y}\left( n\bar{x} \right) +n\bar{x}\bar{y}\\\<br>&amp;=\sum_{i=1}^n{x_iy_i}-\bar{x}\left( \sum_{i=1}^n{y_i} \right)-\bar{y}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}\bar{y}}\\\<br>&amp;=\sum_{i=1}^n{\left( x_iy_i-\bar{x}y_i-\bar{y}x_i+\bar{x}\bar{y} \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}\end{aligned}<br>$$<br>再变分母，<br>$$<br>\begin{aligned}\text{分母}&amp;=\sum_{i=1}^n{x_i^2}-\frac{1}{n}\left( n\bar{x} \right) ^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2n\bar{x}^2+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\left( n\bar{x} \right) \bar{x}+n\bar{x}^2\\\<br>&amp;=\sum_{i=1}^n{x_i^2}-2\bar{x}\left( \sum_{i=1}^n{x_i} \right) +\sum_{i=1}^n{\bar{x}^2}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i^2-2\bar{x}x_i+\bar{x}^2 \right)}\\\<br>&amp;=\sum_{i=1}^n{\left( x_i-\bar{x} \right) ^2}\end{aligned}<br>$$<br>最后得到<br>$$<br>\hat{\beta}_1=\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>$$<br>刚才的$\beta_0$用$\beta_1$表示，现在求出了$\beta_1$，$\beta_0$也就呼之欲出<br>$$<br>\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}<br>$$<br>最终，得到对于一元线性回归模型OLS的解为<br>$$<br>\left \{<br>\begin{array}{l}<br>    \hat{\beta}_0= \bar{y}-\hat{\beta}_1\bar{x} \\<br>    \hat{\beta}_1= \displaystyle\frac{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) \left( y_i-\bar{y} \right)}}{\sum\limits_{i=1}^n{\left( x_i-\bar{x} \right) ^2}}<br>\end{array}<br>\right.<br>$$</p>
<h2 id="2-多元线性回归"><a href="#2-多元线性回归" class="headerlink" title="2. 多元线性回归"></a>2. 多元线性回归</h2><p>模型：$$\mathbf{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\varepsilon }$$<br>$$\begin{array}{l}\mathbf{y}=\begin{pmatrix}y_1\\y_2\\\vdots\\y_n\end{pmatrix}\\\<br>\boldsymbol{X}=\begin{pmatrix}1&amp;x_{11}&amp;x_{12}&amp;\dots&amp;x_{1p}\\1&amp;x_{21}&amp;x_{22}&amp;\dots&amp;x_{2p}\\\vdots&amp; &amp;&amp;\ddots&amp;\vdots\\1&amp;x_{n1}&amp;x_{n2}&amp;\dots&amp;x_{np}\end{pmatrix},\quad<br>\boldsymbol{\beta}=\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}\\\<br>\boldsymbol{\varepsilon}=\begin{pmatrix}\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n\end{pmatrix}\end{array}$$<br>对于每个样本：$$\begin{aligned}y_i<br>&amp;=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\cdots+\beta_px_{ip}+\varepsilon_i\\\<br>&amp;=\begin{pmatrix}1&amp;x_{i1}&amp;x_{i2}&amp;\cdots&amp;x_{ip}\end{pmatrix}\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}+\varepsilon_i\end{aligned}$$<br>残差平方和$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp; =\sum_{i=1}^n{\left( y_i-\hat{y}_i \right) ^2}=\sum_{i=1}^n{\varepsilon _i^2}\\\<br>&amp; =\varepsilon _1^2+\varepsilon _2^2+\cdots +\varepsilon _n^2\\\<br>&amp;=\left( \begin{matrix}<br>    \varepsilon _1&amp;        \varepsilon _2&amp;        \cdots&amp;        \varepsilon _n\<br>\end{matrix} \right) \left( \begin{array}{c}<br>    \varepsilon _1\\\<br>    \varepsilon _2\\\<br>    \vdots\\\<br>    \varepsilon _n<br>\end{array} \right) \\\<br>&amp; =\boldsymbol{\varepsilon }^T\boldsymbol{\varepsilon }\\\<br>&amp; =\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)<br>\end{aligned}<br>$$</p>
<h3 id="2-1-矩阵相乘"><a href="#2-1-矩阵相乘" class="headerlink" title="2.1 矩阵相乘"></a>2.1 矩阵相乘</h3><p>前方一波矩阵运算，先补一点矩阵运算基础小知识：</p>
<ul>
<li>矩阵相乘不满足交换律<br>$$AB\ne BA$$</li>
<li>矩阵相乘满足结合律<br>$$(AB)C=A(BC)$$</li>
<li>矩阵相乘的转置提出来，要把前后两个矩阵位置换一下<br>$$(AB)^T=B^TA^T$$</li>
</ul>
<p>继续算$L(\boldsymbol{\beta})$<br>$$\begin{aligned}<br>L\left( \boldsymbol{\beta } \right)<br>&amp;=\left( \mathbf{y}-\boldsymbol{X\beta } \right) ^T\left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\left( \mathbf{y}^T-\left( \boldsymbol{X\beta } \right) ^T \right) \left( \mathbf{y}-\boldsymbol{X\beta } \right)\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\left( \boldsymbol{X\beta } \right) ^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\left( \boldsymbol{X\beta } \right) ^T\boldsymbol{X\beta }\\\<br>&amp;=\mathbf{y}^T\mathbf{y}-\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}-\mathbf{y}^T\boldsymbol{X\beta }+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>\end{aligned}$$</p>
<p>第3项$\mathbf{y}^T\boldsymbol{X\beta }$乘出来的结果是一个标量，也就是一个数（一个数怎么转置都是同一个东西）<br>$$\mathbf{y}^T\boldsymbol{X\beta}=<br>\begin{pmatrix}<br>    &amp;&amp; \mathbf{y}^T &amp;&amp;<br>\end{pmatrix}_{1\times n}<br>\begin{pmatrix}<br>    \\\<br>    \\\<br>    \boldsymbol{X\beta}\\\<br>    \\\\<br>\end{pmatrix}_{n\times 1}<br>=(\cdot)_{1\times 1}<br>$$</p>
<p>所以第3项可以写成$\mathbf{y}^T\boldsymbol{X\beta }=\left( \mathbf{y}^T\boldsymbol{X\beta } \right) ^T=\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}$，跟前面第二项一样，合并起来$L(\boldsymbol{\beta})$最后写成这样<br>$$<br>L\left( \boldsymbol{\beta } \right) =\mathbf{y}^T\mathbf{y}-2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}+\boldsymbol{\beta }^T\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<h3 id="2-2-矩阵求导"><a href="#2-2-矩阵求导" class="headerlink" title="2.2 矩阵求导"></a>2.2 矩阵求导</h3><p>接下来要求偏导，又来补一波矩阵小知识，关于对矩阵求导</p>
<ul>
<li>$\boldsymbol{a,b}$都是K阶向量<br>$$<br>\frac{\partial \boldsymbol{a}^T\boldsymbol{b}}{\partial \boldsymbol{b}}=\frac{\partial \boldsymbol{b}^T\boldsymbol{a}}{\partial \boldsymbol{b}}=\boldsymbol{a}<br>$$</li>
<li>$\boldsymbol{A}$是对称矩阵<br>$$<br>\frac{\partial \boldsymbol{b}^T\boldsymbol{Ab}}{\partial \boldsymbol{b}}=2\boldsymbol{Ab}=2\boldsymbol{b}^T\boldsymbol{A}<br>$$</li>
</ul>
<p>OK，刀磨好了继续砍柴，来求$L(\boldsymbol{\beta})$的偏导，第一项没有$\boldsymbol{\beta }$，不管它；第二项的$\boldsymbol{\beta }$是px1向量，$\boldsymbol{X}^T\mathbf{y}$也是px1向量<br>$$<br>\frac{\partial 2\boldsymbol{\beta }^T\boldsymbol{X}^T\mathbf{y}}{\partial \boldsymbol{\beta }}=\frac{\partial 2\boldsymbol{\beta }^T\left( \boldsymbol{X}^T\mathbf{y} \right)}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>第3项的$\boldsymbol{X}^T\boldsymbol{X}$是p×p对称矩阵<br>$$<br>\frac{\partial \boldsymbol{\beta }^T\left( \boldsymbol{X}^T\boldsymbol{X} \right) \boldsymbol{\beta }}{\partial \boldsymbol{\beta }}=2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>所以对$\boldsymbol{X}^T\boldsymbol{X}$求出来偏导是<br>$$<br>\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }<br>$$</p>
<p>令偏导等于零求解<br>$$<br>2\boldsymbol{X}^T\boldsymbol{X\beta }=2\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>约掉2，然后两边都乘$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}$（需要$\boldsymbol{X}^T\boldsymbol{X}$可逆）<br>$$<br>\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\boldsymbol{X\beta }=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>$\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\left( \boldsymbol{X}^T\boldsymbol{X} \right) =\boldsymbol{I}$，$\boldsymbol{I}$是一个pxp的单位矩阵，那么$\boldsymbol{I\beta }=\boldsymbol{\beta }$，所以求出来<br>$$<br>\hat{\beta}_{OLS}=\left( \boldsymbol{X}^T\boldsymbol{X} \right) ^{-1}\boldsymbol{X}^T\mathbf{y}<br>$$</p>
<p>Done！</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="Q1：什么函数能找到它的最小值？"><a href="#Q1：什么函数能找到它的最小值？" class="headerlink" title="Q1：什么函数能找到它的最小值？"></a>Q1：什么函数能找到它的最小值？</h3><p>令偏导为零能找到最小值？<br>对于残差平方和这个函数$L(\boldsymbol{\beta})$是的，这是一个凸函数。<br>一元函数$f\left( x \right) =\left( x-4 \right) ^2+3$，我们求一阶导数$f’\left( x \right) =2\left( x-4 \right)$，再求二阶导数$f’’\left( x \right) =2$，<font color='red'>二阶导大于零</font>，所以这个函数是凸函数，可以在导数等于零的时候找到最小值。<br>多元函数$L(\boldsymbol{\beta})$，一阶导刚才我们求出来，是$\displaystyle\frac{\partial L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }}=-2\boldsymbol{X}^T\mathbf{y}+2\boldsymbol{X}^T\boldsymbol{X\beta }$，继续求二阶导$\displaystyle\frac{\partial ^2L\left( \boldsymbol{\beta } \right)}{\partial \boldsymbol{\beta }^2}=2\boldsymbol{X}^T\boldsymbol{X}$，求出来这个二阶导是个矩阵。跟一元函数类似的，<font color='red'>二阶导如果是一个正定矩阵（类比正实数）</font>，那么这个函数是凸函数，可以找到最小值。而$2\boldsymbol{X}^T\boldsymbol{X}$是一个正定矩阵，因为$\boldsymbol{X}$是满秩的（基于各变量X相互独立的基本假设）。</p>
<h3 id="Q2：-boldsymbol-X-T-boldsymbol-X-可逆？"><a href="#Q2：-boldsymbol-X-T-boldsymbol-X-可逆？" class="headerlink" title="Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？"></a>Q2：$\boldsymbol{X}^T\boldsymbol{X}$可逆？</h3><p>$\boldsymbol{X}^T\boldsymbol{X}$可逆？<br>$\boldsymbol{X}^T\boldsymbol{X}$是正定矩阵  $\Longrightarrow\boldsymbol{X}^T\boldsymbol{X}$的行列式大于零<br>行列式$\ne 0\Longrightarrow$ 可逆</p>
<p>这些问题在「<a href="/2020/03/08/%E7%9F%A9%E9%98%B5%E7%9A%84%E4%B8%80%E4%BA%9B%E4%BA%8B/" title="矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵">矩阵的一些事：满秩矩阵、正定矩阵和可逆矩阵</a>」这篇文章有详解，这里一两句话说不完。</p>
<p>$$ ===== Fin =====$$</p>
<p>参考资料：</p>
<ol>
<li><a href="https://web.stanford.edu/~mrosenfe/soc_meth_proj3/matrix_OLS_NYU_notes.pdf" target="_blank" rel="noopener">OLS in Matrix Form</a></li>
</ol>
]]></content>
      <categories>
        <category>Math</category>
      </categories>
      <tags>
        <tag>OLS</tag>
        <tag>线性回归</tag>
        <tag>数学计算</tag>
      </tags>
  </entry>
</search>
